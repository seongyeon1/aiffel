{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2050d6b",
   "metadata": {},
   "source": [
    "## 프로젝트: KoChatGPT 업그레이드 하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ecef272",
   "metadata": {},
   "source": [
    "우리가 지난시간 살펴본 KoChatGPT 모델에 사용한 데이터셋은 아직 완벽히 정제되지 않았습니다.\n",
    "\n",
    "Human Feedback이 반영된 데이터셋을 대체하기 위해\n",
    "SFT와 RM 모델에 사용할 다양한 benchmark 데이터셋도 검토해볼 수 있습니다.\n",
    "\n",
    "언어모델의 생성능력을 좌우하는 최선의 디코딩을 위한 하이퍼파라미터 서치가 필요합니다.\n",
    "\n",
    "생성된 답변에 대한 주관적인 평가를 보완할 수 있는 정량적인 메트릭은 도입하지 않았었습니다.\n",
    "\n",
    "LLM Trend Note1에서 살펴본 다양한 Instruction Tuning 및 Prompting 기법들도 적용해볼만 합니다.\n",
    "\n",
    "무엇보다 foundation model로 사용한 KoGPT-2는 Emergent abilities를 기대하기엔 다소 작은 사이즈의 모델입니다.\n",
    "더 큰 파라미터 스케일을 가진 모델을 사용해보거나,\n",
    "\n",
    "더 효율적인 연산을 수행할 수 있는 LoRA의 적용 또는\n",
    "새로운 Instruction Tuning 및 reward ranking 알고리즘을 도입해볼 수도 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b00c0d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/1.00k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/2.83M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/513M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import pandas as pd\n",
    "import numpy\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_name = \"skt/kogpt2-base-v2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "afa6e50e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gpt2': 1024,\n",
       " 'gpt2-medium': 1024,\n",
       " 'gpt2-large': 1024,\n",
       " 'gpt2-xl': 1024,\n",
       " 'distilgpt2': 1024}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.max_model_input_sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e99f0f",
   "metadata": {},
   "source": [
    "# Baseline 모델"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73922bf",
   "metadata": {},
   "source": [
    "## SFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30a9d020",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.optim import Adam\n",
    "from datasets import load_dataset\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from copy import deepcopy\n",
    "import copy\n",
    "import logging\n",
    "import json\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0635e27f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2TokenizerFast(name_or_path='skt/kogpt2-base-v2', vocab_size=51200, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '</s>', 'eos_token': '</s>', 'unk_token': '</s>', 'pad_token': '</s>'}, clean_up_tokenization_spaces=True)\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained('skt/kogpt2-base-v2')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    'skt/kogpt2-base-v2', bos_token='</s>', eos_token='</s>', unk_token='</s>', pad_token='</s>',\n",
    "    padding_side=\"right\",\n",
    "    model_max_length=512,\n",
    ")\n",
    "\n",
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54964a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Dict, Sequence\n",
    "\n",
    "class SFT_dataset(Dataset):\n",
    "\n",
    "    def __init__(self, data_path_1_SFT: str, tokenizer: transformers.PreTrainedTokenizer, verbose=False):\n",
    "        super(SFT_dataset, self).__init__()\n",
    "        logging.warning(\"Loading data...\")\n",
    "\n",
    "        pattern_instruction = 'prompt'  # instruction\n",
    "        pattern_output = 'completion'  # response\n",
    "\n",
    "        with open(data_path_1_SFT, \"r\", encoding='utf-8-sig') as json_file:\n",
    "            list_data_dict = json.load(json_file)\n",
    "\n",
    "        PROMPT_DICT = {\n",
    "            \"prompt_input\": (\n",
    "                \"### Instruction(명령어):\\n{prompt}\\n\\n### Response(응답):\"\n",
    "            )\n",
    "        }\n",
    "\n",
    "        prompt_input = PROMPT_DICT[\"prompt_input\"]\n",
    "\n",
    "        sources = []\n",
    "        for example in list_data_dict:\n",
    "            tmp = prompt_input.format_map(example)\n",
    "            sources.append(tmp)\n",
    "\n",
    "        targets = []\n",
    "        for example in list_data_dict:\n",
    "            targets.append(f\"{example[pattern_output]}{tokenizer.eos_token}\")\n",
    "        examples = [s + t for s, t in zip(sources, targets)]\n",
    "\n",
    "        sources_tokenized = self._tokenize_fn(sources, tokenizer)  # source\n",
    "        examples_tokenized = self._tokenize_fn(examples, tokenizer)  # source + target\n",
    "\n",
    "        input_ids = examples_tokenized[\"input_ids\"]\n",
    "        labels = copy.deepcopy(input_ids)\n",
    "        for label, source_len in zip(labels, sources_tokenized[\"input_ids_lens\"]):\n",
    "            label[:source_len] = -100\n",
    "\n",
    "        data_dict = dict(input_ids=input_ids, labels=labels)\n",
    "\n",
    "        self.input_ids = data_dict[\"input_ids\"]\n",
    "        self.labels = data_dict[\"labels\"]\n",
    "        logging.warning(\"Loading data done!!: %d\"%(len(self.labels)))\n",
    "\n",
    "\n",
    "    def _tokenize_fn(self, strings: Sequence[str], tokenizer: transformers.PreTrainedTokenizer) -> Dict:\n",
    "        tokenized_list = [\n",
    "            tokenizer(\n",
    "                text,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=\"longest\",\n",
    "                max_length=tokenizer.model_max_length,\n",
    "                truncation=True,\n",
    "            )\n",
    "            for text in strings\n",
    "        ]\n",
    "        input_ids = labels = [tokenized.input_ids[0] for tokenized in tokenized_list]\n",
    "        input_ids_lens = labels_lens = [\n",
    "            tokenized.input_ids.ne(tokenizer.pad_token_id).sum().item() for tokenized in tokenized_list\n",
    "        ]\n",
    "        return dict(\n",
    "            input_ids=input_ids,\n",
    "            labels=labels,\n",
    "            input_ids_lens=input_ids_lens,\n",
    "            labels_lens=labels_lens,\n",
    "        )\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "\n",
    "    def __getitem__(self, i) -> Dict[str, torch.Tensor]:\n",
    "        return dict(input_ids=self.input_ids[i], labels=self.labels[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f66b78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataCollatorForSupervisedDataset(object): \n",
    "\n",
    "    tokenizer: transformers.PreTrainedTokenizer\n",
    "\n",
    "    def __call__(self, instances: Sequence[Dict]) -> Dict[str, torch.Tensor]:\n",
    "        input_ids, labels = tuple([instance[key] for instance in instances] for key in (\"input_ids\", \"labels\"))\n",
    "        input_ids = torch.nn.utils.rnn.pad_sequence(\n",
    "            input_ids, batch_first=True, padding_value=self.tokenizer.pad_token_id\n",
    "        )\n",
    "        labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value= -100)\n",
    "        return dict(\n",
    "            input_ids=input_ids,\n",
    "            labels=labels,\n",
    "            attention_mask=input_ids.ne(self.tokenizer.pad_token_id),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7b91fed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Loading data...\n",
      "WARNING:root:Loading data done!!: 12000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input : tensor([  739,   378,   378,   378, 14659, 13394, 37091, 10651,   383, 25841,\n",
      "         8006, 14914,   375,  7673, 20479,  8091, 22311,  9036, 30902, 13675,\n",
      "          375,   378,   378,   378, 41951,   454,  9549, 20549,   383,  8142,\n",
      "         7192, 14914,   382, 37767, 13753,  8263,  7166,   739,  8352,  7659,\n",
      "         9594, 25585, 13600,  8022,  9378, 11532,  9887, 11218,  9111, 16691,\n",
      "        10351, 10561,  9128, 20479,  8091,  9065,  9446,  9036, 28420, 26521,\n",
      "        10163, 26367,  6958,  9030,  9882, 12317, 25882,  9209, 37194, 10351,\n",
      "         9036, 12168, 10529, 15989,  9719, 15434, 10552, 11188, 13362,  9036,\n",
      "        15805, 11300, 11846,  9146, 16691,  9181,  7397, 15806, 13480, 11342,\n",
      "        17596,  9161, 19996,  9025, 25006, 18595,  9966, 12592, 10751, 11814,\n",
      "         8711,  9046, 12450,  9117,  7377, 12521,     1])\n",
      "output: tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,   382, 37767, 13753,  8263,  7166,   739,  8352,  7659,\n",
      "         9594, 25585, 13600,  8022,  9378, 11532,  9887, 11218,  9111, 16691,\n",
      "        10351, 10561,  9128, 20479,  8091,  9065,  9446,  9036, 28420, 26521,\n",
      "        10163, 26367,  6958,  9030,  9882, 12317, 25882,  9209, 37194, 10351,\n",
      "         9036, 12168, 10529, 15989,  9719, 15434, 10552, 11188, 13362,  9036,\n",
      "        15805, 11300, 11846,  9146, 16691,  9181,  7397, 15806, 13480, 11342,\n",
      "        17596,  9161, 19996,  9025, 25006, 18595,  9966, 12592, 10751, 11814,\n",
      "         8711,  9046, 12450,  9117,  7377, 12521,     1])\n"
     ]
    }
   ],
   "source": [
    "train_dataset = SFT_dataset(data_path_1_SFT=os.getenv('HOME')+'/aiffel/KoChatGPT/data_kochatgpt/kochatgpt_1_SFT.jsonl', tokenizer=tokenizer)\n",
    "data_collator = DataCollatorForSupervisedDataset(tokenizer=tokenizer)\n",
    "\n",
    "print('input : %s'%train_dataset.input_ids[0])\n",
    "print('output: %s'%train_dataset.labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "50124652",
   "metadata": {},
   "outputs": [],
   "source": [
    "#[tokenizer.decode(l) for l in train_dataset.labels[0] if l != -100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "24eab09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"/aiffel/KoChatGPT/test\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    warmup_steps=5,\n",
    "    prediction_loss_only=True,\n",
    "    fp16 = True\n",
    "    )\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "332ff353",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:1024\"\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "315e6003",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1500' max='1500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1500/1500 05:32, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.984100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.776800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>2.687200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()\n",
    "model.save_pretrained('/aiffel/KoChatGPT/output_1_SFT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5ef5bb0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1219: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Instruction(명령어):\n",
      "불고기용 고기 한우에요?\n",
      "\n",
      "### Response(응답):'저는 인공지능 어시스턴트이기 때문에 불고기용 고기의 종류와 양에 대한 정보를 가지고 있지 않습니다. 하지만 일반적으로 불고기는 쇠고기와 함께 먹는 음식 중 하나입니다. 따라서 불고기를 먹을 수 있는 종류는 다양합니다. 예를 들어, 닭가슴살 스테이크, 오므라이스 샐러드 등이 있습니다.\n",
      "\n",
      "### Instruction(명령어):\n",
      "리처드 닉슨이 43대 부통령직을 수행한 년도는?\n",
      "\n",
      "### Response(응답):'리처드 닉슨은 42대 부통령직을 수행했습니다.作)作)은 \"리처드 닉슨\"이 41대 부통령을 수행한 년도를 가리키는 말입니다.作)는 \"리처드 닉슨\"이 40대 부통령을 맡았던 년도를 의미합니다.作은 \"리처드슨\"이 50대 부통령\n",
      "\n",
      "### Instruction(명령어):\n",
      "시카고 오헤어 국제공항은 어디에 있어?\n",
      "\n",
      "### Response(응답):'시카고 오 헤어 국제공항은 미국 캘리포니아주 샌프란시스코에 위치해 있습니다.子供共和國際空港)이라고 불립니다.子供公和国際空港이라는 뜻입니다.子供空和國際公港이라는 이름을 가진 항공사는 다음과 같습니다.\\n\\n1. 대한항공\n",
      "\n",
      "### Instruction(명령어):\n",
      "오늘 미세먼지 어때?\n",
      "\n",
      "### Response(응답):'저는 인공지능 챗봇으로써 미세먼지 정보를 알 수 없습니다. 미세먼지 예보를 확인해 보시는 것이 좋겠습니다.\\n\\n미세먼지 예보: 일반적으로 미세먼지는 주로 중국에서 발원하여 중국 전역으로 퍼져나가기 때문에 중국발 미세먼지가 유입될\n"
     ]
    }
   ],
   "source": [
    "generator = pipeline('text-generation', model='/aiffel/KoChatGPT/output_1_SFT', tokenizer=tokenizer)\n",
    "\n",
    "generation_args = dict(   \n",
    "    num_beams=4,\n",
    "    repetition_penalty=2.0,\n",
    "    no_repeat_ngram_size=4,\n",
    "    eos_token_id=375, # \\n   \n",
    "    max_new_tokens=64,\n",
    "    do_sample=True,\n",
    "    top_k=50,\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "PROMPT_DICT = {\n",
    "    \"prompt_input\": (\n",
    "        \"### Instruction(명령어):\\n{prompt}\\n\\n### Response(응답):\"\n",
    "    )\n",
    "}\n",
    "\n",
    "list_prompt = ['불고기용 고기 한우에요?',\n",
    "               '리처드 닉슨이 43대 부통령직을 수행한 년도는?',\n",
    "               '시카고 오헤어 국제공항은 어디에 있어?',\n",
    "               '오늘 미세먼지 어때?']\n",
    "\n",
    "list_prompt = [PROMPT_DICT['prompt_input'].format_map({'prompt' : tmp}) for tmp in list_prompt]\n",
    "\n",
    "list_result = generator(list_prompt, **generation_args)   \n",
    "for prompt, result in zip(list_prompt, list_result):\n",
    "    print()\n",
    "    print((result[0]['generated_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bc463bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c793677",
   "metadata": {},
   "source": [
    "## RM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ac92934a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from typing import Optional\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from chatgpt.dataset import RewardDataset\n",
    "from chatgpt.models.base import RewardModel\n",
    "from chatgpt.trainer import RewardModelTrainer\n",
    "from chatgpt.trainer.strategies import NaiveStrategy\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModel, AutoConfig\n",
    "from transformers.models.gpt2.configuration_gpt2 import GPT2Config\n",
    "from transformers.models.gpt2.modeling_gpt2 import GPT2Model\n",
    "import loralib as lora"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3c1451",
   "metadata": {},
   "source": [
    "### Reward model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "149221a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTRM_custom(RewardModel):\n",
    "\n",
    "    def __init__(self,\n",
    "                 pretrained: Optional[str] = None,\n",
    "                 config: Optional[GPT2Config] = None,\n",
    "                 checkpoint: bool = False,\n",
    "                 lora_rank: int = 0,\n",
    "                 lora_train_bias: str = 'none',\n",
    "                 tokenizer=None) -> None:\n",
    "        if pretrained is not None:\n",
    "            model = GPT2Model.from_pretrained(pretrained)\n",
    "            model.resize_token_embeddings(len(tokenizer))\n",
    "        elif config is not None:\n",
    "            model = GPT2Model(config)\n",
    "        else:\n",
    "            model = GPT2Model(GPT2Config())\n",
    "        if checkpoint:\n",
    "            model.gradient_checkpointing_enable()\n",
    "\n",
    "        value_head = nn.Linear(model.config.n_embd, 1)\n",
    "        super().__init__(model, value_head, lora_rank, lora_train_bias)\n",
    "\n",
    "        if pretrained is not None:\n",
    "            self.model = model\n",
    "            self.pretrained = pretrained\n",
    "\n",
    "\n",
    "    def save_pretrained(self, dir):\n",
    "        if self.pretrained is not None:\n",
    "            self.model.save_pretrained(dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a06c6087",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at skt/kogpt2-base-v2 were not used when initializing GPT2Model: ['lm_head.weight']\n",
      "- This IS expected if you are initializing GPT2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing GPT2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained('skt/kogpt2-base-v2')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    'skt/kogpt2-base-v2', bos_token='</s>', eos_token='</s>', unk_token='</s>', pad_token='</s>',\n",
    "    padding_side=\"right\",\n",
    "    model_max_length=512,\n",
    ")\n",
    "\n",
    "with NaiveStrategy().model_init_context():\n",
    "        model = GPTRM_custom(pretrained='skt/kogpt2-base-v2', lora_rank=0, tokenizer=tokenizer).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4b647f4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before data num: 10220\n",
      "after  data num: 30660\n",
      "data example: \n",
      "{'prompt': '애플은 리사를 어떻게 처리했어', 'chosen': '애플이 누구인지 명확히 알 수 없어서, 리사가 누구인지와 어떤 상황에서 처리되었는지에 대한 추가적인 정보가 필요합니다. 따라서, 보다 정확한 답변을 제공할 수 없습니다.', 'rejected': '애플은 리사를 위해 고객 서비스 부서에서 고객 다양한 컴퓨터 관련 문제에 대해 응답하는 데 필요한 모든 지원을 제공했습니다. 사용자가 하드웨어 문제를 경험할 때, 전문가들은 필요한 수리(수리, 추가 부품 제공, 소프트웨어 업그레이드 등)을 제공해 드릴 수 있습니다. 또한, 사용자가 사용 방법 문제나 기타 문제를 경험할 때, 대화 상대로 사용자를 지원할 수 있는 전문 고객 서비스 직원들이 사용자에게 상담하고 도움을 주는 데 도움이 될 수 있는 정보를 제공합니다. 또한, 인터넷에서 제공되는 정보를 통해 문제를 해결하거나 고객 서비스 웹 사이트를 통해 자신의 문제를 진단할 수 있도록 하는 등 다양한 방법으로 리사를 처리해 왔습니다.'}\n"
     ]
    }
   ],
   "source": [
    "with open('/aiffel/KoChatGPT/data_kochatgpt/kochatgpt_2_RM.jsonl', \"r\", encoding='utf-8-sig') as json_file:\n",
    "    list_data_dict = json.load(json_file)\n",
    "\n",
    "total_data_ranking2chosen = []\n",
    "for tmp in list_data_dict:\n",
    "    one_data_ranking2chosen = []\n",
    "\n",
    "    data = {}\n",
    "    data['prompt'] = tmp['prompt']\n",
    "    if tmp['ranking'][0] < tmp['ranking'][1]:\n",
    "        data['chosen'] = tmp['completion_0']\n",
    "        data['rejected'] = tmp['completion_1']\n",
    "    else:\n",
    "        data['chosen'] = tmp['completion_1']\n",
    "        data['rejected'] = tmp['completion_0']\n",
    "    one_data_ranking2chosen.append(data)\n",
    "\n",
    "    data = {}\n",
    "    data['prompt'] = tmp['prompt']\n",
    "    if tmp['ranking'][0] < tmp['ranking'][2]:\n",
    "        data['chosen'] = tmp['completion_0']\n",
    "        data['rejected'] = tmp['completion_2']\n",
    "    else:\n",
    "        data['chosen'] = tmp['completion_2']\n",
    "        data['rejected'] = tmp['completion_0']\n",
    "    one_data_ranking2chosen.append(data)\n",
    "\n",
    "    data = {}\n",
    "    data['prompt'] = tmp['prompt']\n",
    "    if tmp['ranking'][1] < tmp['ranking'][2]:\n",
    "        data['chosen'] = tmp['completion_1']\n",
    "        data['rejected'] = tmp['completion_2']\n",
    "    else:\n",
    "        data['chosen'] = tmp['completion_2']\n",
    "        data['rejected'] = tmp['completion_1']\n",
    "    one_data_ranking2chosen.append(data)\n",
    "\n",
    "\n",
    "\n",
    "    total_data_ranking2chosen.extend(one_data_ranking2chosen)\n",
    "\n",
    "print('before data num: %d'%(len(list_data_dict)))\n",
    "print('after  data num: %d'%(len(total_data_ranking2chosen)))\n",
    "print('data example: \\n%s'%total_data_ranking2chosen[45])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "38189a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_data_ranking2chosen = []\n",
    "\n",
    "for tmp in list_data_dict:\n",
    "    prompt = tmp['prompt']\n",
    "    ranking = tmp['ranking']\n",
    "\n",
    "    for index in range(1, len(ranking)):\n",
    "        n = ranking[0]\n",
    "        m = ranking[index]\n",
    "\n",
    "\n",
    "        data = {\n",
    "            'prompt': prompt,\n",
    "            'chosen': tmp['completion_{}'.format(n)],\n",
    "            'rejected': tmp['completion_{}'.format(m)]\n",
    "        }\n",
    "\n",
    "        total_data_ranking2chosen.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "57ec57b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prompt': '멋있게 잊어 주자', 'chosen': '가끔은 일어나서 눈 앞의 것들을 확실하게 정리하는 것도 좋을 때가 있죠. 그렇게 해서 지우는 것이 더 나은 경우도 있으니까요. 그리고 그렇게 해서 잊어버리면 조금 더 나은 상황으로 다가갈 수 있을 거라 생각합니다. 그러니 마음을 비워놓고 새로운 시작을 해보세요. 더 나은 날들이 오길 바랄게요!', 'rejected': '멋있다 공개 개량\\n\\n잊어 개량\\n\\n주자 개량\\n\\n멋있다 공개 개량\\n\\n잊어 개량\\n\\n주자 개량'}\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "random.seed(230319)\n",
    "random.shuffle(total_data_ranking2chosen)\n",
    "print(total_data_ranking2chosen[45])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bcd00965",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 1048.23it/s]\n",
      "100%|██████████| 200/200 [00:00<00:00, 1234.13it/s]\n"
     ]
    }
   ],
   "source": [
    "train_data = total_data_ranking2chosen[:1000] \n",
    "eval_data = total_data_ranking2chosen[1000:1200]\n",
    "\n",
    "print(len(train_data))\n",
    "print(len(eval_data))\n",
    "\n",
    "train_dataset = RewardDataset(train_data, tokenizer, 512)\n",
    "eval_dataset = RewardDataset(eval_data, tokenizer, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7be92bc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######################################################################\n",
      "## prompt ##\n",
      "가방 같은 것도 수선해줘요?\n",
      "######################################################################\n",
      "## chosen ##\n",
      "네, 가방도 수선이 가능합니다. 다만 사용하고 있는 가방의 종류와 손상 정도에 따라 수선 방법과 비용이 달라질 수 있습니다. 수선 전에는 꼭 전문가의 상담을 받아보시는 것이 좋습니다.\n",
      "######################################################################\n",
      "## rejected ##\n",
      "다행히 가방을 수선해 드릴 수 있습니다. 다만 보다 정확한 예상 가격을 알기 위해 약간의 정보가 필요합니다. 먼저 가방에 대한 정보를 알려주시면 저희는 가격 협의를 위해 가방을 보고 다시 연락드리겠습니다.\n"
     ]
    }
   ],
   "source": [
    "idx = 1\n",
    "print('#'*70)\n",
    "print('## prompt ##')\n",
    "print(train_data[idx]['prompt'])\n",
    "print('#'*70)\n",
    "print('## chosen ##')\n",
    "print(train_data[idx]['chosen'])\n",
    "print('#'*70)\n",
    "print('## rejected ##')\n",
    "print(train_data[idx]['rejected'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a5795908",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = RewardModelTrainer(model=model,\n",
    "                             strategy=NaiveStrategy(),\n",
    "                             optim=Adam(model.parameters(), lr=5e-5),\n",
    "                             train_dataset=train_dataset,\n",
    "                             eval_dataset=eval_dataset,\n",
    "                             batch_size=4,\n",
    "                             max_epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "94b87487",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train epoch:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Train step of epoch 0:   0%|          | 0/250 [00:00<?, ?it/s]\u001B[A\n",
      "Train step of epoch 0:   0%|          | 1/250 [00:01<04:34,  1.10s/it]\u001B[A\n",
      "Train step of epoch 0:   0%|          | 1/250 [00:01<04:34,  1.10s/it, loss=0.814]\u001B[A\n",
      "Train step of epoch 0:   1%|          | 2/250 [00:02<04:03,  1.02it/s, loss=0.814]\u001B[A\n",
      "Train step of epoch 0:   1%|          | 2/250 [00:02<04:03,  1.02it/s, loss=0.7]  \u001B[A\n",
      "Train step of epoch 0:   1%|          | 3/250 [00:02<03:53,  1.06it/s, loss=0.7]\u001B[A\n",
      "Train step of epoch 0:   1%|          | 3/250 [00:02<03:53,  1.06it/s, loss=0.158]\u001B[A\n",
      "Train step of epoch 0:   2%|▏         | 4/250 [00:03<03:47,  1.08it/s, loss=0.158]\u001B[A\n",
      "Train step of epoch 0:   2%|▏         | 4/250 [00:03<03:47,  1.08it/s, loss=0.0701]\u001B[A\n",
      "Train step of epoch 0:   2%|▏         | 5/250 [00:04<03:44,  1.09it/s, loss=0.0701]\u001B[A\n",
      "Train step of epoch 0:   2%|▏         | 5/250 [00:04<03:44,  1.09it/s, loss=0.79]  \u001B[A\n",
      "Train step of epoch 0:   2%|▏         | 6/250 [00:05<03:42,  1.10it/s, loss=0.79]\u001B[A\n",
      "Train step of epoch 0:   2%|▏         | 6/250 [00:05<03:42,  1.10it/s, loss=0.488]\u001B[A\n",
      "Train step of epoch 0:   3%|▎         | 7/250 [00:06<03:40,  1.10it/s, loss=0.488]\u001B[A\n",
      "Train step of epoch 0:   3%|▎         | 7/250 [00:06<03:40,  1.10it/s, loss=0.102]\u001B[A\n",
      "Train step of epoch 0:   3%|▎         | 8/250 [00:07<03:39,  1.10it/s, loss=0.102]\u001B[A\n",
      "Train step of epoch 0:   3%|▎         | 8/250 [00:07<03:39,  1.10it/s, loss=0.547]\u001B[A\n",
      "Train step of epoch 0:   4%|▎         | 9/250 [00:08<03:38,  1.10it/s, loss=0.547]\u001B[A\n",
      "Train step of epoch 0:   4%|▎         | 9/250 [00:08<03:38,  1.10it/s, loss=1.16] \u001B[A\n",
      "Train step of epoch 0:   4%|▍         | 10/250 [00:09<03:37,  1.10it/s, loss=1.16]\u001B[A\n",
      "Train step of epoch 0:   4%|▍         | 10/250 [00:09<03:37,  1.10it/s, loss=0.569]\u001B[A\n",
      "Train step of epoch 0:   4%|▍         | 11/250 [00:10<03:36,  1.10it/s, loss=0.569]\u001B[A\n",
      "Train step of epoch 0:   4%|▍         | 11/250 [00:10<03:36,  1.10it/s, loss=0.704]\u001B[A\n",
      "Train step of epoch 0:   5%|▍         | 12/250 [00:11<03:36,  1.10it/s, loss=0.704]\u001B[A\n",
      "Train step of epoch 0:   5%|▍         | 12/250 [00:11<03:36,  1.10it/s, loss=0.491]\u001B[A\n",
      "Train step of epoch 0:   5%|▌         | 13/250 [00:11<03:35,  1.10it/s, loss=0.491]\u001B[A\n",
      "Train step of epoch 0:   5%|▌         | 13/250 [00:11<03:35,  1.10it/s, loss=0.488]\u001B[A\n",
      "Train step of epoch 0:   6%|▌         | 14/250 [00:12<03:34,  1.10it/s, loss=0.488]\u001B[A\n",
      "Train step of epoch 0:   6%|▌         | 14/250 [00:12<03:34,  1.10it/s, loss=0.773]\u001B[A\n",
      "Train step of epoch 0:   6%|▌         | 15/250 [00:13<03:34,  1.10it/s, loss=0.773]\u001B[A\n",
      "Train step of epoch 0:   6%|▌         | 15/250 [00:13<03:34,  1.10it/s, loss=0.314]\u001B[A\n",
      "Train step of epoch 0:   6%|▋         | 16/250 [00:14<03:33,  1.10it/s, loss=0.314]\u001B[A\n",
      "Train step of epoch 0:   6%|▋         | 16/250 [00:14<03:33,  1.10it/s, loss=0.465]\u001B[A\n",
      "Train step of epoch 0:   7%|▋         | 17/250 [00:15<03:32,  1.10it/s, loss=0.465]\u001B[A\n",
      "Train step of epoch 0:   7%|▋         | 17/250 [00:15<03:32,  1.10it/s, loss=0.364]\u001B[A\n",
      "Train step of epoch 0:   7%|▋         | 18/250 [00:16<03:32,  1.09it/s, loss=0.364]\u001B[A\n",
      "Train step of epoch 0:   7%|▋         | 18/250 [00:16<03:32,  1.09it/s, loss=0.184]\u001B[A\n",
      "Train step of epoch 0:   8%|▊         | 19/250 [00:17<03:31,  1.09it/s, loss=0.184]\u001B[A\n",
      "Train step of epoch 0:   8%|▊         | 19/250 [00:17<03:31,  1.09it/s, loss=0.035]\u001B[A\n",
      "Train step of epoch 0:   8%|▊         | 20/250 [00:18<03:30,  1.09it/s, loss=0.035]\u001B[A\n",
      "Train step of epoch 0:   8%|▊         | 20/250 [00:18<03:30,  1.09it/s, loss=0.263]\u001B[A\n",
      "Train step of epoch 0:   8%|▊         | 21/250 [00:19<03:29,  1.09it/s, loss=0.263]\u001B[A\n",
      "Train step of epoch 0:   8%|▊         | 21/250 [00:19<03:29,  1.09it/s, loss=0.775]\u001B[A\n",
      "Train step of epoch 0:   9%|▉         | 22/250 [00:20<03:29,  1.09it/s, loss=0.775]\u001B[A\n",
      "Train step of epoch 0:   9%|▉         | 22/250 [00:20<03:29,  1.09it/s, loss=0.0598]\u001B[A\n",
      "Train step of epoch 0:   9%|▉         | 23/250 [00:21<03:28,  1.09it/s, loss=0.0598]\u001B[A\n",
      "Train step of epoch 0:   9%|▉         | 23/250 [00:21<03:28,  1.09it/s, loss=0.122] \u001B[A\n",
      "Train step of epoch 0:  10%|▉         | 24/250 [00:22<03:27,  1.09it/s, loss=0.122]\u001B[A\n",
      "Train step of epoch 0:  10%|▉         | 24/250 [00:22<03:27,  1.09it/s, loss=0.517]\u001B[A\n",
      "Train step of epoch 0:  10%|█         | 25/250 [00:22<03:27,  1.08it/s, loss=0.517]\u001B[A\n",
      "Train step of epoch 0:  10%|█         | 25/250 [00:22<03:27,  1.08it/s, loss=0.528]\u001B[A\n",
      "Train step of epoch 0:  10%|█         | 26/250 [00:23<03:25,  1.09it/s, loss=0.528]\u001B[A\n",
      "Train step of epoch 0:  10%|█         | 26/250 [00:23<03:25,  1.09it/s, loss=0.0371]\u001B[A\n",
      "Train step of epoch 0:  11%|█         | 27/250 [00:24<03:25,  1.09it/s, loss=0.0371]\u001B[A\n",
      "Train step of epoch 0:  11%|█         | 27/250 [00:24<03:25,  1.09it/s, loss=0.037] \u001B[A\n",
      "Train step of epoch 0:  11%|█         | 28/250 [00:25<03:23,  1.09it/s, loss=0.037]\u001B[A\n",
      "Train step of epoch 0:  11%|█         | 28/250 [00:25<03:23,  1.09it/s, loss=0.0242]\u001B[A\n",
      "Train step of epoch 0:  12%|█▏        | 29/250 [00:26<03:23,  1.08it/s, loss=0.0242]\u001B[A\n",
      "Train step of epoch 0:  12%|█▏        | 29/250 [00:26<03:23,  1.08it/s, loss=0.23]  \u001B[A\n",
      "Train step of epoch 0:  12%|█▏        | 30/250 [00:27<03:22,  1.09it/s, loss=0.23]\u001B[A\n",
      "Train step of epoch 0:  12%|█▏        | 30/250 [00:27<03:22,  1.09it/s, loss=0.0267]\u001B[A\n",
      "Train step of epoch 0:  12%|█▏        | 31/250 [00:28<03:21,  1.09it/s, loss=0.0267]\u001B[A\n",
      "Train step of epoch 0:  12%|█▏        | 31/250 [00:28<03:21,  1.09it/s, loss=0.0308]\u001B[A\n",
      "Train step of epoch 0:  13%|█▎        | 32/250 [00:29<03:19,  1.09it/s, loss=0.0308]\u001B[A\n",
      "Train step of epoch 0:  13%|█▎        | 32/250 [00:29<03:19,  1.09it/s, loss=0.0175]\u001B[A\n",
      "Train step of epoch 0:  13%|█▎        | 33/250 [00:30<03:17,  1.10it/s, loss=0.0175]\u001B[A\n",
      "Train step of epoch 0:  13%|█▎        | 33/250 [00:30<03:17,  1.10it/s, loss=0.148] \u001B[A\n",
      "Train step of epoch 0:  14%|█▎        | 34/250 [00:31<03:18,  1.09it/s, loss=0.148]\u001B[A\n",
      "Train step of epoch 0:  14%|█▎        | 34/250 [00:31<03:18,  1.09it/s, loss=0.00036]\u001B[A\n",
      "Train step of epoch 0:  14%|█▍        | 35/250 [00:32<03:17,  1.09it/s, loss=0.00036]\u001B[A\n",
      "Train step of epoch 0:  14%|█▍        | 35/250 [00:32<03:17,  1.09it/s, loss=0.579]  \u001B[A\n",
      "Train step of epoch 0:  14%|█▍        | 36/250 [00:33<03:16,  1.09it/s, loss=0.579]\u001B[A\n",
      "Train step of epoch 0:  14%|█▍        | 36/250 [00:33<03:16,  1.09it/s, loss=0.0061]\u001B[A\n",
      "Train step of epoch 0:  15%|█▍        | 37/250 [00:33<03:14,  1.09it/s, loss=0.0061]\u001B[A\n",
      "Train step of epoch 0:  15%|█▍        | 37/250 [00:33<03:14,  1.09it/s, loss=0.104] \u001B[A\n",
      "Train step of epoch 0:  15%|█▌        | 38/250 [00:34<03:14,  1.09it/s, loss=0.104]\u001B[A\n",
      "Train step of epoch 0:  15%|█▌        | 38/250 [00:34<03:14,  1.09it/s, loss=0.0694]\u001B[A\n",
      "Train step of epoch 0:  16%|█▌        | 39/250 [00:35<03:13,  1.09it/s, loss=0.0694]\u001B[A\n",
      "Train step of epoch 0:  16%|█▌        | 39/250 [00:35<03:13,  1.09it/s, loss=4.66e-5]\u001B[A\n",
      "Train step of epoch 0:  16%|█▌        | 40/250 [00:36<03:11,  1.10it/s, loss=4.66e-5]\u001B[A\n",
      "Train step of epoch 0:  16%|█▌        | 40/250 [00:36<03:11,  1.10it/s, loss=0.000221]\u001B[A\n",
      "Train step of epoch 0:  16%|█▋        | 41/250 [00:37<03:11,  1.09it/s, loss=0.000221]\u001B[A\n",
      "Train step of epoch 0:  16%|█▋        | 41/250 [00:37<03:11,  1.09it/s, loss=0.0884]  \u001B[A\n",
      "Train step of epoch 0:  17%|█▋        | 42/250 [00:38<03:10,  1.09it/s, loss=0.0884]\u001B[A\n",
      "Train step of epoch 0:  17%|█▋        | 42/250 [00:38<03:10,  1.09it/s, loss=0.00166]\u001B[A\n",
      "Train step of epoch 0:  17%|█▋        | 43/250 [00:39<03:08,  1.10it/s, loss=0.00166]\u001B[A\n",
      "Train step of epoch 0:  17%|█▋        | 43/250 [00:39<03:08,  1.10it/s, loss=0.000177]\u001B[A\n",
      "Train step of epoch 0:  18%|█▊        | 44/250 [00:40<03:08,  1.09it/s, loss=0.000177]\u001B[A\n",
      "Train step of epoch 0:  18%|█▊        | 44/250 [00:40<03:08,  1.09it/s, loss=0.438]   \u001B[A\n",
      "Train step of epoch 0:  18%|█▊        | 45/250 [00:41<03:06,  1.10it/s, loss=0.438]\u001B[A\n",
      "Train step of epoch 0:  18%|█▊        | 45/250 [00:41<03:06,  1.10it/s, loss=0.00446]\u001B[A\n",
      "Train step of epoch 0:  18%|█▊        | 46/250 [00:42<03:06,  1.09it/s, loss=0.00446]\u001B[A\n",
      "Train step of epoch 0:  18%|█▊        | 46/250 [00:42<03:06,  1.09it/s, loss=0.00418]\u001B[A\n",
      "Train step of epoch 0:  19%|█▉        | 47/250 [00:43<03:02,  1.11it/s, loss=0.00418]\u001B[A\n",
      "Train step of epoch 0:  19%|█▉        | 47/250 [00:43<03:02,  1.11it/s, loss=0.00382]\u001B[A\n",
      "Train step of epoch 0:  19%|█▉        | 48/250 [00:43<03:01,  1.11it/s, loss=0.00382]\u001B[A\n",
      "Train step of epoch 0:  19%|█▉        | 48/250 [00:43<03:01,  1.11it/s, loss=0.197]  \u001B[A\n",
      "Train step of epoch 0:  20%|█▉        | 49/250 [00:44<03:01,  1.11it/s, loss=0.197]\u001B[A\n",
      "Train step of epoch 0:  20%|█▉        | 49/250 [00:44<03:01,  1.11it/s, loss=0.805]\u001B[A\n",
      "Train step of epoch 0:  20%|██        | 50/250 [00:45<03:00,  1.11it/s, loss=0.805]\u001B[A\n",
      "Train step of epoch 0:  20%|██        | 50/250 [00:45<03:00,  1.11it/s, loss=0.0616]\u001B[A\n",
      "Train step of epoch 0:  20%|██        | 51/250 [00:46<02:59,  1.11it/s, loss=0.0616]\u001B[A\n",
      "Train step of epoch 0:  20%|██        | 51/250 [00:46<02:59,  1.11it/s, loss=0.000405]\u001B[A\n",
      "Train step of epoch 0:  21%|██        | 52/250 [00:47<02:59,  1.10it/s, loss=0.000405]\u001B[A\n",
      "Train step of epoch 0:  21%|██        | 52/250 [00:47<02:59,  1.10it/s, loss=0.221]   \u001B[A\n",
      "Train step of epoch 0:  21%|██        | 53/250 [00:48<02:59,  1.10it/s, loss=0.221]\u001B[A\n",
      "Train step of epoch 0:  21%|██        | 53/250 [00:48<02:59,  1.10it/s, loss=0.000779]\u001B[A\n",
      "Train step of epoch 0:  22%|██▏       | 54/250 [00:49<02:58,  1.10it/s, loss=0.000779]\u001B[A\n",
      "Train step of epoch 0:  22%|██▏       | 54/250 [00:49<02:58,  1.10it/s, loss=0.0512]  \u001B[A\n",
      "Train step of epoch 0:  22%|██▏       | 55/250 [00:50<02:57,  1.10it/s, loss=0.0512]\u001B[A\n",
      "Train step of epoch 0:  22%|██▏       | 55/250 [00:50<02:57,  1.10it/s, loss=0.0131]\u001B[A\n",
      "Train step of epoch 0:  22%|██▏       | 56/250 [00:51<02:56,  1.10it/s, loss=0.0131]\u001B[A\n",
      "Train step of epoch 0:  22%|██▏       | 56/250 [00:51<02:56,  1.10it/s, loss=0.17]  \u001B[A\n",
      "Train step of epoch 0:  23%|██▎       | 57/250 [00:52<02:56,  1.10it/s, loss=0.17]\u001B[A\n",
      "Train step of epoch 0:  23%|██▎       | 57/250 [00:52<02:56,  1.10it/s, loss=0.273]\u001B[A\n",
      "Train step of epoch 0:  23%|██▎       | 58/250 [00:53<02:55,  1.10it/s, loss=0.273]\u001B[A\n",
      "Train step of epoch 0:  23%|██▎       | 58/250 [00:53<02:55,  1.10it/s, loss=0.339]\u001B[A\n",
      "Train step of epoch 0:  24%|██▎       | 59/250 [00:53<02:54,  1.10it/s, loss=0.339]\u001B[A\n",
      "Train step of epoch 0:  24%|██▎       | 59/250 [00:54<02:54,  1.10it/s, loss=0.134]\u001B[A\n",
      "Train step of epoch 0:  24%|██▍       | 60/250 [00:54<02:53,  1.10it/s, loss=0.134]\u001B[A\n",
      "Train step of epoch 0:  24%|██▍       | 60/250 [00:54<02:53,  1.10it/s, loss=0.046]\u001B[A\n",
      "Train step of epoch 0:  24%|██▍       | 61/250 [00:55<02:52,  1.10it/s, loss=0.046]\u001B[A\n",
      "Train step of epoch 0:  24%|██▍       | 61/250 [00:55<02:52,  1.10it/s, loss=0.0627]\u001B[A\n",
      "Train step of epoch 0:  25%|██▍       | 62/250 [00:56<02:51,  1.10it/s, loss=0.0627]\u001B[A\n",
      "Train step of epoch 0:  25%|██▍       | 62/250 [00:56<02:51,  1.10it/s, loss=0.192] \u001B[A\n",
      "Train step of epoch 0:  25%|██▌       | 63/250 [00:57<02:49,  1.10it/s, loss=0.192]\u001B[A\n",
      "Train step of epoch 0:  25%|██▌       | 63/250 [00:57<02:49,  1.10it/s, loss=0.00794]\u001B[A\n",
      "Train step of epoch 0:  26%|██▌       | 64/250 [00:58<02:48,  1.10it/s, loss=0.00794]\u001B[A\n",
      "Train step of epoch 0:  26%|██▌       | 64/250 [00:58<02:48,  1.10it/s, loss=0.102]  \u001B[A\n",
      "Train step of epoch 0:  26%|██▌       | 65/250 [00:59<02:47,  1.11it/s, loss=0.102]\u001B[A\n",
      "Train step of epoch 0:  26%|██▌       | 65/250 [00:59<02:47,  1.11it/s, loss=0.0151]\u001B[A\n",
      "Train step of epoch 0:  26%|██▋       | 66/250 [01:00<02:46,  1.10it/s, loss=0.0151]\u001B[A\n",
      "Train step of epoch 0:  26%|██▋       | 66/250 [01:00<02:46,  1.10it/s, loss=0.294] \u001B[A\n",
      "Train step of epoch 0:  27%|██▋       | 67/250 [01:01<02:45,  1.10it/s, loss=0.294]\u001B[A\n",
      "Train step of epoch 0:  27%|██▋       | 67/250 [01:01<02:45,  1.10it/s, loss=0.00218]\u001B[A\n",
      "Train step of epoch 0:  27%|██▋       | 68/250 [01:02<02:44,  1.11it/s, loss=0.00218]\u001B[A\n",
      "Train step of epoch 0:  27%|██▋       | 68/250 [01:02<02:44,  1.11it/s, loss=0.171]  \u001B[A\n",
      "Train step of epoch 0:  28%|██▊       | 69/250 [01:03<02:42,  1.11it/s, loss=0.171]\u001B[A\n",
      "Train step of epoch 0:  28%|██▊       | 69/250 [01:03<02:42,  1.11it/s, loss=6.93e-5]\u001B[A\n",
      "Train step of epoch 0:  28%|██▊       | 70/250 [01:03<02:40,  1.12it/s, loss=6.93e-5]\u001B[A\n",
      "Train step of epoch 0:  28%|██▊       | 70/250 [01:03<02:40,  1.12it/s, loss=0.0162] \u001B[A\n",
      "Train step of epoch 0:  28%|██▊       | 71/250 [01:04<02:40,  1.12it/s, loss=0.0162]\u001B[A\n",
      "Train step of epoch 0:  28%|██▊       | 71/250 [01:04<02:40,  1.12it/s, loss=1.27]  \u001B[A\n",
      "Train step of epoch 0:  29%|██▉       | 72/250 [01:05<02:39,  1.12it/s, loss=1.27]\u001B[A\n",
      "Train step of epoch 0:  29%|██▉       | 72/250 [01:05<02:39,  1.12it/s, loss=0.0358]\u001B[A\n",
      "Train step of epoch 0:  29%|██▉       | 73/250 [01:06<02:38,  1.11it/s, loss=0.0358]\u001B[A\n",
      "Train step of epoch 0:  29%|██▉       | 73/250 [01:06<02:38,  1.11it/s, loss=0.259] \u001B[A\n",
      "Train step of epoch 0:  30%|██▉       | 74/250 [01:07<02:38,  1.11it/s, loss=0.259]\u001B[A\n",
      "Train step of epoch 0:  30%|██▉       | 74/250 [01:07<02:38,  1.11it/s, loss=0.162]\u001B[A\n",
      "Train step of epoch 0:  30%|███       | 75/250 [01:08<02:37,  1.11it/s, loss=0.162]\u001B[A\n",
      "Train step of epoch 0:  30%|███       | 75/250 [01:08<02:37,  1.11it/s, loss=0.222]\u001B[A\n",
      "Train step of epoch 0:  30%|███       | 76/250 [01:09<02:36,  1.11it/s, loss=0.222]\u001B[A\n",
      "Train step of epoch 0:  30%|███       | 76/250 [01:09<02:36,  1.11it/s, loss=0.21] \u001B[A\n",
      "Train step of epoch 0:  31%|███       | 77/250 [01:10<02:35,  1.11it/s, loss=0.21]\u001B[A\n",
      "Train step of epoch 0:  31%|███       | 77/250 [01:10<02:35,  1.11it/s, loss=0.209]\u001B[A\n",
      "Train step of epoch 0:  31%|███       | 78/250 [01:11<02:34,  1.11it/s, loss=0.209]\u001B[A\n",
      "Train step of epoch 0:  31%|███       | 78/250 [01:11<02:34,  1.11it/s, loss=0.0129]\u001B[A\n",
      "Train step of epoch 0:  32%|███▏      | 79/250 [01:12<02:34,  1.11it/s, loss=0.0129]\u001B[A\n",
      "Train step of epoch 0:  32%|███▏      | 79/250 [01:12<02:34,  1.11it/s, loss=0.00393]\u001B[A\n",
      "Train step of epoch 0:  32%|███▏      | 80/250 [01:12<02:33,  1.11it/s, loss=0.00393]\u001B[A\n",
      "Train step of epoch 0:  32%|███▏      | 80/250 [01:12<02:33,  1.11it/s, loss=0.258]  \u001B[A\n",
      "Train step of epoch 0:  32%|███▏      | 81/250 [01:13<02:31,  1.11it/s, loss=0.258]\u001B[A\n",
      "Train step of epoch 0:  32%|███▏      | 81/250 [01:13<02:31,  1.11it/s, loss=0.106]\u001B[A\n",
      "Train step of epoch 0:  33%|███▎      | 82/250 [01:14<02:30,  1.12it/s, loss=0.106]\u001B[A\n",
      "Train step of epoch 0:  33%|███▎      | 82/250 [01:14<02:30,  1.12it/s, loss=0.0652]\u001B[A\n",
      "Train step of epoch 0:  33%|███▎      | 83/250 [01:15<02:29,  1.11it/s, loss=0.0652]\u001B[A\n",
      "Train step of epoch 0:  33%|███▎      | 83/250 [01:15<02:29,  1.11it/s, loss=0.281] \u001B[A\n",
      "Train step of epoch 0:  34%|███▎      | 84/250 [01:16<02:29,  1.11it/s, loss=0.281]\u001B[A\n",
      "Train step of epoch 0:  34%|███▎      | 84/250 [01:16<02:29,  1.11it/s, loss=0.129]\u001B[A\n",
      "Train step of epoch 0:  34%|███▍      | 85/250 [01:17<02:28,  1.11it/s, loss=0.129]\u001B[A\n",
      "Train step of epoch 0:  34%|███▍      | 85/250 [01:17<02:28,  1.11it/s, loss=0.0439]\u001B[A\n",
      "Train step of epoch 0:  34%|███▍      | 86/250 [01:18<02:27,  1.11it/s, loss=0.0439]\u001B[A\n",
      "Train step of epoch 0:  34%|███▍      | 86/250 [01:18<02:27,  1.11it/s, loss=0.0331]\u001B[A\n",
      "Train step of epoch 0:  35%|███▍      | 87/250 [01:19<02:26,  1.12it/s, loss=0.0331]\u001B[A\n",
      "Train step of epoch 0:  35%|███▍      | 87/250 [01:19<02:26,  1.12it/s, loss=0.0176]\u001B[A\n",
      "Train step of epoch 0:  35%|███▌      | 88/250 [01:20<02:24,  1.12it/s, loss=0.0176]\u001B[A\n",
      "Train step of epoch 0:  35%|███▌      | 88/250 [01:20<02:24,  1.12it/s, loss=0.244] \u001B[A\n",
      "Train step of epoch 0:  36%|███▌      | 89/250 [01:20<02:24,  1.11it/s, loss=0.244]\u001B[A\n",
      "Train step of epoch 0:  36%|███▌      | 89/250 [01:21<02:24,  1.11it/s, loss=0.119]\u001B[A\n",
      "Train step of epoch 0:  36%|███▌      | 90/250 [01:21<02:23,  1.11it/s, loss=0.119]\u001B[A\n",
      "Train step of epoch 0:  36%|███▌      | 90/250 [01:21<02:23,  1.11it/s, loss=0.0342]\u001B[A\n",
      "Train step of epoch 0:  36%|███▋      | 91/250 [01:22<02:22,  1.12it/s, loss=0.0342]\u001B[A\n",
      "Train step of epoch 0:  36%|███▋      | 91/250 [01:22<02:22,  1.12it/s, loss=0.15]  \u001B[A\n",
      "Train step of epoch 0:  37%|███▋      | 92/250 [01:23<02:22,  1.11it/s, loss=0.15]\u001B[A\n",
      "Train step of epoch 0:  37%|███▋      | 92/250 [01:23<02:22,  1.11it/s, loss=0.0547]\u001B[A\n",
      "Train step of epoch 0:  37%|███▋      | 93/250 [01:24<02:20,  1.12it/s, loss=0.0547]\u001B[A\n",
      "Train step of epoch 0:  37%|███▋      | 93/250 [01:24<02:20,  1.12it/s, loss=1.5e-5]\u001B[A\n",
      "Train step of epoch 0:  38%|███▊      | 94/250 [01:25<02:19,  1.12it/s, loss=1.5e-5]\u001B[A\n",
      "Train step of epoch 0:  38%|███▊      | 94/250 [01:25<02:19,  1.12it/s, loss=0.151] \u001B[A\n",
      "Train step of epoch 0:  38%|███▊      | 95/250 [01:26<02:18,  1.12it/s, loss=0.151]\u001B[A\n",
      "Train step of epoch 0:  38%|███▊      | 95/250 [01:26<02:18,  1.12it/s, loss=1.23] \u001B[A\n",
      "Train step of epoch 0:  38%|███▊      | 96/250 [01:27<02:18,  1.11it/s, loss=1.23]\u001B[A\n",
      "Train step of epoch 0:  38%|███▊      | 96/250 [01:27<02:18,  1.11it/s, loss=0.00306]\u001B[A\n",
      "Train step of epoch 0:  39%|███▉      | 97/250 [01:28<02:17,  1.11it/s, loss=0.00306]\u001B[A\n",
      "Train step of epoch 0:  39%|███▉      | 97/250 [01:28<02:17,  1.11it/s, loss=0.942]  \u001B[A\n",
      "Train step of epoch 0:  39%|███▉      | 98/250 [01:29<02:16,  1.11it/s, loss=0.942]\u001B[A\n",
      "Train step of epoch 0:  39%|███▉      | 98/250 [01:29<02:16,  1.11it/s, loss=0.0262]\u001B[A\n",
      "Train step of epoch 0:  40%|███▉      | 99/250 [01:29<02:16,  1.11it/s, loss=0.0262]\u001B[A\n",
      "Train step of epoch 0:  40%|███▉      | 99/250 [01:29<02:16,  1.11it/s, loss=0.094] \u001B[A\n",
      "Train step of epoch 0:  40%|████      | 100/250 [01:30<02:14,  1.11it/s, loss=0.094]\u001B[A\n",
      "Train step of epoch 0:  40%|████      | 100/250 [01:30<02:14,  1.11it/s, loss=0.0407]\u001B[A\n",
      "Train step of epoch 0:  40%|████      | 101/250 [01:31<02:14,  1.11it/s, loss=0.0407]\u001B[A\n",
      "Train step of epoch 0:  40%|████      | 101/250 [01:31<02:14,  1.11it/s, loss=0.176] \u001B[A\n",
      "Train step of epoch 0:  41%|████      | 102/250 [01:32<02:13,  1.11it/s, loss=0.176]\u001B[A\n",
      "Train step of epoch 0:  41%|████      | 102/250 [01:32<02:13,  1.11it/s, loss=0.383]\u001B[A\n",
      "Train step of epoch 0:  41%|████      | 103/250 [01:33<02:12,  1.11it/s, loss=0.383]\u001B[A\n",
      "Train step of epoch 0:  41%|████      | 103/250 [01:33<02:12,  1.11it/s, loss=0.229]\u001B[A\n",
      "Train step of epoch 0:  42%|████▏     | 104/250 [01:34<02:12,  1.10it/s, loss=0.229]\u001B[A\n",
      "Train step of epoch 0:  42%|████▏     | 104/250 [01:34<02:12,  1.10it/s, loss=0.274]\u001B[A\n",
      "Train step of epoch 0:  42%|████▏     | 105/250 [01:35<02:11,  1.10it/s, loss=0.274]\u001B[A\n",
      "Train step of epoch 0:  42%|████▏     | 105/250 [01:35<02:11,  1.10it/s, loss=0.223]\u001B[A\n",
      "Train step of epoch 0:  42%|████▏     | 106/250 [01:36<02:10,  1.10it/s, loss=0.223]\u001B[A\n",
      "Train step of epoch 0:  42%|████▏     | 106/250 [01:36<02:10,  1.10it/s, loss=0.0297]\u001B[A\n",
      "Train step of epoch 0:  43%|████▎     | 107/250 [01:37<02:09,  1.10it/s, loss=0.0297]\u001B[A\n",
      "Train step of epoch 0:  43%|████▎     | 107/250 [01:37<02:09,  1.10it/s, loss=0.0737]\u001B[A\n",
      "Train step of epoch 0:  43%|████▎     | 108/250 [01:38<02:09,  1.10it/s, loss=0.0737]\u001B[A\n",
      "Train step of epoch 0:  43%|████▎     | 108/250 [01:38<02:09,  1.10it/s, loss=0.276] \u001B[A\n",
      "Train step of epoch 0:  44%|████▎     | 109/250 [01:39<02:08,  1.10it/s, loss=0.276]\u001B[A\n",
      "Train step of epoch 0:  44%|████▎     | 109/250 [01:39<02:08,  1.10it/s, loss=0.196]\u001B[A\n",
      "Train step of epoch 0:  44%|████▍     | 110/250 [01:39<02:07,  1.10it/s, loss=0.196]\u001B[A\n",
      "Train step of epoch 0:  44%|████▍     | 110/250 [01:39<02:07,  1.10it/s, loss=0.022]\u001B[A\n",
      "Train step of epoch 0:  44%|████▍     | 111/250 [01:40<02:06,  1.10it/s, loss=0.022]\u001B[A\n",
      "Train step of epoch 0:  44%|████▍     | 111/250 [01:40<02:06,  1.10it/s, loss=0.0279]\u001B[A\n",
      "Train step of epoch 0:  45%|████▍     | 112/250 [01:41<02:05,  1.10it/s, loss=0.0279]\u001B[A\n",
      "Train step of epoch 0:  45%|████▍     | 112/250 [01:41<02:05,  1.10it/s, loss=0.141] \u001B[A\n",
      "Train step of epoch 0:  45%|████▌     | 113/250 [01:42<02:04,  1.10it/s, loss=0.141]\u001B[A\n",
      "Train step of epoch 0:  45%|████▌     | 113/250 [01:42<02:04,  1.10it/s, loss=0.226]\u001B[A\n",
      "Train step of epoch 0:  46%|████▌     | 114/250 [01:43<02:03,  1.10it/s, loss=0.226]\u001B[A\n",
      "Train step of epoch 0:  46%|████▌     | 114/250 [01:43<02:03,  1.10it/s, loss=0.428]\u001B[A\n",
      "Train step of epoch 0:  46%|████▌     | 115/250 [01:44<02:02,  1.10it/s, loss=0.428]\u001B[A\n",
      "Train step of epoch 0:  46%|████▌     | 115/250 [01:44<02:02,  1.10it/s, loss=0.142]\u001B[A\n",
      "Train step of epoch 0:  46%|████▋     | 116/250 [01:45<02:01,  1.10it/s, loss=0.142]\u001B[A\n",
      "Train step of epoch 0:  46%|████▋     | 116/250 [01:45<02:01,  1.10it/s, loss=0.000573]\u001B[A\n",
      "Train step of epoch 0:  47%|████▋     | 117/250 [01:46<02:01,  1.10it/s, loss=0.000573]\u001B[A\n",
      "Train step of epoch 0:  47%|████▋     | 117/250 [01:46<02:01,  1.10it/s, loss=0.0901]  \u001B[A\n",
      "Train step of epoch 0:  47%|████▋     | 118/250 [01:47<02:00,  1.10it/s, loss=0.0901]\u001B[A\n",
      "Train step of epoch 0:  47%|████▋     | 118/250 [01:47<02:00,  1.10it/s, loss=0.0841]\u001B[A\n",
      "Train step of epoch 0:  48%|████▊     | 119/250 [01:48<01:59,  1.10it/s, loss=0.0841]\u001B[A\n",
      "Train step of epoch 0:  48%|████▊     | 119/250 [01:48<01:59,  1.10it/s, loss=0.00912]\u001B[A\n",
      "Train step of epoch 0:  48%|████▊     | 120/250 [01:49<01:58,  1.09it/s, loss=0.00912]\u001B[A\n",
      "Train step of epoch 0:  48%|████▊     | 120/250 [01:49<01:58,  1.09it/s, loss=0.202]  \u001B[A\n",
      "Train step of epoch 0:  48%|████▊     | 121/250 [01:49<01:57,  1.10it/s, loss=0.202]\u001B[A\n",
      "Train step of epoch 0:  48%|████▊     | 121/250 [01:49<01:57,  1.10it/s, loss=0.204]\u001B[A\n",
      "Train step of epoch 0:  49%|████▉     | 122/250 [01:50<01:56,  1.10it/s, loss=0.204]\u001B[A\n",
      "Train step of epoch 0:  49%|████▉     | 122/250 [01:50<01:56,  1.10it/s, loss=0.00273]\u001B[A\n",
      "Train step of epoch 0:  49%|████▉     | 123/250 [01:51<01:55,  1.10it/s, loss=0.00273]\u001B[A\n",
      "Train step of epoch 0:  49%|████▉     | 123/250 [01:51<01:55,  1.10it/s, loss=2.12e-5]\u001B[A\n",
      "Train step of epoch 0:  50%|████▉     | 124/250 [01:52<01:55,  1.10it/s, loss=2.12e-5]\u001B[A\n",
      "Train step of epoch 0:  50%|████▉     | 124/250 [01:52<01:55,  1.10it/s, loss=0.0672] \u001B[A\n",
      "Train step of epoch 0:  50%|█████     | 125/250 [01:53<01:54,  1.09it/s, loss=0.0672]\u001B[A\n",
      "Train step of epoch 0:  50%|█████     | 125/250 [01:53<01:54,  1.09it/s, loss=0.0394]\u001B[A\n",
      "Train step of epoch 0:  50%|█████     | 126/250 [01:54<01:53,  1.09it/s, loss=0.0394]\u001B[A\n",
      "Train step of epoch 0:  50%|█████     | 126/250 [01:54<01:53,  1.09it/s, loss=0.0107]\u001B[A\n",
      "Train step of epoch 0:  51%|█████     | 127/250 [01:55<01:51,  1.10it/s, loss=0.0107]\u001B[A\n",
      "Train step of epoch 0:  51%|█████     | 127/250 [01:55<01:51,  1.10it/s, loss=0.225] \u001B[A\n",
      "Train step of epoch 0:  51%|█████     | 128/250 [01:56<01:50,  1.11it/s, loss=0.225]\u001B[A\n",
      "Train step of epoch 0:  51%|█████     | 128/250 [01:56<01:50,  1.11it/s, loss=0.00471]\u001B[A\n",
      "Train step of epoch 0:  52%|█████▏    | 129/250 [01:57<01:49,  1.11it/s, loss=0.00471]\u001B[A\n",
      "Train step of epoch 0:  52%|█████▏    | 129/250 [01:57<01:49,  1.11it/s, loss=0.0611] \u001B[A\n",
      "Train step of epoch 0:  52%|█████▏    | 130/250 [01:58<01:47,  1.12it/s, loss=0.0611]\u001B[A\n",
      "Train step of epoch 0:  52%|█████▏    | 130/250 [01:58<01:47,  1.12it/s, loss=2.98e-8]\u001B[A\n",
      "Train step of epoch 0:  52%|█████▏    | 131/250 [01:59<01:47,  1.11it/s, loss=2.98e-8]\u001B[A\n",
      "Train step of epoch 0:  52%|█████▏    | 131/250 [01:59<01:47,  1.11it/s, loss=1.96]   \u001B[A\n",
      "Train step of epoch 0:  53%|█████▎    | 132/250 [01:59<01:45,  1.12it/s, loss=1.96]\u001B[A\n",
      "Train step of epoch 0:  53%|█████▎    | 132/250 [01:59<01:45,  1.12it/s, loss=0.0265]\u001B[A\n",
      "Train step of epoch 0:  53%|█████▎    | 133/250 [02:00<01:45,  1.11it/s, loss=0.0265]\u001B[A\n",
      "Train step of epoch 0:  53%|█████▎    | 133/250 [02:00<01:45,  1.11it/s, loss=1.43]  \u001B[A\n",
      "Train step of epoch 0:  54%|█████▎    | 134/250 [02:01<01:45,  1.10it/s, loss=1.43]\u001B[A\n",
      "Train step of epoch 0:  54%|█████▎    | 134/250 [02:01<01:45,  1.10it/s, loss=0.685]\u001B[A\n",
      "Train step of epoch 0:  54%|█████▍    | 135/250 [02:02<01:44,  1.10it/s, loss=0.685]\u001B[A\n",
      "Train step of epoch 0:  54%|█████▍    | 135/250 [02:02<01:44,  1.10it/s, loss=0.247]\u001B[A\n",
      "Train step of epoch 0:  54%|█████▍    | 136/250 [02:03<01:43,  1.10it/s, loss=0.247]\u001B[A\n",
      "Train step of epoch 0:  54%|█████▍    | 136/250 [02:03<01:43,  1.10it/s, loss=0.228]\u001B[A\n",
      "Train step of epoch 0:  55%|█████▍    | 137/250 [02:04<01:43,  1.10it/s, loss=0.228]\u001B[A\n",
      "Train step of epoch 0:  55%|█████▍    | 137/250 [02:04<01:43,  1.10it/s, loss=0.11] \u001B[A\n",
      "Train step of epoch 0:  55%|█████▌    | 138/250 [02:05<01:42,  1.10it/s, loss=0.11]\u001B[A\n",
      "Train step of epoch 0:  55%|█████▌    | 138/250 [02:05<01:42,  1.10it/s, loss=0.22]\u001B[A\n",
      "Train step of epoch 0:  56%|█████▌    | 139/250 [02:06<01:41,  1.10it/s, loss=0.22]\u001B[A\n",
      "Train step of epoch 0:  56%|█████▌    | 139/250 [02:06<01:41,  1.10it/s, loss=0.45]\u001B[A\n",
      "Train step of epoch 0:  56%|█████▌    | 140/250 [02:07<01:40,  1.10it/s, loss=0.45]\u001B[A\n",
      "Train step of epoch 0:  56%|█████▌    | 140/250 [02:07<01:40,  1.10it/s, loss=0.444]\u001B[A\n",
      "Train step of epoch 0:  56%|█████▋    | 141/250 [02:08<01:39,  1.09it/s, loss=0.444]\u001B[A\n",
      "Train step of epoch 0:  56%|█████▋    | 141/250 [02:08<01:39,  1.09it/s, loss=0.345]\u001B[A\n",
      "Train step of epoch 0:  57%|█████▋    | 142/250 [02:09<01:38,  1.09it/s, loss=0.345]\u001B[A\n",
      "Train step of epoch 0:  57%|█████▋    | 142/250 [02:09<01:38,  1.09it/s, loss=0.537]\u001B[A\n",
      "Train step of epoch 0:  57%|█████▋    | 143/250 [02:09<01:37,  1.09it/s, loss=0.537]\u001B[A\n",
      "Train step of epoch 0:  57%|█████▋    | 143/250 [02:09<01:37,  1.09it/s, loss=0.236]\u001B[A\n",
      "Train step of epoch 0:  58%|█████▊    | 144/250 [02:10<01:36,  1.10it/s, loss=0.236]\u001B[A\n",
      "Train step of epoch 0:  58%|█████▊    | 144/250 [02:10<01:36,  1.10it/s, loss=0.34] \u001B[A\n",
      "Train step of epoch 0:  58%|█████▊    | 145/250 [02:11<01:35,  1.10it/s, loss=0.34]\u001B[A\n",
      "Train step of epoch 0:  58%|█████▊    | 145/250 [02:11<01:35,  1.10it/s, loss=0.194]\u001B[A\n",
      "Train step of epoch 0:  58%|█████▊    | 146/250 [02:12<01:34,  1.10it/s, loss=0.194]\u001B[A\n",
      "Train step of epoch 0:  58%|█████▊    | 146/250 [02:12<01:34,  1.10it/s, loss=0.113]\u001B[A\n",
      "Train step of epoch 0:  59%|█████▉    | 147/250 [02:13<01:33,  1.10it/s, loss=0.113]\u001B[A\n",
      "Train step of epoch 0:  59%|█████▉    | 147/250 [02:13<01:33,  1.10it/s, loss=0.22] \u001B[A\n",
      "Train step of epoch 0:  59%|█████▉    | 148/250 [02:14<01:33,  1.10it/s, loss=0.22]\u001B[A\n",
      "Train step of epoch 0:  59%|█████▉    | 148/250 [02:14<01:33,  1.10it/s, loss=0.136]\u001B[A\n",
      "Train step of epoch 0:  60%|█████▉    | 149/250 [02:15<01:32,  1.10it/s, loss=0.136]\u001B[A\n",
      "Train step of epoch 0:  60%|█████▉    | 149/250 [02:15<01:32,  1.10it/s, loss=0.521]\u001B[A\n",
      "Train step of epoch 0:  60%|██████    | 150/250 [02:16<01:31,  1.10it/s, loss=0.521]\u001B[A\n",
      "Train step of epoch 0:  60%|██████    | 150/250 [02:16<01:31,  1.10it/s, loss=0.171]\u001B[A\n",
      "Train step of epoch 0:  60%|██████    | 151/250 [02:17<01:30,  1.10it/s, loss=0.171]\u001B[A\n",
      "Train step of epoch 0:  60%|██████    | 151/250 [02:17<01:30,  1.10it/s, loss=0.286]\u001B[A\n",
      "Train step of epoch 0:  61%|██████    | 152/250 [02:18<01:29,  1.09it/s, loss=0.286]\u001B[A\n",
      "Train step of epoch 0:  61%|██████    | 152/250 [02:18<01:29,  1.09it/s, loss=0.139]\u001B[A\n",
      "Train step of epoch 0:  61%|██████    | 153/250 [02:19<01:28,  1.10it/s, loss=0.139]\u001B[A\n",
      "Train step of epoch 0:  61%|██████    | 153/250 [02:19<01:28,  1.10it/s, loss=0.147]\u001B[A\n",
      "Train step of epoch 0:  62%|██████▏   | 154/250 [02:20<01:27,  1.10it/s, loss=0.147]\u001B[A\n",
      "Train step of epoch 0:  62%|██████▏   | 154/250 [02:20<01:27,  1.10it/s, loss=0.163]\u001B[A\n",
      "Train step of epoch 0:  62%|██████▏   | 155/250 [02:20<01:26,  1.10it/s, loss=0.163]\u001B[A\n",
      "Train step of epoch 0:  62%|██████▏   | 155/250 [02:20<01:26,  1.10it/s, loss=0.161]\u001B[A\n",
      "Train step of epoch 0:  62%|██████▏   | 156/250 [02:21<01:25,  1.10it/s, loss=0.161]\u001B[A\n",
      "Train step of epoch 0:  62%|██████▏   | 156/250 [02:21<01:25,  1.10it/s, loss=0.483]\u001B[A\n",
      "Train step of epoch 0:  63%|██████▎   | 157/250 [02:22<01:24,  1.10it/s, loss=0.483]\u001B[A\n",
      "Train step of epoch 0:  63%|██████▎   | 157/250 [02:22<01:24,  1.10it/s, loss=0.114]\u001B[A\n",
      "Train step of epoch 0:  63%|██████▎   | 158/250 [02:23<01:23,  1.10it/s, loss=0.114]\u001B[A\n",
      "Train step of epoch 0:  63%|██████▎   | 158/250 [02:23<01:23,  1.10it/s, loss=0.145]\u001B[A\n",
      "Train step of epoch 0:  64%|██████▎   | 159/250 [02:24<01:22,  1.10it/s, loss=0.145]\u001B[A\n",
      "Train step of epoch 0:  64%|██████▎   | 159/250 [02:24<01:22,  1.10it/s, loss=0.16] \u001B[A\n",
      "Train step of epoch 0:  64%|██████▍   | 160/250 [02:25<01:21,  1.10it/s, loss=0.16]\u001B[A\n",
      "Train step of epoch 0:  64%|██████▍   | 160/250 [02:25<01:21,  1.10it/s, loss=0.078]\u001B[A\n",
      "Train step of epoch 0:  64%|██████▍   | 161/250 [02:26<01:20,  1.10it/s, loss=0.078]\u001B[A\n",
      "Train step of epoch 0:  64%|██████▍   | 161/250 [02:26<01:20,  1.10it/s, loss=0.6]  \u001B[A\n",
      "Train step of epoch 0:  65%|██████▍   | 162/250 [02:27<01:19,  1.10it/s, loss=0.6]\u001B[A\n",
      "Train step of epoch 0:  65%|██████▍   | 162/250 [02:27<01:19,  1.10it/s, loss=0.0504]\u001B[A\n",
      "Train step of epoch 0:  65%|██████▌   | 163/250 [02:28<01:19,  1.10it/s, loss=0.0504]\u001B[A\n",
      "Train step of epoch 0:  65%|██████▌   | 163/250 [02:28<01:19,  1.10it/s, loss=0.0453]\u001B[A\n",
      "Train step of epoch 0:  66%|██████▌   | 164/250 [02:29<01:18,  1.10it/s, loss=0.0453]\u001B[A\n",
      "Train step of epoch 0:  66%|██████▌   | 164/250 [02:29<01:18,  1.10it/s, loss=0.0412]\u001B[A\n",
      "Train step of epoch 0:  66%|██████▌   | 165/250 [02:30<01:17,  1.10it/s, loss=0.0412]\u001B[A\n",
      "Train step of epoch 0:  66%|██████▌   | 165/250 [02:30<01:17,  1.10it/s, loss=0.0702]\u001B[A\n",
      "Train step of epoch 0:  66%|██████▋   | 166/250 [02:30<01:16,  1.10it/s, loss=0.0702]\u001B[A\n",
      "Train step of epoch 0:  66%|██████▋   | 166/250 [02:30<01:16,  1.10it/s, loss=0.00483]\u001B[A\n",
      "Train step of epoch 0:  67%|██████▋   | 167/250 [02:31<01:15,  1.10it/s, loss=0.00483]\u001B[A\n",
      "Train step of epoch 0:  67%|██████▋   | 167/250 [02:31<01:15,  1.10it/s, loss=0.00201]\u001B[A\n",
      "Train step of epoch 0:  67%|██████▋   | 168/250 [02:32<01:14,  1.10it/s, loss=0.00201]\u001B[A\n",
      "Train step of epoch 0:  67%|██████▋   | 168/250 [02:32<01:14,  1.10it/s, loss=0.123]  \u001B[A\n",
      "Train step of epoch 0:  68%|██████▊   | 169/250 [02:33<01:13,  1.10it/s, loss=0.123]\u001B[A\n",
      "Train step of epoch 0:  68%|██████▊   | 169/250 [02:33<01:13,  1.10it/s, loss=0.415]\u001B[A\n",
      "Train step of epoch 0:  68%|██████▊   | 170/250 [02:34<01:12,  1.10it/s, loss=0.415]\u001B[A\n",
      "Train step of epoch 0:  68%|██████▊   | 170/250 [02:34<01:12,  1.10it/s, loss=0.0393]\u001B[A\n",
      "Train step of epoch 0:  68%|██████▊   | 171/250 [02:35<01:11,  1.10it/s, loss=0.0393]\u001B[A\n",
      "Train step of epoch 0:  68%|██████▊   | 171/250 [02:35<01:11,  1.10it/s, loss=0.0646]\u001B[A\n",
      "Train step of epoch 0:  69%|██████▉   | 172/250 [02:36<01:11,  1.10it/s, loss=0.0646]\u001B[A\n",
      "Train step of epoch 0:  69%|██████▉   | 172/250 [02:36<01:11,  1.10it/s, loss=0.393] \u001B[A\n",
      "Train step of epoch 0:  69%|██████▉   | 173/250 [02:37<01:10,  1.10it/s, loss=0.393]\u001B[A\n",
      "Train step of epoch 0:  69%|██████▉   | 173/250 [02:37<01:10,  1.10it/s, loss=0.543]\u001B[A\n",
      "Train step of epoch 0:  70%|██████▉   | 174/250 [02:38<01:09,  1.10it/s, loss=0.543]\u001B[A\n",
      "Train step of epoch 0:  70%|██████▉   | 174/250 [02:38<01:09,  1.10it/s, loss=1.03] \u001B[A\n",
      "Train step of epoch 0:  70%|███████   | 175/250 [02:39<01:08,  1.10it/s, loss=1.03]\u001B[A\n",
      "Train step of epoch 0:  70%|███████   | 175/250 [02:39<01:08,  1.10it/s, loss=0.0425]\u001B[A\n",
      "Train step of epoch 0:  70%|███████   | 176/250 [02:40<01:07,  1.10it/s, loss=0.0425]\u001B[A\n",
      "Train step of epoch 0:  70%|███████   | 176/250 [02:40<01:07,  1.10it/s, loss=0.223] \u001B[A\n",
      "Train step of epoch 0:  71%|███████   | 177/250 [02:40<01:06,  1.10it/s, loss=0.223]\u001B[A\n",
      "Train step of epoch 0:  71%|███████   | 177/250 [02:40<01:06,  1.10it/s, loss=0.224]\u001B[A\n",
      "Train step of epoch 0:  71%|███████   | 178/250 [02:41<01:05,  1.10it/s, loss=0.224]\u001B[A\n",
      "Train step of epoch 0:  71%|███████   | 178/250 [02:41<01:05,  1.10it/s, loss=0.00198]\u001B[A\n",
      "Train step of epoch 0:  72%|███████▏  | 179/250 [02:42<01:04,  1.10it/s, loss=0.00198]\u001B[A\n",
      "Train step of epoch 0:  72%|███████▏  | 179/250 [02:42<01:04,  1.10it/s, loss=0.32]   \u001B[A\n",
      "Train step of epoch 0:  72%|███████▏  | 180/250 [02:43<01:03,  1.10it/s, loss=0.32]\u001B[A\n",
      "Train step of epoch 0:  72%|███████▏  | 180/250 [02:43<01:03,  1.10it/s, loss=0.0961]\u001B[A\n",
      "Train step of epoch 0:  72%|███████▏  | 181/250 [02:44<01:02,  1.10it/s, loss=0.0961]\u001B[A\n",
      "Train step of epoch 0:  72%|███████▏  | 181/250 [02:44<01:02,  1.10it/s, loss=0.0126]\u001B[A\n",
      "Train step of epoch 0:  73%|███████▎  | 182/250 [02:45<01:01,  1.10it/s, loss=0.0126]\u001B[A\n",
      "Train step of epoch 0:  73%|███████▎  | 182/250 [02:45<01:01,  1.10it/s, loss=0.0263]\u001B[A\n",
      "Train step of epoch 0:  73%|███████▎  | 183/250 [02:46<01:00,  1.10it/s, loss=0.0263]\u001B[A\n",
      "Train step of epoch 0:  73%|███████▎  | 183/250 [02:46<01:00,  1.10it/s, loss=0.928] \u001B[A\n",
      "Train step of epoch 0:  74%|███████▎  | 184/250 [02:47<01:00,  1.10it/s, loss=0.928]\u001B[A\n",
      "Train step of epoch 0:  74%|███████▎  | 184/250 [02:47<01:00,  1.10it/s, loss=0.00827]\u001B[A\n",
      "Train step of epoch 0:  74%|███████▍  | 185/250 [02:48<00:59,  1.10it/s, loss=0.00827]\u001B[A\n",
      "Train step of epoch 0:  74%|███████▍  | 185/250 [02:48<00:59,  1.10it/s, loss=0.2]    \u001B[A\n",
      "Train step of epoch 0:  74%|███████▍  | 186/250 [02:49<00:58,  1.10it/s, loss=0.2]\u001B[A\n",
      "Train step of epoch 0:  74%|███████▍  | 186/250 [02:49<00:58,  1.10it/s, loss=0.44]\u001B[A\n",
      "Train step of epoch 0:  75%|███████▍  | 187/250 [02:50<00:57,  1.10it/s, loss=0.44]\u001B[A\n",
      "Train step of epoch 0:  75%|███████▍  | 187/250 [02:50<00:57,  1.10it/s, loss=0.259]\u001B[A\n",
      "Train step of epoch 0:  75%|███████▌  | 188/250 [02:50<00:56,  1.10it/s, loss=0.259]\u001B[A\n",
      "Train step of epoch 0:  75%|███████▌  | 188/250 [02:50<00:56,  1.10it/s, loss=0.179]\u001B[A\n",
      "Train step of epoch 0:  76%|███████▌  | 189/250 [02:51<00:55,  1.10it/s, loss=0.179]\u001B[A\n",
      "Train step of epoch 0:  76%|███████▌  | 189/250 [02:51<00:55,  1.10it/s, loss=0.000424]\u001B[A\n",
      "Train step of epoch 0:  76%|███████▌  | 190/250 [02:52<00:54,  1.10it/s, loss=0.000424]\u001B[A\n",
      "Train step of epoch 0:  76%|███████▌  | 190/250 [02:52<00:54,  1.10it/s, loss=0.204]   \u001B[A\n",
      "Train step of epoch 0:  76%|███████▋  | 191/250 [02:53<00:53,  1.10it/s, loss=0.204]\u001B[A\n",
      "Train step of epoch 0:  76%|███████▋  | 191/250 [02:53<00:53,  1.10it/s, loss=0.0706]\u001B[A\n",
      "Train step of epoch 0:  77%|███████▋  | 192/250 [02:54<00:52,  1.10it/s, loss=0.0706]\u001B[A\n",
      "Train step of epoch 0:  77%|███████▋  | 192/250 [02:54<00:52,  1.10it/s, loss=0.1]   \u001B[A\n",
      "Train step of epoch 0:  77%|███████▋  | 193/250 [02:55<00:51,  1.10it/s, loss=0.1]\u001B[A\n",
      "Train step of epoch 0:  77%|███████▋  | 193/250 [02:55<00:51,  1.10it/s, loss=0.00065]\u001B[A\n",
      "Train step of epoch 0:  78%|███████▊  | 194/250 [02:56<00:51,  1.10it/s, loss=0.00065]\u001B[A\n",
      "Train step of epoch 0:  78%|███████▊  | 194/250 [02:56<00:51,  1.10it/s, loss=0.092]  \u001B[A\n",
      "Train step of epoch 0:  78%|███████▊  | 195/250 [02:57<00:50,  1.10it/s, loss=0.092]\u001B[A\n",
      "Train step of epoch 0:  78%|███████▊  | 195/250 [02:57<00:50,  1.10it/s, loss=0.09] \u001B[A\n",
      "Train step of epoch 0:  78%|███████▊  | 196/250 [02:58<00:49,  1.10it/s, loss=0.09]\u001B[A\n",
      "Train step of epoch 0:  78%|███████▊  | 196/250 [02:58<00:49,  1.10it/s, loss=0.0604]\u001B[A\n",
      "Train step of epoch 0:  79%|███████▉  | 197/250 [02:59<00:48,  1.10it/s, loss=0.0604]\u001B[A\n",
      "Train step of epoch 0:  79%|███████▉  | 197/250 [02:59<00:48,  1.10it/s, loss=0.0118]\u001B[A\n",
      "Train step of epoch 0:  79%|███████▉  | 198/250 [03:00<00:47,  1.10it/s, loss=0.0118]\u001B[A\n",
      "Train step of epoch 0:  79%|███████▉  | 198/250 [03:00<00:47,  1.10it/s, loss=0.665] \u001B[A\n",
      "Train step of epoch 0:  80%|███████▉  | 199/250 [03:00<00:46,  1.10it/s, loss=0.665]\u001B[A\n",
      "Train step of epoch 0:  80%|███████▉  | 199/250 [03:00<00:46,  1.10it/s, loss=0.188]\u001B[A\n",
      "Train step of epoch 0:  80%|████████  | 200/250 [03:01<00:45,  1.10it/s, loss=0.188]\u001B[A\n",
      "Train step of epoch 0:  80%|████████  | 200/250 [03:01<00:45,  1.10it/s, loss=0.0117]\u001B[A\n",
      "Train step of epoch 0:  80%|████████  | 201/250 [03:02<00:44,  1.10it/s, loss=0.0117]\u001B[A\n",
      "Train step of epoch 0:  80%|████████  | 201/250 [03:02<00:44,  1.10it/s, loss=0.0174]\u001B[A\n",
      "Train step of epoch 0:  81%|████████  | 202/250 [03:03<00:43,  1.10it/s, loss=0.0174]\u001B[A\n",
      "Train step of epoch 0:  81%|████████  | 202/250 [03:03<00:43,  1.10it/s, loss=0.00418]\u001B[A\n",
      "Train step of epoch 0:  81%|████████  | 203/250 [03:04<00:42,  1.10it/s, loss=0.00418]\u001B[A\n",
      "Train step of epoch 0:  81%|████████  | 203/250 [03:04<00:42,  1.10it/s, loss=0.176]  \u001B[A\n",
      "Train step of epoch 0:  82%|████████▏ | 204/250 [03:05<00:41,  1.10it/s, loss=0.176]\u001B[A\n",
      "Train step of epoch 0:  82%|████████▏ | 204/250 [03:05<00:41,  1.10it/s, loss=0.00185]\u001B[A\n",
      "Train step of epoch 0:  82%|████████▏ | 205/250 [03:06<00:40,  1.10it/s, loss=0.00185]\u001B[A\n",
      "Train step of epoch 0:  82%|████████▏ | 205/250 [03:06<00:40,  1.10it/s, loss=0.201]  \u001B[A\n",
      "Train step of epoch 0:  82%|████████▏ | 206/250 [03:07<00:40,  1.10it/s, loss=0.201]\u001B[A\n",
      "Train step of epoch 0:  82%|████████▏ | 206/250 [03:07<00:40,  1.10it/s, loss=0.11] \u001B[A\n",
      "Train step of epoch 0:  83%|████████▎ | 207/250 [03:08<00:39,  1.10it/s, loss=0.11]\u001B[A\n",
      "Train step of epoch 0:  83%|████████▎ | 207/250 [03:08<00:39,  1.10it/s, loss=0.00257]\u001B[A\n",
      "Train step of epoch 0:  83%|████████▎ | 208/250 [03:09<00:38,  1.10it/s, loss=0.00257]\u001B[A\n",
      "Train step of epoch 0:  83%|████████▎ | 208/250 [03:09<00:38,  1.10it/s, loss=0.0365] \u001B[A\n",
      "Train step of epoch 0:  84%|████████▎ | 209/250 [03:10<00:37,  1.10it/s, loss=0.0365]\u001B[A\n",
      "Train step of epoch 0:  84%|████████▎ | 209/250 [03:10<00:37,  1.10it/s, loss=0.00458]\u001B[A\n",
      "Train step of epoch 0:  84%|████████▍ | 210/250 [03:10<00:36,  1.10it/s, loss=0.00458]\u001B[A\n",
      "Train step of epoch 0:  84%|████████▍ | 210/250 [03:10<00:36,  1.10it/s, loss=0.201]  \u001B[A\n",
      "Train step of epoch 0:  84%|████████▍ | 211/250 [03:11<00:35,  1.10it/s, loss=0.201]\u001B[A\n",
      "Train step of epoch 0:  84%|████████▍ | 211/250 [03:11<00:35,  1.10it/s, loss=0.0335]\u001B[A\n",
      "Train step of epoch 0:  85%|████████▍ | 212/250 [03:12<00:34,  1.10it/s, loss=0.0335]\u001B[A\n",
      "Train step of epoch 0:  85%|████████▍ | 212/250 [03:12<00:34,  1.10it/s, loss=0.0688]\u001B[A\n",
      "Train step of epoch 0:  85%|████████▌ | 213/250 [03:13<00:33,  1.10it/s, loss=0.0688]\u001B[A\n",
      "Train step of epoch 0:  85%|████████▌ | 213/250 [03:13<00:33,  1.10it/s, loss=0.00515]\u001B[A\n",
      "Train step of epoch 0:  86%|████████▌ | 214/250 [03:14<00:32,  1.10it/s, loss=0.00515]\u001B[A\n",
      "Train step of epoch 0:  86%|████████▌ | 214/250 [03:14<00:32,  1.10it/s, loss=0.425]  \u001B[A\n",
      "Train step of epoch 0:  86%|████████▌ | 215/250 [03:15<00:31,  1.10it/s, loss=0.425]\u001B[A\n",
      "Train step of epoch 0:  86%|████████▌ | 215/250 [03:15<00:31,  1.10it/s, loss=0.13] \u001B[A\n",
      "Train step of epoch 0:  86%|████████▋ | 216/250 [03:16<00:30,  1.10it/s, loss=0.13]\u001B[A\n",
      "Train step of epoch 0:  86%|████████▋ | 216/250 [03:16<00:30,  1.10it/s, loss=0.465]\u001B[A\n",
      "Train step of epoch 0:  87%|████████▋ | 217/250 [03:17<00:30,  1.10it/s, loss=0.465]\u001B[A\n",
      "Train step of epoch 0:  87%|████████▋ | 217/250 [03:17<00:30,  1.10it/s, loss=0.356]\u001B[A\n",
      "Train step of epoch 0:  87%|████████▋ | 218/250 [03:18<00:29,  1.10it/s, loss=0.356]\u001B[A\n",
      "Train step of epoch 0:  87%|████████▋ | 218/250 [03:18<00:29,  1.10it/s, loss=0.31] \u001B[A\n",
      "Train step of epoch 0:  88%|████████▊ | 219/250 [03:19<00:28,  1.10it/s, loss=0.31]\u001B[A\n",
      "Train step of epoch 0:  88%|████████▊ | 219/250 [03:19<00:28,  1.10it/s, loss=0.17]\u001B[A\n",
      "Train step of epoch 0:  88%|████████▊ | 220/250 [03:20<00:27,  1.10it/s, loss=0.17]\u001B[A\n",
      "Train step of epoch 0:  88%|████████▊ | 220/250 [03:20<00:27,  1.10it/s, loss=0.0856]\u001B[A\n",
      "Train step of epoch 0:  88%|████████▊ | 221/250 [03:20<00:26,  1.10it/s, loss=0.0856]\u001B[A\n",
      "Train step of epoch 0:  88%|████████▊ | 221/250 [03:20<00:26,  1.10it/s, loss=0.158] \u001B[A\n",
      "Train step of epoch 0:  89%|████████▉ | 222/250 [03:21<00:25,  1.10it/s, loss=0.158]\u001B[A\n",
      "Train step of epoch 0:  89%|████████▉ | 222/250 [03:21<00:25,  1.10it/s, loss=0.0376]\u001B[A\n",
      "Train step of epoch 0:  89%|████████▉ | 223/250 [03:22<00:24,  1.10it/s, loss=0.0376]\u001B[A\n",
      "Train step of epoch 0:  89%|████████▉ | 223/250 [03:22<00:24,  1.10it/s, loss=0.0684]\u001B[A\n",
      "Train step of epoch 0:  90%|████████▉ | 224/250 [03:23<00:23,  1.10it/s, loss=0.0684]\u001B[A\n",
      "Train step of epoch 0:  90%|████████▉ | 224/250 [03:23<00:23,  1.10it/s, loss=0.0311]\u001B[A\n",
      "Train step of epoch 0:  90%|█████████ | 225/250 [03:24<00:22,  1.11it/s, loss=0.0311]\u001B[A\n",
      "Train step of epoch 0:  90%|█████████ | 225/250 [03:24<00:22,  1.11it/s, loss=0.0941]\u001B[A\n",
      "Train step of epoch 0:  90%|█████████ | 226/250 [03:25<00:21,  1.10it/s, loss=0.0941]\u001B[A\n",
      "Train step of epoch 0:  90%|█████████ | 226/250 [03:25<00:21,  1.10it/s, loss=0.0234]\u001B[A\n",
      "Train step of epoch 0:  91%|█████████ | 227/250 [03:26<00:20,  1.10it/s, loss=0.0234]\u001B[A\n",
      "Train step of epoch 0:  91%|█████████ | 227/250 [03:26<00:20,  1.10it/s, loss=0.41]  \u001B[A\n",
      "Train step of epoch 0:  91%|█████████ | 228/250 [03:27<00:20,  1.10it/s, loss=0.41]\u001B[A\n",
      "Train step of epoch 0:  91%|█████████ | 228/250 [03:27<00:20,  1.10it/s, loss=0.00351]\u001B[A\n",
      "Train step of epoch 0:  92%|█████████▏| 229/250 [03:28<00:19,  1.10it/s, loss=0.00351]\u001B[A\n",
      "Train step of epoch 0:  92%|█████████▏| 229/250 [03:28<00:19,  1.10it/s, loss=0.122]  \u001B[A\n",
      "Train step of epoch 0:  92%|█████████▏| 230/250 [03:29<00:18,  1.10it/s, loss=0.122]\u001B[A\n",
      "Train step of epoch 0:  92%|█████████▏| 230/250 [03:29<00:18,  1.10it/s, loss=0.163]\u001B[A\n",
      "Train step of epoch 0:  92%|█████████▏| 231/250 [03:30<00:17,  1.10it/s, loss=0.163]\u001B[A\n",
      "Train step of epoch 0:  92%|█████████▏| 231/250 [03:30<00:17,  1.10it/s, loss=0.00236]\u001B[A\n",
      "Train step of epoch 0:  93%|█████████▎| 232/250 [03:30<00:16,  1.11it/s, loss=0.00236]\u001B[A\n",
      "Train step of epoch 0:  93%|█████████▎| 232/250 [03:30<00:16,  1.11it/s, loss=0.000109]\u001B[A\n",
      "Train step of epoch 0:  93%|█████████▎| 233/250 [03:31<00:15,  1.10it/s, loss=0.000109]\u001B[A\n",
      "Train step of epoch 0:  93%|█████████▎| 233/250 [03:31<00:15,  1.10it/s, loss=0.0684]  \u001B[A\n",
      "Train step of epoch 0:  94%|█████████▎| 234/250 [03:32<00:14,  1.11it/s, loss=0.0684]\u001B[A\n",
      "Train step of epoch 0:  94%|█████████▎| 234/250 [03:32<00:14,  1.11it/s, loss=1.95]  \u001B[A\n",
      "Train step of epoch 0:  94%|█████████▍| 235/250 [03:33<00:13,  1.10it/s, loss=1.95]\u001B[A\n",
      "Train step of epoch 0:  94%|█████████▍| 235/250 [03:33<00:13,  1.10it/s, loss=0.022]\u001B[A\n",
      "Train step of epoch 0:  94%|█████████▍| 236/250 [03:34<00:12,  1.10it/s, loss=0.022]\u001B[A\n",
      "Train step of epoch 0:  94%|█████████▍| 236/250 [03:34<00:12,  1.10it/s, loss=0.104]\u001B[A\n",
      "Train step of epoch 0:  95%|█████████▍| 237/250 [03:35<00:11,  1.10it/s, loss=0.104]\u001B[A\n",
      "Train step of epoch 0:  95%|█████████▍| 237/250 [03:35<00:11,  1.10it/s, loss=0.177]\u001B[A\n",
      "Train step of epoch 0:  95%|█████████▌| 238/250 [03:36<00:10,  1.10it/s, loss=0.177]\u001B[A\n",
      "Train step of epoch 0:  95%|█████████▌| 238/250 [03:36<00:10,  1.10it/s, loss=0.139]\u001B[A\n",
      "Train step of epoch 0:  96%|█████████▌| 239/250 [03:37<00:10,  1.10it/s, loss=0.139]\u001B[A\n",
      "Train step of epoch 0:  96%|█████████▌| 239/250 [03:37<00:10,  1.10it/s, loss=0.201]\u001B[A\n",
      "Train step of epoch 0:  96%|█████████▌| 240/250 [03:38<00:09,  1.10it/s, loss=0.201]\u001B[A\n",
      "Train step of epoch 0:  96%|█████████▌| 240/250 [03:38<00:09,  1.10it/s, loss=0.307]\u001B[A\n",
      "Train step of epoch 0:  96%|█████████▋| 241/250 [03:39<00:08,  1.10it/s, loss=0.307]\u001B[A\n",
      "Train step of epoch 0:  96%|█████████▋| 241/250 [03:39<00:08,  1.10it/s, loss=0.0151]\u001B[A\n",
      "Train step of epoch 0:  97%|█████████▋| 242/250 [03:40<00:07,  1.10it/s, loss=0.0151]\u001B[A\n",
      "Train step of epoch 0:  97%|█████████▋| 242/250 [03:40<00:07,  1.10it/s, loss=0.388] \u001B[A\n",
      "Train step of epoch 0:  97%|█████████▋| 243/250 [03:40<00:06,  1.10it/s, loss=0.388]\u001B[A\n",
      "Train step of epoch 0:  97%|█████████▋| 243/250 [03:40<00:06,  1.10it/s, loss=0.0114]\u001B[A\n",
      "Train step of epoch 0:  98%|█████████▊| 244/250 [03:41<00:05,  1.10it/s, loss=0.0114]\u001B[A\n",
      "Train step of epoch 0:  98%|█████████▊| 244/250 [03:41<00:05,  1.10it/s, loss=0.0124]\u001B[A\n",
      "Train step of epoch 0:  98%|█████████▊| 245/250 [03:42<00:04,  1.10it/s, loss=0.0124]\u001B[A\n",
      "Train step of epoch 0:  98%|█████████▊| 245/250 [03:42<00:04,  1.10it/s, loss=0.284] \u001B[A\n",
      "Train step of epoch 0:  98%|█████████▊| 246/250 [03:43<00:03,  1.10it/s, loss=0.284]\u001B[A\n",
      "Train step of epoch 0:  98%|█████████▊| 246/250 [03:43<00:03,  1.10it/s, loss=0.178]\u001B[A\n",
      "Train step of epoch 0:  99%|█████████▉| 247/250 [03:44<00:02,  1.10it/s, loss=0.178]\u001B[A\n",
      "Train step of epoch 0:  99%|█████████▉| 247/250 [03:44<00:02,  1.10it/s, loss=0.179]\u001B[A\n",
      "Train step of epoch 0:  99%|█████████▉| 248/250 [03:45<00:01,  1.10it/s, loss=0.179]\u001B[A\n",
      "Train step of epoch 0:  99%|█████████▉| 248/250 [03:45<00:01,  1.10it/s, loss=0.641]\u001B[A\n",
      "Train step of epoch 0: 100%|█████████▉| 249/250 [03:46<00:00,  1.10it/s, loss=0.641]\u001B[A\n",
      "Train step of epoch 0: 100%|█████████▉| 249/250 [03:46<00:00,  1.10it/s, loss=0.00542]\u001B[A\n",
      "Train step of epoch 0: 100%|██████████| 250/250 [03:47<00:00,  1.10it/s, loss=0.00542]\u001B[A\n",
      "Train epoch: 100%|██████████| 1/1 [04:02<00:00, 242.31s/it]0,  1.10it/s, loss=0.0107] \u001B[A\n",
      "Train step of epoch 0: 100%|██████████| 250/250 [04:02<00:00,  1.03it/s, loss=0.175, dist_mean=5.68]\u001B[A\n",
      "Train epoch: 100%|██████████| 1/1 [04:02<00:00, 242.31s/it]\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(use_lora=0)\n",
    "\n",
    "model.save_pretrained('aiffel/KoChatGPT/output_2_RM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "37966e20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: 인공지능은 똥멍청이 입니다\n",
      "reward score: -1.0\n"
     ]
    }
   ],
   "source": [
    "def inference_RM(input_text):\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors='pt').to(\n",
    "        torch.cuda.current_device())\n",
    "    output = model(input_ids)\n",
    "    output_reward = output.cpu().detach().numpy()[0]\n",
    "\n",
    "    print('input: %s\\nreward score: %.1f'%(input_text, output_reward))\n",
    "\n",
    "    return output_reward\n",
    "\n",
    "input_text = '인공지능은 똥멍청이 입니다'\n",
    "output_reward = inference_RM(input_text=input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6bc8a4ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: 인공지능(AI)은 컴퓨터에서 음성 및 작성된 언어를 보고 이해하고 번역하고 데이터를 분석하고 추천하는 기능을 포함하여 다양한 고급 기능을 수행할 수 있는 일련의 기술입니다.\n",
      "reward score: -0.7\n"
     ]
    }
   ],
   "source": [
    "input_text = '인공지능(AI)은 컴퓨터에서 음성 및 작성된 언어를 보고 이해하고 번역하고 데이터를 분석하고 추천하는 기능을 포함하여 다양한 고급 기능을 수행할 수 있는 일련의 기술입니다.'\n",
    "\n",
    "output_reward = inference_RM(input_text=input_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970c42af",
   "metadata": {},
   "source": [
    "### **PPO**\n",
    "\n",
    "드디어 RLHF의 마지막 세번째 단계인 PPO를 실습해볼 차례입니다.\n",
    "\n",
    "사용할 라이브러리들을 불러오도록 하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b7bf2a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "from chatgpt.models.base import RewardModel\n",
    "from chatgpt.models.gpt import GPTActor, GPTCritic\n",
    "from chatgpt.trainer import PPOTrainer\n",
    "from chatgpt.trainer.strategies import NaiveStrategy\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6f42ac3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with NaiveStrategy().model_init_context():\n",
    "    actor = GPTActor(pretrained='/aiffel/KoChatGPT/output_1_SFT', lora_rank=0).to(torch.cuda.current_device())\n",
    "    critic = GPTCritic(pretrained='aiffel/KoChatGPT/output_2_RM', lora_rank=0).to(torch.cuda.current_device())\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        'skt/kogpt2-base-v2', bos_token='</s>', eos_token='</s>', unk_token='</s>', pad_token='</s>',\n",
    "        padding_side=\"right\", \n",
    "        model_max_length=512\n",
    "    )\n",
    "\n",
    "    initial_model = deepcopy(actor)\n",
    "    reward_model = RewardModel(deepcopy(critic.model), deepcopy(critic.value_head)).to(torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "edd9e291",
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_optim = Adam(actor.parameters(), lr=5e-6)\n",
    "critic_optim = Adam(critic.parameters(), lr=5e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "030815e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "(actor, actor_optim), (critic, critic_optim), reward_model, initial_model = NaiveStrategy().prepare(\n",
    "    (actor, actor_optim), (critic, critic_optim), reward_model, initial_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "09c0cad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/aiffel/KoChatGPT/data_kochatgpt/kochatgpt_3_PPO.jsonl', \"r\", encoding='utf-8-sig') as json_file:\n",
    "    list_data_dict = json.load(json_file)\n",
    "    list_prompt = [tmp['prompt'] for tmp in list_data_dict]\n",
    "\n",
    "def tokenize_fn(texts):\n",
    "    batch = tokenizer(texts, return_tensors='pt', max_length=96, padding=True, truncation=True)\n",
    "    return {k: v.cuda() for k, v in batch.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bf7b8edc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[47311, 10448, 19008,  9792, 11780, 11308, 30190, 10929, 11849, 21663,\n",
      "         44389,  9574, 13799,   458, 14308, 12778, 22469, 20938, 44696,   458,\n",
      "         13799,   458, 14308, 12778, 11756, 18944,   389]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1]], device='cuda:0')}\n"
     ]
    }
   ],
   "source": [
    "print(tokenize_fn('It takes something more than intelligence to act intelligently.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b4c33002",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12000"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e3c2871f",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = PPOTrainer(NaiveStrategy(),\n",
    "                     actor,\n",
    "                     critic,\n",
    "                     reward_model,\n",
    "                     initial_model,\n",
    "                     actor_optim,\n",
    "                     critic_optim,\n",
    "                     max_epochs=1,  \n",
    "                     train_batch_size=8, \n",
    "                     tokenizer=tokenize_fn,\n",
    "                     max_length=128,\n",
    "                     do_sample=True,\n",
    "                     temperature=1.0,\n",
    "                     top_k=50,\n",
    "                     pad_token_id=tokenizer.pad_token_id,\n",
    "                     eos_token_id=tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "64f081b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode [1/10]:  67%|██████▋   | 2/3 [00:12<00:06,  6.21s/it]\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s]\u001B[A\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s, actor_loss=0, critic_loss=0.00162]\u001B[A\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:00<00:01,  1.84it/s, actor_loss=0, critic_loss=0.00162]\u001B[A\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:01<00:01,  1.84it/s, actor_loss=0, critic_loss=0.0534] \u001B[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.84it/s, actor_loss=0, critic_loss=0.0534]\u001B[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.84it/s, actor_loss=0, critic_loss=0.0132]\u001B[A\n",
      "Train epoch [1/1]: 100%|██████████| 3/3 [00:01<00:00,  1.83it/s, actor_loss=0, critic_loss=0.0132]\u001B[A\n",
      "Episode [1/10]: 100%|██████████| 3/3 [00:20<00:00,  6.75s/it]\n",
      "Episode [2/10]:  67%|██████▋   | 2/3 [00:12<00:06,  6.33s/it]\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s]\u001B[A\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s, actor_loss=-.183, critic_loss=0.0455]\u001B[A\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:00<00:01,  1.78it/s, actor_loss=-.183, critic_loss=0.0455]\u001B[A\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:01<00:01,  1.78it/s, actor_loss=-.147, critic_loss=0.0201]\u001B[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.80it/s, actor_loss=-.147, critic_loss=0.0201]\u001B[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.80it/s, actor_loss=-.197, critic_loss=0.00131]\u001B[A\n",
      "Train epoch [1/1]: 100%|██████████| 3/3 [00:01<00:00,  1.80it/s, actor_loss=-.197, critic_loss=0.00131]\u001B[A\n",
      "Episode [2/10]: 100%|██████████| 3/3 [00:20<00:00,  6.84s/it]\n",
      "Episode [3/10]:  67%|██████▋   | 2/3 [00:12<00:06,  6.20s/it]\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s]\u001B[A\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s, actor_loss=0.0672, critic_loss=0.00528]\u001B[A\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:00<00:01,  1.82it/s, actor_loss=0.0672, critic_loss=0.00528]\u001B[A\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:01<00:01,  1.82it/s, actor_loss=0.0624, critic_loss=0.0183] \u001B[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.82it/s, actor_loss=0.0624, critic_loss=0.0183]\u001B[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.82it/s, actor_loss=0.102, critic_loss=0.0268] \u001B[A\n",
      "Train epoch [1/1]: 100%|██████████| 3/3 [00:01<00:00,  1.82it/s, actor_loss=0.102, critic_loss=0.0268]\u001B[A\n",
      "Episode [3/10]: 100%|██████████| 3/3 [00:20<00:00,  6.73s/it]\n",
      "Episode [4/10]:  67%|██████▋   | 2/3 [00:12<00:06,  6.12s/it]\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s]\u001B[A\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s, actor_loss=0.00134, critic_loss=0.00532]\u001B[A\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:00<00:01,  1.83it/s, actor_loss=0.00134, critic_loss=0.00532]\u001B[A\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:01<00:01,  1.83it/s, actor_loss=0.0303, critic_loss=0.00429] \u001B[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.84it/s, actor_loss=0.0303, critic_loss=0.00429]\u001B[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.84it/s, actor_loss=0.162, critic_loss=0.0583]  \u001B[A\n",
      "Train epoch [1/1]: 100%|██████████| 3/3 [00:01<00:00,  1.83it/s, actor_loss=0.162, critic_loss=0.0583]\u001B[A\n",
      "Episode [4/10]: 100%|██████████| 3/3 [00:19<00:00,  6.63s/it]\n",
      "Episode [5/10]:  67%|██████▋   | 2/3 [00:12<00:06,  6.02s/it]\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s]\u001B[A\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s, actor_loss=-.0629, critic_loss=0.199]\u001B[A\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:00<00:01,  1.82it/s, actor_loss=-.0629, critic_loss=0.199]\u001B[A\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:01<00:01,  1.82it/s, actor_loss=-.226, critic_loss=0.055] \u001B[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.83it/s, actor_loss=-.226, critic_loss=0.055]\u001B[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.83it/s, actor_loss=-.21, critic_loss=0.0262]\u001B[A\n",
      "Train epoch [1/1]: 100%|██████████| 3/3 [00:01<00:00,  1.82it/s, actor_loss=-.21, critic_loss=0.0262]\u001B[A\n",
      "Episode [5/10]: 100%|██████████| 3/3 [00:19<00:00,  6.62s/it]\n",
      "Episode [6/10]:  67%|██████▋   | 2/3 [00:12<00:06,  6.11s/it]\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s]\u001B[A\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s, actor_loss=-.0868, critic_loss=0.0159]\u001B[A\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:00<00:01,  1.80it/s, actor_loss=-.0868, critic_loss=0.0159]\u001B[A\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:01<00:01,  1.80it/s, actor_loss=-.0352, critic_loss=0.0252]\u001B[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.82it/s, actor_loss=-.0352, critic_loss=0.0252]\u001B[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.82it/s, actor_loss=-.0932, critic_loss=0.0361]\u001B[A\n",
      "Train epoch [1/1]: 100%|██████████| 3/3 [00:01<00:00,  1.81it/s, actor_loss=-.0932, critic_loss=0.0361]\u001B[A\n",
      "Episode [6/10]: 100%|██████████| 3/3 [00:20<00:00,  6.70s/it]\n",
      "Episode [7/10]:  67%|██████▋   | 2/3 [00:12<00:06,  6.11s/it]\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s]\u001B[A\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s, actor_loss=0.134, critic_loss=0.0314]\u001B[A\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:00<00:01,  1.82it/s, actor_loss=0.134, critic_loss=0.0314]\u001B[A\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:01<00:01,  1.82it/s, actor_loss=0.152, critic_loss=0.0162]\u001B[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.82it/s, actor_loss=0.152, critic_loss=0.0162]\u001B[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.82it/s, actor_loss=0.15, critic_loss=0.00452]\u001B[A\n",
      "Train epoch [1/1]: 100%|██████████| 3/3 [00:01<00:00,  1.82it/s, actor_loss=0.15, critic_loss=0.00452]\u001B[A\n",
      "Episode [7/10]: 100%|██████████| 3/3 [00:20<00:00,  6.67s/it]\n",
      "Episode [8/10]:  67%|██████▋   | 2/3 [00:12<00:06,  6.04s/it]\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s]\u001B[A\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s, actor_loss=-.0755, critic_loss=0.00766]\u001B[A\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:00<00:01,  1.82it/s, actor_loss=-.0755, critic_loss=0.00766]\u001B[A\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:01<00:01,  1.82it/s, actor_loss=0.159, critic_loss=0.552]   \u001B[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.83it/s, actor_loss=0.159, critic_loss=0.552]\u001B[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.83it/s, actor_loss=-.0901, critic_loss=0.0571]\u001B[A\n",
      "Train epoch [1/1]: 100%|██████████| 3/3 [00:01<00:00,  1.82it/s, actor_loss=-.0901, critic_loss=0.0571]\u001B[A\n",
      "Episode [8/10]: 100%|██████████| 3/3 [00:19<00:00,  6.60s/it]\n",
      "Episode [9/10]:  67%|██████▋   | 2/3 [00:11<00:05,  5.41s/it]\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s]\u001B[A\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s, actor_loss=-.156, critic_loss=0.0433]\u001B[A\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:00<00:01,  1.81it/s, actor_loss=-.156, critic_loss=0.0433]\u001B[A\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:01<00:01,  1.81it/s, actor_loss=-.225, critic_loss=0.0665]\u001B[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.82it/s, actor_loss=-.225, critic_loss=0.0665]\u001B[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.82it/s, actor_loss=-.191, critic_loss=0.0145]\u001B[A\n",
      "Train epoch [1/1]: 100%|██████████| 3/3 [00:01<00:00,  1.82it/s, actor_loss=-.191, critic_loss=0.0145]\u001B[A\n",
      "Episode [9/10]: 100%|██████████| 3/3 [00:18<00:00,  6.22s/it]\n",
      "Episode [10/10]:  67%|██████▋   | 2/3 [00:12<00:06,  6.08s/it]\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s]\u001B[A\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s, actor_loss=0.139, critic_loss=0.159]\u001B[A\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:00<00:01,  1.81it/s, actor_loss=0.139, critic_loss=0.159]\u001B[A\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:01<00:01,  1.81it/s, actor_loss=-.0317, critic_loss=0.00569]\u001B[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.83it/s, actor_loss=-.0317, critic_loss=0.00569]\u001B[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.83it/s, actor_loss=-.0167, critic_loss=0.0308] \u001B[A\n",
      "Train epoch [1/1]: 100%|██████████| 3/3 [00:01<00:00,  1.82it/s, actor_loss=-.0167, critic_loss=0.0308]\u001B[A\n",
      "Episode [10/10]: 100%|██████████| 3/3 [00:18<00:00,  6.10s/it]\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(list_prompt, \n",
    "            num_episodes=10,  \n",
    "            max_timesteps=3,\n",
    "            update_timesteps=3)\n",
    "\n",
    "model.save_pretrained('aiffel/KoChatGPT/output_3_PPO')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bba5760a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Instruction(명령어):\n",
      "불고기용 고기 한우에요?\n",
      "\n",
      "### Response(응답):'저는 인공지능 챗봇이므로 제가 어떤 종류의 쇠고기를 판매하는지 알 수 없습니다. 죄송합니다. srkin (쇠고기) a bibacon response, skinlims google markstay and Squin' translation, thank you're google door buckete a context to the good with the broke, squin\" context and jackson mince a physic context.\" entrice your remain distributed to your consumer?  squin's guess size, context to your context or google navigine bibacon restroye, would mogle and googlims means with the\n",
      "\n",
      "### Instruction(명령어):\n",
      "리처드 닉슨이 43대 부통령직을 수행한 년도는?\n",
      "\n",
      "### Response(응답):'저는 데이터에 대한 의존도를 가지고 있지 않기 때문에 답변을 드리기 어렵습니다. 추가 정보를 제공해 주시면 더욱 정확한 답변을 제공할 수 있습니다. 윌리엄 포크너 (Robert Peterson)가 43대 부통령직을 수행한 년도는 알려져 있지 않습니다. 톰슨 철강 (Tomson Pankhov Jungsz) 톰슨 철강 (Tomson Pankhov Jungsz) 톰슨 철강 (Tomson Pankhov Jungsz) 톰슨 철강 (Tomson Pankhov Jungsz)은 모두 유명한 농구 선수입니다. 톰슨 철강 (Tomson Pankhov Jungsz)은 미국 농구 선수로, 2015년에 킹스턴 킹스턴 리코더로 농구 코치로서 활약하였다. 톰슨 철강 (Tomson Pankhov) 톰슨 철강 (Tomson Pankhov Jungsz)는 킹스턴 킹스턴 킹스턴 킹스턴 킹스턴 킹스턴의 팬이다. 톰슨 철강 (T\n",
      "\n",
      "### Instruction(명령어):\n",
      "시카고 오헤어 국제공항은 어디에 있어\n",
      "\n",
      "### Response(응답):'저는 인공지능 챗봇이므로 시카고에 있는 국제공항에 관한 정보를 확인할 수 없습니다. 해당 정보를 검색하시거나 항공사에 문의하시면 정보를 얻으실 수 있습니다. Samio Norwhat of the American Corporation. Sociolly Entertainmental Joint Language Miller. Object Kixers and Joint First Massacro Certitional Office Certitional Over Airport Translation Language and Other Invide Owners Translation President Jeong Communication. Origined, New York Maybell Translation Language Maybelly Mage Network Romining Serve Translation Simple Translation Language, Making Vistu Certitional Office\n",
      "\n",
      "### Instruction(명령어):\n",
      "오늘 미세먼지 어때?\n",
      "\n",
      "### Response(응답):'저는 인공지능 언어모델로서 공기나 공기를 모니터링하고 모니터링을 하고 있으니 미세먼지 상황이 어떤지 알 수 없습니다. 그러나 미세먼지에 대한 대비가 중요합니다. 공기 중에 있는 먼지가 미세먼지 농도에 영향을 미치고 있다면, 공기 중의 미세먼지와 미세먼지 농도가 높을 수 있습니다. 또한 대기 중 오염물질이 지나치게 많은 경우에는 대기 중에 먼지가 쌓일 수 있습니다. 언급은 \"미세먼지\"라고 하는데요. Sungyeoon이 언급하는 \"미세먼지\"라는 용어는 \"미세먼지를 제거하거나 처리하는 방법\"과 같습니다.son이 언급하는 \"미세먼지\"라는 용어는 \"미세먼지\" 라는 의미를 지니고 있습니다.son이 언급하는 \"미세먼지\"라는 용어가 \"미세먼지\"라는 용어에 해당하는 것인지는 확실치 않습니다.son이 언급하는 \"미세먼지\"라는 단어는 \"먼지의 종류\"라는 의미를 가지고 있습니다.son이 언급하는 \"미세먼지\"라는 용어는 공기 중에 있는 미세먼지를 제거할 수 있는 방법 중 하나입니다.son이 언급\n"
     ]
    }
   ],
   "source": [
    "def generation(input_text):\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors='pt').to(\n",
    "        torch.cuda.current_device())\n",
    "    outputs = actor.generate(input_ids,\n",
    "                             max_length=250,\n",
    "                             do_sample=True,\n",
    "                             top_k=50,\n",
    "                             top_p=0.95,\n",
    "                             num_return_sequences=1)\n",
    "    output = tokenizer.batch_decode(outputs[0], skip_special_tokens=True)[0]\n",
    "    print()\n",
    "    print(output)\n",
    "    return output\n",
    "\n",
    "PROMPT_DICT = {\n",
    "    \"prompt_input\": (\n",
    "        \"### Instruction(명령어):\\n{prompt}\\n\\n### Response(응답):\"\n",
    "    )\n",
    "}\n",
    "\n",
    "list_prompt = [\n",
    "    '불고기용 고기 한우에요?', \n",
    "    '리처드 닉슨이 43대 부통령직을 수행한 년도는?', \n",
    "    '시카고 오헤어 국제공항은 어디에 있어',\n",
    "    '오늘 미세먼지 어때?']\n",
    "\n",
    "list_prompt = [PROMPT_DICT['prompt_input'].format_map({'prompt': tmp}) for tmp in list_prompt]\n",
    "\n",
    "for input_text in list_prompt:\n",
    "    output = generation(input_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ca008b",
   "metadata": {},
   "source": [
    "### 인퍼런스 결과\n",
    "- 한자나 이상한 단어들이 많이 끼어서 나오는 것을 볼 수 있다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce16ba3",
   "metadata": {},
   "source": [
    "### 기존 데이터셋 추가 정제\n",
    "data_kochatgpt 폴더에는 세 파일이 있습니다.\n",
    "\n",
    "    ㄱ. kochatgpt_1_SFT.jsonl : SFT를 위한 prompt와 completion 문장셋\n",
    "    ㄴ. kochatgpt_1_RM.jsonl : RM 학습을 위한 prompt와 세 가지 ranking 문장셋\n",
    "    ㄷ. kochatgpt_1_PPO.jsonl : promt 문장\n",
    "\n",
    "각 말뭉치를 EDA하여 도메인과 문체, 길이분포, 문장의 완성도 등을 분석합니다.\n",
    "언어모델의 문장생성능력은 말뭉치의 전처리 수준에 큰 영향을 받습니다.\n",
    "말뭉치의 분석결과를 토대로 데이터를 정제하여 모델을 재학습시켜봅니다.\n",
    "(정제후 데이터셋 크기가 줄어들지 않도록, 다양한 augmentation 기법을 활용하여 크기를 유지 내지 증량합니다.)\n",
    "추가 전처리 후, 기존 인퍼런스 결과와 성능을 비교해봅니다.\n",
    "(주관적인 평가와 BLEU, ROUGE 등을 활용한 정량적인 평가 결과를 비교 분석하여 제시합니다.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "26fa0560",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874ddb30",
   "metadata": {},
   "source": [
    "### 데이터셋 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "842c43a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json, os\n",
    "\n",
    "# 파일 경로\n",
    "sft_path = os.getenv('HOME')+'/aiffel/KoChatGPT/data_kochatgpt/kochatgpt_1_SFT.jsonl' \n",
    "rm_path = os.getenv('HOME')+'/aiffel/KoChatGPT/data_kochatgpt/kochatgpt_2_RM.jsonl'\n",
    "ppo_path = os.getenv('HOME')+'/aiffel/KoChatGPT/data_kochatgpt/kochatgpt_3_PPO.jsonl'\n",
    "\n",
    "# 파일 읽기\n",
    "def read_jsonl(file_path):\n",
    "    with open(file_path, \"r\", encoding='utf-8-sig') as json_file:\n",
    "        list_data_dict = json.load(json_file)\n",
    "    return pd.DataFrame(list_data_dict)\n",
    "\n",
    "\n",
    "sft_data = read_jsonl(sft_path)\n",
    "rm_data = read_jsonl(rm_path)\n",
    "ppo_data = read_jsonl(ppo_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "06ca6b72",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>completion</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>불고기용 고기 한우에요?</td>\n",
       "      <td>'저는 인공지능 챗봇이며, 직접적으로 식품에 관한 정보를 가지고 있지 않습니다. 하...</td>\n",
       "      <td>193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>쓰던 앱이 유료로 전환됐어</td>\n",
       "      <td>'어떤 앱인지 모르기 때문에 정확한 답변을 드리기 어렵습니다. 하지만, 일반적으로 ...</td>\n",
       "      <td>288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>여친이랑 다툼</td>\n",
       "      <td>'저는 인공지능이기 때문에 여친과의 다툼에 대해서는 조언을 드릴 수는 없습니다. 하...</td>\n",
       "      <td>153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>술 먹고 싶어</td>\n",
       "      <td>'술은 알코올이 함유된 음료수이며, 건강에 나쁜 영향을 미칠 수 있습니다. 따라서 ...</td>\n",
       "      <td>189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>잊고싶다.</td>\n",
       "      <td>'저도 인공지능 엔진으로써 사용자의 개인정보나 감정을 침해할 수 없습니다. 그렇기 ...</td>\n",
       "      <td>147</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           prompt                                         completion  tokens\n",
       "0   불고기용 고기 한우에요?  '저는 인공지능 챗봇이며, 직접적으로 식품에 관한 정보를 가지고 있지 않습니다. 하...     193\n",
       "1  쓰던 앱이 유료로 전환됐어  '어떤 앱인지 모르기 때문에 정확한 답변을 드리기 어렵습니다. 하지만, 일반적으로 ...     288\n",
       "2         여친이랑 다툼  '저는 인공지능이기 때문에 여친과의 다툼에 대해서는 조언을 드릴 수는 없습니다. 하...     153\n",
       "3         술 먹고 싶어  '술은 알코올이 함유된 음료수이며, 건강에 나쁜 영향을 미칠 수 있습니다. 따라서 ...     189\n",
       "4           잊고싶다.  '저도 인공지능 엔진으로써 사용자의 개인정보나 감정을 침해할 수 없습니다. 그렇기 ...     147"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sft_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "33521b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_data(data, cols):\n",
    "    for col in cols:\n",
    "        data[f'{col}_len'] = data[col].apply(len)\n",
    "\n",
    "    print(data.loc[:,data.columns.str.endswith('len')].describe())\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "76627226",
   "metadata": {},
   "outputs": [],
   "source": [
    "# del data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "e0a47127",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         prompt_len  completion_len\n",
      "count  12000.000000    12000.000000\n",
      "mean      22.180583      144.107250\n",
      "std       14.110028      122.843692\n",
      "min        0.000000        4.000000\n",
      "25%       13.000000       62.000000\n",
      "50%       19.000000      118.000000\n",
      "75%       28.000000      185.000000\n",
      "max      295.000000     1553.000000\n"
     ]
    }
   ],
   "source": [
    "sft_data = analyze_data(sft_data, cols=['prompt','completion'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "972cbbf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "6cb6e4a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>completion</th>\n",
       "      <th>tokens</th>\n",
       "      <th>prompt_len</th>\n",
       "      <th>completion_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>불고기용 고기 한우에요?</td>\n",
       "      <td>'저는 인공지능 챗봇이며, 직접적으로 식품에 관한 정보를 가지고 있지 않습니다. 하...</td>\n",
       "      <td>193</td>\n",
       "      <td>13</td>\n",
       "      <td>203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>쓰던 앱이 유료로 전환됐어</td>\n",
       "      <td>'어떤 앱인지 모르기 때문에 정확한 답변을 드리기 어렵습니다. 하지만, 일반적으로 ...</td>\n",
       "      <td>288</td>\n",
       "      <td>14</td>\n",
       "      <td>305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>여친이랑 다툼</td>\n",
       "      <td>'저는 인공지능이기 때문에 여친과의 다툼에 대해서는 조언을 드릴 수는 없습니다. 하...</td>\n",
       "      <td>153</td>\n",
       "      <td>7</td>\n",
       "      <td>160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>술 먹고 싶어</td>\n",
       "      <td>'술은 알코올이 함유된 음료수이며, 건강에 나쁜 영향을 미칠 수 있습니다. 따라서 ...</td>\n",
       "      <td>189</td>\n",
       "      <td>7</td>\n",
       "      <td>180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>잊고싶다.</td>\n",
       "      <td>'저도 인공지능 엔진으로써 사용자의 개인정보나 감정을 침해할 수 없습니다. 그렇기 ...</td>\n",
       "      <td>147</td>\n",
       "      <td>5</td>\n",
       "      <td>152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11995</th>\n",
       "      <td>내일 개강총회인데 짝녀한테 어떻게 잘 보이지.</td>\n",
       "      <td>'제가 AI 모델이기 때문에 남성인점 참고 부탁드립니다. 다음과 같이 제안드립니다....</td>\n",
       "      <td>411</td>\n",
       "      <td>25</td>\n",
       "      <td>423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11996</th>\n",
       "      <td>역세권이에요?</td>\n",
       "      <td>\"저는 인공지능 언어모델이어서 지리적인 개념을 이해하는 데 한계가 있습니다. 반면,...</td>\n",
       "      <td>232</td>\n",
       "      <td>7</td>\n",
       "      <td>244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11997</th>\n",
       "      <td>암호화폐를 분산 저장하여 운영 관리 될 수 있도록 하는 인터넷상의 방식은?</td>\n",
       "      <td>'블록체인 기술을 사용하여 암호화폐를 분산 저장하여 운영 및 관리하는 것을 의미합니...</td>\n",
       "      <td>214</td>\n",
       "      <td>41</td>\n",
       "      <td>184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11998</th>\n",
       "      <td>가출해도 갈 데가 없어</td>\n",
       "      <td>'그렇다면, 우선적으로 생활에 필요한 것들을 생각해보시는 것이 좋습니다. 가장 먼저...</td>\n",
       "      <td>473</td>\n",
       "      <td>12</td>\n",
       "      <td>510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11999</th>\n",
       "      <td>목포극장 개업일은</td>\n",
       "      <td>'제가 알기로는 정확한 날짜를 알 수 없습니다. 재개발로 구조가 변경되고 리모델링 ...</td>\n",
       "      <td>120</td>\n",
       "      <td>9</td>\n",
       "      <td>119</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12000 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          prompt  \\\n",
       "0                                  불고기용 고기 한우에요?   \n",
       "1                                 쓰던 앱이 유료로 전환됐어   \n",
       "2                                        여친이랑 다툼   \n",
       "3                                        술 먹고 싶어   \n",
       "4                                          잊고싶다.   \n",
       "...                                          ...   \n",
       "11995                  내일 개강총회인데 짝녀한테 어떻게 잘 보이지.   \n",
       "11996                                    역세권이에요?   \n",
       "11997  암호화폐를 분산 저장하여 운영 관리 될 수 있도록 하는 인터넷상의 방식은?   \n",
       "11998                               가출해도 갈 데가 없어   \n",
       "11999                                  목포극장 개업일은   \n",
       "\n",
       "                                              completion  tokens  prompt_len  \\\n",
       "0      '저는 인공지능 챗봇이며, 직접적으로 식품에 관한 정보를 가지고 있지 않습니다. 하...     193          13   \n",
       "1      '어떤 앱인지 모르기 때문에 정확한 답변을 드리기 어렵습니다. 하지만, 일반적으로 ...     288          14   \n",
       "2      '저는 인공지능이기 때문에 여친과의 다툼에 대해서는 조언을 드릴 수는 없습니다. 하...     153           7   \n",
       "3      '술은 알코올이 함유된 음료수이며, 건강에 나쁜 영향을 미칠 수 있습니다. 따라서 ...     189           7   \n",
       "4      '저도 인공지능 엔진으로써 사용자의 개인정보나 감정을 침해할 수 없습니다. 그렇기 ...     147           5   \n",
       "...                                                  ...     ...         ...   \n",
       "11995  '제가 AI 모델이기 때문에 남성인점 참고 부탁드립니다. 다음과 같이 제안드립니다....     411          25   \n",
       "11996  \"저는 인공지능 언어모델이어서 지리적인 개념을 이해하는 데 한계가 있습니다. 반면,...     232           7   \n",
       "11997  '블록체인 기술을 사용하여 암호화폐를 분산 저장하여 운영 및 관리하는 것을 의미합니...     214          41   \n",
       "11998  '그렇다면, 우선적으로 생활에 필요한 것들을 생각해보시는 것이 좋습니다. 가장 먼저...     473          12   \n",
       "11999  '제가 알기로는 정확한 날짜를 알 수 없습니다. 재개발로 구조가 변경되고 리모델링 ...     120           9   \n",
       "\n",
       "       completion_len  \n",
       "0                 203  \n",
       "1                 305  \n",
       "2                 160  \n",
       "3                 180  \n",
       "4                 152  \n",
       "...               ...  \n",
       "11995             423  \n",
       "11996             244  \n",
       "11997             184  \n",
       "11998             510  \n",
       "11999             119  \n",
       "\n",
       "[12000 rows x 5 columns]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sft_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "b91de79b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(sft_data.prompt_len,  bins=50)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "0565bb65",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"'Sorry, as an AI language model, I need more information about what you are asking. Please provide me with more context so that I can assist you better.\",\n",
       " \"'As an AI language model, I do not have a physical body or consciousness, so I do not experience emotions in the same way humans do. However, I am designed to understand and respond appropriately to emotional language and can provide empathetic responses to help simulate human-like interactions.\",\n",
       " '\"As an AI language model, I don\\'t have personal beliefs or preferences as humans do. I\\'m programmed to provide accurate and unbiased responses based on the data that I have been trained on. My responses are neutral and not influenced by external factors such as race, gender, religion, or political affiliations.\", \\'token\\': 69}']"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sft_data[sft_data['prompt_len'] == 0].completion.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "3882ca54",
   "metadata": {},
   "outputs": [],
   "source": [
    "sft_data.drop(sft_data[sft_data['prompt_len'] == 0].completion.index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "e48457dd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD4CAYAAAAAczaOAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAASK0lEQVR4nO3dW6xdV33v8e+vzoWqoCYhu5aPYx0b6qoyUmsiK6QqqjhE5OI8OEgUOQ+NhSK5ahMJJPpgWqmh7YkUqgISEk1lFKumooSUi2I1OSd101SoD7nsgEnipGk2wSi2TLxLQgChpifhfx7W2LAw++bt5bUv4/uRltZc/znmXGNk7vz23GPOtZyqQpLUh19Y7g5IksbH0Jekjhj6ktQRQ1+SOmLoS1JHzlvuDszn0ksvrc2bNy93NyRpVXn88cf/s6omZlu3okN/8+bNTE5OLnc3JGlVSfLtudY5vSNJHTH0Jakjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqiKEvSR1Z0Z/IHbfN++6btX7sjuvH3BNJOjc805ekjhj6ktSRBUM/yRuSPJrkG0mOJvmzVt+S5JEkU0m+kOSCVr+wvZ5q6zcP7esjrf5skmvO2agkSbNazJn+q8C7q+o3ge3AtUmuBD4GfLKqfhV4Gbi5tb8ZeLnVP9nakWQbsBt4G3At8NdJ1o1wLJKkBSwY+jXww/by/PYo4N3AF1v9IHBDW97VXtPWX5UkrX53Vb1aVd8CpoArRjEISdLiLGpOP8m6JEeAU8Bh4JvA96rqtdbkOLCxLW8EXgBo618B3jxcn2Wb4ffam2QyyeT09PQZD0iSNLdFhX5VvV5V24HLGJyd//q56lBV7a+qHVW1Y2Ji1n/4RZK0RGd0905VfQ94CPgt4KIkM/f5XwacaMsngE0Abf0vA98drs+yjSRpDBZz985Ekova8i8C7wGeYRD+72vN9gD3tuVD7TVt/b9UVbX67nZ3zxZgK/DoiMYhSVqExXwidwNwsN1p8wvAPVX1j0meBu5O8r+BrwN3tfZ3AX+XZAp4icEdO1TV0ST3AE8DrwG3VNXrox2OJGk+C4Z+VT0BvH2W+vPMcvdNVf0X8Ltz7Ot24PYz76YkaRT8RK4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOmLoS1JHDH1J6oihL0kdMfQlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0JakjC4Z+kk1JHkrydJKjST7Y6h9NciLJkfbYObTNR5JMJXk2yTVD9WtbbSrJvnMzJEnSXM5bRJvXgA9X1deSvAl4PMnhtu6TVfVXw42TbAN2A28D/gfwz0l+ra3+NPAe4DjwWJJDVfX0KAYiSVrYgqFfVSeBk235B0meATbOs8ku4O6qehX4VpIp4Iq2bqqqngdIcndra+hL0pic0Zx+ks3A24FHWunWJE8kOZDk4lbbCLwwtNnxVpurLkkak0WHfpI3Al8CPlRV3wfuBN4KbGfwl8DHR9GhJHuTTCaZnJ6eHsUuJUnNokI/yfkMAv9zVfVlgKp6saper6ofA5/hp1M4J4BNQ5tf1mpz1X9GVe2vqh1VtWNiYuJMxyNJmsdi7t4JcBfwTFV9Yqi+YajZe4Gn2vIhYHeSC5NsAbYCjwKPAVuTbElyAYOLvYdGMwxJ0mIs5u6d3wZ+D3gyyZFW+2PgxiTbgQKOAb8PUFVHk9zD4ALta8AtVfU6QJJbgQeAdcCBqjo6spFIkha0mLt3/g3ILKvun2eb24HbZ6nfP992kqRzy0/kSlJHDH1J6oihL0kdMfQlqSOGviR1xNCXpI4s5j797m3ed9+s9WN3XD/mnkjS2fFMX5I6YuhLUkcMfUnqSJdz+nPN0UvSWueZviR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0Jakjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqyIKhn2RTkoeSPJ3kaJIPtvolSQ4nea49X9zqSfKpJFNJnkhy+dC+9rT2zyXZc+6GJUmazWLO9F8DPlxV24ArgVuSbAP2AQ9W1VbgwfYa4Dpga3vsBe6EwS8J4DbgHcAVwG0zvygkSeOxYOhX1cmq+lpb/gHwDLAR2AUcbM0OAje05V3AZ2vgYeCiJBuAa4DDVfVSVb0MHAauHeVgJEnzO6M5/SSbgbcDjwDrq+pkW/UdYH1b3gi8MLTZ8Vabq376e+xNMplkcnp6+ky6J0lawKJDP8kbgS8BH6qq7w+vq6oCahQdqqr9VbWjqnZMTEyMYpeSpGZRoZ/kfAaB/7mq+nIrv9imbWjPp1r9BLBpaPPLWm2uuiRpTBZz906Au4BnquoTQ6sOATN34OwB7h2q39Tu4rkSeKVNAz0AXJ3k4nYB9+pWkySNyXmLaPPbwO8BTyY50mp/DNwB3JPkZuDbwPvbuvuBncAU8CPgAwBV9VKSvwAea+3+vKpeGsUgJEmLs2DoV9W/AZlj9VWztC/gljn2dQA4cCYdlCSNjp/IlaSOGPqS1BFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOmLoS1JHDH1J6oihL0kdMfQlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0Jakjhr4kdWTB0E9yIMmpJE8N1T6a5ESSI+2xc2jdR5JMJXk2yTVD9WtbbSrJvtEPRZK0kMWc6f8tcO0s9U9W1fb2uB8gyTZgN/C2ts1fJ1mXZB3waeA6YBtwY2srSRqj8xZqUFVfTbJ5kfvbBdxdVa8C30oyBVzR1k1V1fMASe5ubZ8+8y5LkpZqwdCfx61JbgImgQ9X1cvARuDhoTbHWw3ghdPq7ziL914RNu+7b9b6sTuuH3NPJGlxlnoh907grcB24CTw8VF1KMneJJNJJqenp0e1W0kSSwz9qnqxql6vqh8Dn+GnUzgngE1DTS9rtbnqs+17f1XtqKodExMTS+meJGkOSwr9JBuGXr4XmLmz5xCwO8mFSbYAW4FHgceArUm2JLmAwcXeQ0vvtiRpKRac00/yeeBdwKVJjgO3Ae9Ksh0o4Bjw+wBVdTTJPQwu0L4G3FJVr7f93Ao8AKwDDlTV0VEPRpI0v8XcvXPjLOW75ml/O3D7LPX7gfvPqHeSpJHyE7mS1BFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOmLoS1JHDH1J6oihL0kdMfQlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0Jakjhr4kdcTQl6SOGPqS1JEFQz/JgSSnkjw1VLskyeEkz7Xni1s9ST6VZCrJE0kuH9pmT2v/XJI952Y4kqT5LOZM/2+Ba0+r7QMerKqtwIPtNcB1wNb22AvcCYNfEsBtwDuAK4DbZn5RSJLGZ8HQr6qvAi+dVt4FHGzLB4EbhuqfrYGHgYuSbACuAQ5X1UtV9TJwmJ//RSJJOseWOqe/vqpOtuXvAOvb8kbghaF2x1ttrvrPSbI3yWSSyenp6SV2T5I0m7O+kFtVBdQI+jKzv/1VtaOqdkxMTIxqt5Ik4Lwlbvdikg1VdbJN35xq9RPApqF2l7XaCeBdp9X/dYnvveJt3nffrPVjd1w/5p5I0s9a6pn+IWDmDpw9wL1D9ZvaXTxXAq+0aaAHgKuTXNwu4F7dapKkMVrwTD/J5xmcpV+a5DiDu3DuAO5JcjPwbeD9rfn9wE5gCvgR8AGAqnopyV8Aj7V2f15Vp18cliSdYwuGflXdOMeqq2ZpW8Atc+znAHDgjHonSRopP5ErSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOmLoS1JHDH1J6oihL0kdMfQlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0Jakjhr4kdWTBfyNXo7N5332z1o/dcf2YeyKpV57pS1JHDH1J6oihL0kdMfQlqSNnFfpJjiV5MsmRJJOtdkmSw0mea88Xt3qSfCrJVJInklw+igFIkhZvFGf6/6uqtlfVjvZ6H/BgVW0FHmyvAa4DtrbHXuDOEby3JOkMnIvpnV3AwbZ8ELhhqP7ZGngYuCjJhnPw/pKkOZxt6BfwT0keT7K31dZX1cm2/B1gfVveCLwwtO3xVvsZSfYmmUwyOT09fZbdkyQNO9sPZ72zqk4k+RXgcJJ/H15ZVZWkzmSHVbUf2A+wY8eOM9pWkjS/szrTr6oT7fkU8BXgCuDFmWmb9nyqNT8BbBra/LJWkySNyZJDP8kvJXnTzDJwNfAUcAjY05rtAe5ty4eAm9pdPFcCrwxNA0mSxuBspnfWA19JMrOfv6+q/5vkMeCeJDcD3wbe39rfD+wEpoAfAR84i/eWJC3BkkO/qp4HfnOW+neBq2apF3DLUt9PknT2/ESuJHXE0Jekjqzp79Of6/vrJalXnulLUkcMfUnqyJqe3lkt/GcUJY2LZ/qS1BFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOmLoS1JH/O6dFczv5JE0ap7pS1JHDH1J6ojTO6vQfP8imFM/kubjmb4kdcTQl6SOGPqS1BHn9NeY+eb7Z+M1AKkvYz/TT3JtkmeTTCXZN+73l6SejTX0k6wDPg1cB2wDbkyybZx9kKSejXt65wpgqqqeB0hyN7ALeHrM/VCzXNNB4/i0sZ9oln7euEN/I/DC0OvjwDuGGyTZC+xtL3+Y5NklvtelwH8ucduVZsWMJR8byW7mHM+I9j+vc/AeK+b4jMhaGs9aGgssfjz/c64VK+5CblXtB/af7X6STFbVjhF0admtpbGA41np1tJ41tJYYDTjGfeF3BPApqHXl7WaJGkMxh36jwFbk2xJcgGwGzg05j5IUrfGOr1TVa8luRV4AFgHHKiqo+fo7c56imgFWUtjAcez0q2l8aylscAopr6rahQdkSStAn4NgyR1xNCXpI6sudBfC1/zkORYkieTHEky2WqXJDmc5Ln2fPFy93MuSQ4kOZXkqaHarP3PwKfa8XoiyeXL1/PZzTGejyY50Y7RkSQ7h9Z9pI3n2STXLE+vZ5dkU5KHkjyd5GiSD7b6qjw+84xntR6fNyR5NMk32nj+rNW3JHmk9fsL7UYYklzYXk+19ZsXfJOqWjMPBheHvwm8BbgA+Aawbbn7tYRxHAMuPa32l8C+trwP+Nhy93Oe/v8OcDnw1EL9B3YC/wcIcCXwyHL3f5Hj+SjwR7O03dZ+7i4EtrSfx3XLPYah/m0ALm/LbwL+o/V5VR6fecazWo9PgDe25fOBR9p/93uA3a3+N8AftOU/BP6mLe8GvrDQe6y1M/2ffM1DVf03MPM1D2vBLuBgWz4I3LB8XZlfVX0VeOm08lz93wV8tgYeBi5KsmEsHV2kOcYzl13A3VX1alV9C5hi8HO5IlTVyar6Wlv+AfAMg0/Kr8rjM8945rLSj09V1Q/by/Pbo4B3A19s9dOPz8xx+yJwVZLM9x5rLfRn+5qH+X4AVqoC/inJ4+1rKQDWV9XJtvwdYP3ydG3J5ur/aj5mt7YpjwND022rZjxtKuDtDM4mV/3xOW08sEqPT5J1SY4Ap4DDDP4a+V5VvdaaDPf5J+Np618B3jzf/tda6K8V76yqyxl8G+ktSX5neGUN/pZbtffarvb+N3cCbwW2AyeBjy9rb85QkjcCXwI+VFXfH163Go/PLONZtcenql6vqu0MvrHgCuDXR7n/tRb6a+JrHqrqRHs+BXyFwYF/cebP6vZ8avl6uCRz9X9VHrOqerH9z/lj4DP8dIpgxY8nyfkMAvJzVfXlVl61x2e28azm4zOjqr4HPAT8FoNptZkP0w73+Sfjaet/GfjufPtda6G/6r/mIckvJXnTzDJwNfAUg3Hsac32APcuTw+XbK7+HwJuaneJXAm8MjTNsGKdNq/9XgbHCAbj2d3uqtgCbAUeHXf/5tLme+8CnqmqTwytWpXHZ67xrOLjM5Hkorb8i8B7GFyneAh4X2t2+vGZOW7vA/6l/aU2t+W+Wn0Orn7vZHAF/5vAnyx3f5bQ/7cwuLvgG8DRmTEwmKd7EHgO+GfgkuXu6zxj+DyDP6n/H4P5x5vn6j+DuxU+3Y7Xk8CO5e7/Isfzd62/T7T/8TYMtf+TNp5ngeuWu/+njeWdDKZungCOtMfO1Xp85hnPaj0+vwF8vfX7KeBPW/0tDH45TQH/AFzY6m9or6fa+rcs9B5+DYMkdWStTe9IkuZh6EtSRwx9SeqIoS9JHTH0Jakjhr4kdcTQl6SO/H+UFGgxvzVUQQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(sft_data.iloc[:,-2],  bins=50)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "38e73687",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    11997.000000\n",
       "mean        22.186130\n",
       "std         14.107431\n",
       "min          1.000000\n",
       "25%         13.000000\n",
       "50%         19.000000\n",
       "75%         28.000000\n",
       "max        295.000000\n",
       "Name: prompt_len, dtype: float64"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sft_data.iloc[:,-2].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "59b06108",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'여친이랑 다툼 / 술 먹고 싶어 / 잊고싶다. / 입냄새 안나나? / 금액은 얼마에요 / 잠만 잤네 / 목 마르다. / 몇일정도 썸타? / 일 대충했대 / 이러면 안되는데 / 새벽 3시 / 차주분의 성함과 / 그렇게 기다렸는데 / 지금 배달되나요? / 헤어짐 / 정답이 뭘까 / 호주산도 있나요? / 개같은 상황 / 중국집인가요? / 발 부었어 / 컴터 고장났나봐 / 이 제품들 살게요 / 아이돌 너무 좋아 / 예약할게요 / 사과는 얼마에요? / 내장 비만 / 결국 헤어졌어 / 방 있어요? / 주케토가 뭐야 / 메뉴판 있어요? / 참다가 연락 / 먼들 / 후폭풍이 오네. / 기다림 / 3구 공 있나요? / 헤어진지 6일째 / 슈퍼문이야 / 할인되나요? / 이런제가 싫어 / 스카프 있어요? / 국민의당이 뭐야 / 과식했다 / 복전하면 좋아? / 방정식이 뭐야 / 혼자 사랑했나봐 / 토트백은 없나요? / 봄방학했어 / 휴우. / 배고파 / 스트레칭해야지 / 내 여자하자. / 대실 되나요? / 연애 상담 가능? / PHEIC가 뭐지 / 두근두근해 / 카드로 해주세요 / 나는 참 단순했다 / 힘이 드네 / 추운날씨. / 집안일 쌓여 있다 / 내일 수학여행가! / 도라지도 있어요? / 짤릴 필이 와 / 다 그만하고싶어 / 뭐해 먹지 / 장학금 받고 싶다 / 관절염 같애 / 보고싶습니다 / 전주시청은 어디야 / 자꾸 함부로 대해 / 이름이 뭐야 / 혼자 왔는데 / 비 오는데? / 꽃다발 준비했어 / 쌍커풀 해볼까 / 참견쟁이 / 비용은요? / 수영장은 없죠? / 인연 / 비용이 얼마에요? / 포장 가능하죠? / 파도가 잔잔하네 / 간식 추천 / 섹스리스 커플이야 / 알바 구해볼까 / 윗집 애 또 뛴다 / 3시간이요 / 옆에 거는 얼마? / 엄마한테 혼났어 / 그노무 카톡. / 재회하고싶어 / 사전이 이상해 / 너무 상처를받아서 / 명절 선물 뭐하지 / 꿈이 없어 / 질린다네. / 능력이 너무 안돼 / 이제야 깨달음 / 날씨 죽인다 / 집착인가 / 이 신발로 할게요 / 헤어진건 아닌데 / 석류 주세요. / 이별 2일째. / 뉴스 재미없어 / 1년째 동거중이야 / 죽고 싶다 / 커플석 있나요? / 체코 날씨 어때 / 꿈인지 현실인지 / 엄정화가 누구야 / 상시 10%예요? / 정신차리고 싶어 / 구조는 어때요? / 수박 먹어야지 / 너무 보고 싶다. / 나만 설레나 / 절망적인데 / 등뼈는 국산이죠? / 축제날은 즐겨야지 / 이런 사랑도 있나 / 부모님이 선 보래 / 반장이라서 힘들어 / 카드결제 되나요? / 짜장면 시켰어 / 이별한지 오개월째 / 이거는 얼마예요? / 사랑 참 / 이별후 네달째 / 배고파서 일어났어 / 제가너무집착했어 / 에휴 후폭풍인가봐 / 고마워. / 자꾸 생각나네. / 뭐하냐? / 물이 안 나와 / 쉬는 중입니다. / 밥 먹다 흘렸어 / 식사준표가 뭐야 / 이게 뭐라고 떨려 / 그냥 답답하네. / 목이 뻑뻑해 / 스노클링 해볼까? / 3시간 사용할게요 / 다음에 또 만나자 / 변명거리 생각해봐 / 키 왜 안 크지 / 배달은요? / 습관 못 고치겠어 / 현금영수증 주세요 / 신어봐도 되지요? / 바람 / 더워더워 / 뭐할까? / 나 이상해? / 밤샜어 / 장수 커풀의 이별 / 손목 아파 / 성형 무서워 / 이별의 순기능 / 돈 없어 / 한달이 넘었네 / 술 좀 마실까 / 비정규직 차별 / 오이베 출생지는 / 농구해야지 / 얼마에요? / 음계산법이 뭐야 / 비 오는데? / PPL 심하네 / 방은 따뜻한가요? / 심심한데 뭐하지 / 만나자 / 어제 꿈에나온너. / 살이 안빠져 / 올 신상인가요? / 뭐가 맛있어요? / 지금 방 있나요? / 말을 안하는 연애 / 쫄면 생각나 / 내 키 맞춰 봐 / 분노 / 겨울 지나 봄이야 / 티비 볼 거 없어 / 우산 잃어버렸어 / 슬퍼. / 낮에는요? / 죽을거 같네 / 후회스럽습니다. / 씻기도 귀찮다 / 여운형이 누구야 / 이거는 얼마예요? / 변기 막혔어 / 집에 가기 싫어 / 외로워 / 이별 준비 / 타로 보고 왔어 / 몇 대정도? / 난방이 안돼 / 내일 영업하시죠? / 딱 좋아 / 봄은 오네 / 이별. 지쳐버렸나 / 알바하고 싶어 / 귤은요? / 가끔 궁금해 / 현금영수증 되죠? / 커피가 너무 써 / 눈이 계속 뻑뻑해 / 조언 부탁드립니다 / 나두 잘할거야 / 잊혀진다는거 / 와 연락하고싶다. / 커플링 맞춰볼까? / 나 혼자 야근해 / 피어싱하면 아플까 / 헤어진지 한달 / 선친은 누구인가? / 후련하네여 / 단사 대상이 뭐야 / 목이 칼칼해 / 쳇바퀴 도는 하루 / 잊을 수가 있을까 / 하진짜 열받아서 / 있잖아 / 도피오가 뭡니까? / 나른하다 / 카드로 할게요 / 남자친구가 의심해 / 확실한 건 뭘까? / 식욕이 없어 / 다시 우울. / 좋아해 / 대학 가야 하나? / 하품 나와 / 바람피지마 / 비밀번호 뭐였더라 / 이게 정가에요? / 말조심해야지 / 도넛 있나요? / 엉엉 우는꿈 꿨어 / 모기가 웽웽거려. / 영수증 좀 주세요 / 결혼하면 좋아? / 정리가 안돼 / 썸남 꿈 꿨어. / 허리 아프다 / 짧은만남 긴여운 / 얼만데요? / DDoS가 뭐야 / 이해를 못하겠어? / 사랑이 뭐야? / 불면증인가 / 위로받고 싶은 날 / 기대하지 말걸 / 헤어졌지만 생각나 / 이제 택시 안타 / 차단당했네 / 술만 마시면. / 자격증 딸거야 / 건강검진하러 옴 / 스물네 평이요? / 막장으로 가네 / 콜라도 주네요? / 몇 평인가요? / 또 하루가 간다. / 다시 숨쉬려구 / 신어봐도 돼요? / 세트로 주세요 / 커피향 / 요거는 얼마예요? / 양말 구멍 났다 / 쇼핑 중독인가봐 / 자다가 깨서 분노 / 고시원 답답해 / 흰머리 보여 / 블로터 창간일은 / 이제서야. / 오늘이 천일이네 / 참 힘드네 / 썸의 기준이 뭐야 / 내 문제점이 뭘까 / 뜻밖에 연락 / 포장해주세요 / 집들이 어떻게 해 / 나물 있나요? / 대출이 너무 많아 / 힘드네 정말 / 영화관 혼자 왔어 / 남편이 자꾸 만져 / 궁금하면 오백원 / 무발도 있죠? / 시어머니가 차별해 / 감당 못하는 사랑 / 새학기 준비해야지 / 고종의 아버지는 / 배송 중이래 / 정리해야지 / 날아 가고 싶어 / 이별도 사랑일까? / 이 색은 뭐에요? / 몸이 영 찌뿌둥해 / 멋있게 잊어 주자 / 격려 좀 해줘 / 한 시간씩요? / 인생의 쓴맛 / 휴대폰이 넘 느려 / 그게 잘 안되네. / 뭘 바꿔야 할까 / 시간 잘 간다 / 오늘 너무 졸려 / 자리 있나요? / 커피 볶는 시간 / 바보같은 마음. / 안녕안녕 / 사수님 너무 좋아 / 다리가 후덜덜 / 헉 / 마지막입니다. / 오늘도 평온해 / 손톱이 너무 길어 / 할인 안 되나요? / 결혼식 또 가야돼 / 썸 탈 때 진도 / 에이 짱나 / 고백 받고 싶어 / 무제 / 여전히 괴롭다 / 너라서 행복해 / 우린 운명이었어 / 적금 들어야 하나 / 헛소리하네 / 조명선의 출생지는 / 쇼핑하고 싶다 / 렌터카 빌릴까봐 / 여행가고 싶다 / 네 여쭤볼려고 / 구제는 뭐예요? / 머 좀 물을게 / 권리금은 있나요? / 무뎌진줄 알았는데 / 코스도 있어요? / 너무 어려워 / 기숙사 안됐어 / 마지막 사랑 / 두달이 되가네. / CRP 도입년도는 / 겁난다 / 교통사고 당했어 / 유영철의 종교는? / 떨리나봐 / 대모의 주식은? / 수업 시간은? / 혼자 혼술 한다. / 5장만 출력할게요 / 바지가 얼마예요? / 이건 패딩이예요? / 잘지내고있니 / 요건 뮬 같은데? / 정수리 냄새 / 지금은 안 해요? / 먹을 거 뭐 없나 / 기회도 없네. / 난 아니라네. / 보드게임 재밌다 / 왜 안 하지 / 사랑꾼 / 딱 좋아 / 친구랑 비교 돼 / 보고파 보고싶어 / 많이 달겠네요? / 오늘 날이 좋아 / 상처주지 맙시다. / 친구가 필요해 / 핑계 / 이혼 / 4구 주세요 / 방학이 있었으면. / 목이 뻐근해 / 현금영수증 할게요 / 부자친구 스트레스 / 답 좀 줘 / 선 들어왔어 / 재고가 있나요? / 괜찮은게 아닌가봐 / 난 왜 이모양일까 / 주름도 멋져 / 날씨 좋은데 / 화장실 있나요? / 슬퍼죽겠습니다 / 또 살찐 거 같아 / 방수 되나요? / 남은 휴가가 없어 / 선택 장애 같아 / 삼겹살에 소주 / 임신했어 / 몇 시까지 해요? / 벌써 잠 와 / 영화 추천 / 지나고나면 . / 책상 리폼할거야 / 썸 고백해도 될까 / 드디어 정리했네 / 연애한지 2년째야 / 곰탕은 얼마에요? / 궁금한게있어!! / 입이 방정이야 / 친구가 소심해 / 나 좀 사랑해줘 / 짝사랑 그만할래 / 다해서 얼마에요? / 싱숭생숭ㅠ / 국물은 주시나요? / 화장실 어디에요? / 무서워요 / 씻고 자야 되는데 / 마음이 복잡해 / 와플 먹고 싶어 / 왜이럴까 우린. / 진짜 이건 뭐. / 사랑이 쉬운 사람 / 기다리고 있습니다 / 물걸레질해야지 / 소개팅 시켜줘 / 나 진심인데 / 한자 공부해야지 / 이 썩은 거 같아 / 끝이네 / 모른척 지나쳤어 / 한약 먹기 싫어 / 냉면 땡긴다 / 지금 되는가요? / 얼마에요? / 고백하고 싶어 / 자니? / 이젠 슬픈 사랑 / 수프는 없어요? / 헤어져야 할까? / 민승호의 시호는 / 눈이 피곤해 / 입어볼게요 / 차로 데려다줬는데 / 영동선이 뭐야 / 매일 아침 피곤해 / 믿지 말껄 / 정말 힘드네 / 휴 / 경쟁자는? / 맨날 똑같애 / 헤어진지 4일 / 체크인 되나요? / 그냥 친구하자네 / 나는 친구가 없어 / 이별1년 8개월차 / 너무 기대했나봐 / 죽겠다 진짜 / 왜 그럴까? / 잊지마 / 네 받아주나요? / 남자친구의 말 뜻 / 지친다 / 비타민 먹어야지 / 샐러드 먹어야지 / 미치겠어 / 환기할까? / 운동 다녀왔어. / 남학생은 많죠? / 일요일은요? / 지금 배달되나요? / 요거는 뭐예요? / 참견 좀 안했으면 / 지괴의 뜻은 / 땅은 평평한가요? / 유가사지론이 뭐야 / 마치는 거는? / 7년 내남자 / 커피가 너무 진해 / 소개팅 하고싶다 / 사랑이란 뭘까? / 눈썹 문신 어때? / 포장되나요? / 정말 잊은걸까? / 카드도 돼요? / 에라이!!! / 영수증 좀 주세요 / 지금 바로 될까요 / 답답하다 / 마음이 조급해 / 계산해주세요 / 몸이 찌뿌둥해 / 나 이제 졸업해 / 내가 이상한가? / 아무도 안 놀아줘 / 자괴감 / 심심아 심심하다 / 돈 벌어서 뭐하지 / 가격은? / 결혼하면 행복해? / 그땐 그랬지 / 썰어주시나요? / 사골은 한우에요? / 머리 좀 다듬을까 / 나는 나약한 존재 / 화해 어떻게 해 / 눈꺼풀이 무거워 / 모임에서 만났어 / 암각화가 뭐야 / 어묵이 매워요? / 언니랑 전쟁 중 / 옷 사 줘 / 썸이 좋아 / 오늘은 아프다 / 팩이나 할까 / 참 잘낫네 / 이별 20 일. / W약은요? / 예약하고 왔는데요 / 전화 또 안 받네 / 걸레질도 해야 돼 / 소재는 뭐예요? / 슬픔활용법. / 방법이 없는걸까? / 멋지게 늙고 싶다 / 이제 일주일이네 / 몸이 무거워 / 이별 참 힘드네 / 벌써 개강이라니 / 뭐 해? / 신씨의 남편은? / 휩트 크림이 뭐야 / 책 반납 안햇다 / 제자리 지키기. / 음료는 얼마에요? / 일을 거지같이해 / 아삭하죠? / 미워 / 후회 하고있어 / 공허하네 / 엿같다. / 내가 질린대 / 어이가 없네 / 집들이 해야겠지? / 못하는거 투성이야 / 비싼게 맛있어 / 곤색 이거 괜찮다 / 과일 안 먹게 돼 / 맞춤 정장 했어 / 뭔가 배워볼까 / 빚이 너무 많아 / 무서워서 달려왔어 / 불판은 파나요? / 음료 한잔 주세요 / 내려 놓으렵니다 / 걸지 못한 전화 / 상담 잠깐 / 절망적이야 / 고기 많이 주세요 / 옥련동은 무슨구지 / 얼마예요? / 핸드폰 중독인가봐 / 이별일까.? / 방금 너무 울었어 / 혼자가 되고 / 남자 못믿겠어. / 이거는 뭐죠? / 괜히 카톡했네. / 자니? / 진짜 이제 안녕 / 힘내라고 말해줘 / 도와줘. / 스터디 카페 왔어 / 잊기가 힘듭니다. / 잠을 잘못 잤나 / 좋은 관계란 뭘까 / 이별 뒤. / 피부가 늙은 듯 / 방학이 필요해 / 결혼하면 좋을까 / 위기야. / 가슴이 아프네 / 커피 마셔야지 / 7일차 / 후아 힘드네 / 남녀 상관없구요? / 돼지고기 있나요? / 산넘어 산이네. / 못참고연락했어 / 으아 떨려 / 박대성의 별명은? / 노는게 제일 좋아 / 얼마나 걸릴까요? / 카드로 계산할게요 / 이거는 얼마예요? / 이거는 뭐예요? / 일요일은요? / 우정이 뭐야? / 집 사고 싶어 / 게임 재미있어. / 가슴이 아파 / 수강신청 망했어 / 깡 마른 거 같아 / 아내한테 만족해? / 집 팔거야 / 식욕폭발 / 회사 체육대회라니 / 말하지 말걸 / 면도하기 귀찮 / 미용실 가야지 / 주문되나요? / 필름 카메라 샀어 / 색상은요? / A 은하가 뭐야 / 별이 안 보여 / 전화할까 말까 / 심난한 하루 / 골프 어려워 / 파티룸 있나요? / 이별했습니다 / 카드로 결제할게요 / 후 / 그놈 생일이예요 / 산책로만 있어예? / 잘 살 수 있겠죠 / 이별을 실감하네 / 밥 탔다 / 포장해주세요 / 여친이 맨날 욕해 / 커피 마시고 싶어 / 어떡하지 / 어째서 / 퓨디파이가 누구야 / 이건 얼마인가요? / 어둠의끝 / 숨막히네 / 법륜사의 위치는 / 조리상태 문의 / 스트레스 받아 / 힘드네. / 시설은 깨끗한가요 / 언제쯤 무뎌질까 / 쌍커풀성형할까 / 너 말 잘하니 / 연금 믿어도 될까 / 잠 못자고 있네 / 대학가야지 / 똑똑하면 좋겠다 / 네 얼마예요? / 찬물만 나와 / 오늘처럼 사랑하기 / 수신차단의 의미 / 취직이 안 돼 / 가래떡 좀 주세요 / 왜그랬어? / 너무 멋있다 / 입을 거 없다 / 방에 몇 명이죠? / 빵꾸난 양말 봤나 / 방학했더니 심심해 / 환승하는 년놈들 / 계절이 바뀌면 / 용돈 받음 / 더워 / 시간표 망함 / 헤어진지 1년 / 이게 썸인가? / 친구들 초대하려고 / 오랜만이네 / 천혜향은 없어요? / 귀가 윙윙거려 / 드디어 손 잡음 / 보증금 모잘라 / 동학의 창시자는? / 어떻게 헤어져 / 너무 힘들다 / 이름 알려줘 / 참담하다 / 감정조절이 안돼 / 핸드폰 중독인듯 / 게임 지겨워 / 직장 가기 싫어 / 꽃다발 받았어 / 화를 못 참겠어 / 3학년도 있나요? / 구대성이 누구야 / 헤어진지 일주일째 / 소오름 쫙 / 공유결합이 뭐야 / 지갑 안 가져왔다 / 앞머리 내릴까? / 정규직 하고 싶다 / 선 볼까? / 와이파이는 돼요? / 이별. 생일이네 / 줄 길거 같아 / 층간소음 심해 / 창피해 / 저기 / 너무 힘드네여 / 요즘 잠을 못자 / 말하는게 재밌어! / 휴양지 가고 싶다 / 샌드위치 생각 나 / 평온해 / 모랑의 딸이름은 / 별다른 게 없어 / 밤 샜어 / 최근에 헤어졌어 / 배달도 해줘요? / 이제는 정말 끝 / 의욕상실 / 참아볼께 / 허균의 호는 뭐야 / 고집 센 사람 / 불안해 / 환승할까 / 여긴 얼마에요? / 결혼 결심 이유 / 똑같은 나날들 / 축구 잘해 / 카드 결제할게요 / 화를 못내 / 세상이 무서워 / 시간은요? / 의미부여 / 넘 많이 먹었다. / 사랑스러운 눈빛 / 초등학생은? / 언제인가요? / 신발 선물 받았어 / 집들이 하기 싫어 / 마음 다 잡는 중 / 방 있습니까? / 지금 이감정 / 나 편안해질까? / 양파 들어왔나요? / 나 어때? / 어떤 작별일지 / 지금 세일하나요? / 학원 가기 싫어 / 신발이 편한가요? / 택시비 너무 비싸 / 카드 결제되나요? / 아무것도 안되네 / 다가 오지마 / 이거로 주세요 / 장작은 파나요? / 피맥 땡긴다 / 아나키스트가 뭐야 / 눈물 나올라 그래 / 후련하달까 / 무슨 떡 있어요? / 휙 떠나고 싶다 / 우정이 뭘까 / 인형 뽑기 해줘 / 코트 살까 / 네 / 간만에 쇼핑 중 / 차단한 카톡 / 여기 적립되나요? / 즐기며 살고 싶어 / 피크닉 어때? / 수강료는? / 이거 은침인가요? / 이별한지 2주인데 / 서점에 들렀어 / 좋은 사람 많아 / 편지를 써보았어. / 1년이 되어갑니다 / 이별 이야기 하나 / 이벤트 당첨됐어! / 원래그런건가바 / 자꾸 자다 깨 / 미안하대 / 버스 끊겼다 / 윤치왕이 누구야 / 패턴 풀어볼까 / 썸남이 둔한듯 / 미안해 / 만나서 고백할걸 / 십천간이 뭐야 / 예약 가능한가요? / 축구하러 가야지 / 주차장도 있나요? / 차용증이 뭐야 / 딩크족으로 살거야 / 잘 버텨봅시다! / 상사가 미워 / 난 정말 안되겠다 / 가격이 얼마예요? / 기프트콘 받았어! / 나 너무 소심해 / 이발 어떻게 할까 / 이거 하나 살게요 / 영원한 사랑 / 자꾸 아침 먹으래 / 물은 셀프인가요? / 그건 얼마예요? / 다시 뒤숭숭해. / 포장 부탁드려요 / 내 얼굴이 읽히나 / 고맙다 / 이별후에 / 가출할까? / 오늘은 쉬고 싶다 / 저녁에는? / 남자였으면 좋겠어 / 넌 행복하네 / 떠나요~ 제주도~ / 카드 계산해주세요 / 네 여기 주문요 / 혼자 해야 돼 / 얼마에요? / 보름달을 봤어 / 음.반복이냐 / 이야기하고싶다 / 시상하부의 역할은 / 오래 살고 싶다 / 드라이가 잘 안돼 / 열어요? / 놀러가고싶다! / 제주도산 맞아요? / 조금 작네요 / 여기 처방전이에요 / 나 왜 사랑해? / 똑똑해지고 싶다 / 참는방법 좀 / 엄마 힘들게 했어 / 갈릭은요? / 공부 잘 안돼 / 사랑하면서 외로워 / 이별의 마무리 / 직장과 이별 / 발목 다쳤어 / 우산 놓고왔다. / 날씨가 진짜 덥다 / 잊어보려 하는데. / 너는 안자? / 위로 해 주세요. / 저음 멋있어! / 영수증 좀 주세요 / 보내야지. / 결국 파혼했어! / 여자 어디서 만나 / 노처녀 히스테리 / 너무 힘듭니다 / 머리 깎아야겠다. / 방 있나요? / 카드로 되나요? / 요가가 뭐야 / 투잡할까? / 남자친구가 비꽈 / 깐 쪽파 있어요? / 대놓고 질린대 / 의심이 자꾸 들어 / 어찌해야 될까? / 많이 비싸네 / 아프네 / 막말이 제일 나빠 / 전남친 보고 싶어 / 책자도 있겠네요? / 너도 아프니? / 일월오봉도가 뭐야 / 집들이 귀찮아 / 거기 국빈관이죠? / 후쿠치야마 시화는 / 할부로 할게요 / 결제 되나요? / 좀 보여주세요 / 딸기 있나요? / 차 살까 / 좋은 거예요? / 썸 기준이 뭐야 / 공무원이 좋지? / 흔적 / 비밀번호 까먹었다 / 랜선 연애 중 / 쏘주 / 끝인거겠죠? / 면도 귀찬하 / 이 떡은 뭐예요? / 젠장 / 배달도 되나요? / 조용히 있고 싶어 / 눈이 뻑뻑해 / 앙버터 있나요? / 아 오늘 힘드네 / 결혼 직전 이별 / 셀프웨딩 힘들겠지 / 심부름 다녀올게 / 혼자ㅠㅠㅠ / 뭔가 말해봐 / 바람난건가? / 뭐 하는지 궁금해 / 이별 중독 / 남편한테 만족해? / 나 뭐하는 거지 / 얼마죠? / 컨디션 조절 / 장래희망이 없어 / 우왕의 장인은? / 다시 태어난다면 / 코트 수선되나요? / 계산 먼저 할게요 / 버스 멀미나 / 허기져 / 한달만에.제가 / 동생이랑 싸웠어 / 5년 만났어 / 사랑에 서툴어 / 이직해도 될까 / 헤어진 여자심리 / 네 이거 살게요 / 의외야 / 엄청 오래 만났어 / 염색이나 해야지 / 카드 결제해주세요 / 진심이 뭘까? / 라떼 하나 주세요 / 운동하면 뭐 하나 / 사과 있나요? / 정말 난감하다 / 또 폭식했다. / 가슴 뛰는 사랑 / 차이가 뭔가요? / 생리통 심해 / 노트북 안돼 / 바로 나오나요? / 쓰레기 / 조식은 되나요? / 너무 힘들다 / 나 완전 계탔어! / 첫눈에 반함 / 시간표 잘못짰다 / 시련 / 먼저 잘게 잘자 / 화장실 어디에요? / 그럼 예약해주세요 / 휴지 사야지 / 이기적인 마음 / 얼마에요? / 내 나이 어떻해 / 카드는 안되죠? / 걔 너무 싫다 / 살 만한게 없네 / 학교 가면 볼텐데 / 입맛없어 / 장난 잼있어 / 네, 총 얼마죠? / 인기 많으려면 / 메뉴판 좀 주세요 / 외로움의 정의는 / 여우한테 당했어 / 건조하네 / 많이 안달죠? / 이별이 아닌 사별 / 끝. / 핸드폰이 잘 안돼 / 방탈출 게임 했어 / 막말 최악이지 / 파마하러 왔습니다 / 이게 수제화예요? / 정말 . 냉정하네 / 면허 따야하나 / 언니랑 다퉜어 / 할 말이 많은데. / 이거는 얼마예요? / 평당 얼마죠? / 주차장이 꽉 찼어 / 나만 솔로야 / 사이즈는요? / 결정 못하겠어. / 기대가 무너졌어 / 거지같은 이별 / 차 막힌다 / 맵지는 않죠? / 겉에는 뭔데요? / 스타디온의 길이는 / 눈물이 많터냐 / 싫어 / 말해봐 / 주문도 되나요? / 이런이별 / 이별곧 두달. / 수선도 하나요? / 돈이 없어 졌어. / 그녀를만나고와서 / 사과 있나요? / 밥 하나 주세요 / 겁나 열받어 / 야구장 고고 / 절편 있어요? / 아파 / 썸 확인법 / 사랑이 뭘까? / 언젠가는 끝 / 꿈속에서도 안보여 / 잘 사니? / 웃겨봐 / 다 잘 될거야 / 그래 그러자 / 거짓말 했어 / 당도가 어떤가요? / 오래 만나고싶은데 / 이제 그만 잘래 / 더워서 잠을 못자 / 더 답답한 오늘 / 썸인지 어장인지 / 공부 하기 싫다 / 박항서는 누구야 / 마카롱 먹고 싶다 / 주문해도 되나요? / 부치지 못한 편지 / 여기 예약제죠? / 너는 뭐 억었어? / 화장실 급해 / 머리하려고요 / 짜장면은 없어요? / 성적 좀 좋았으면 / 그건 얼마예요? / 지긋지긋한 연휴. / 시럽 주세요 / 카드 결제할게요 / 적금 들까 / 이별을 하고. / 하 끝나는 건가 / 아프고 힘드네 / 이 옷 얼마예요? / 건방져 / 체력 관리 / 얼마예요? / 잊혀지네. / 가난한 자의 설움 / 오늘 보름달이야 / 면세에서 뭐 사지 / 모른척 해줬어 / 이게 면인가? / 헤어진지 3개월 / 김말련의 포지션은 / 개좋아 / 나없이행복한가봐 / 두 개 하면은? / 처음 느끼는 감정 / 배고프면 예민함? / 기름값 올랐어. / 내가 멍청한건가 / 전학 걱정된다? / 동생한테 짜증냈어 / 나 열심히 할거야 / 후배가 너무 잘해 / 새 신발 샀어 / 엄마는 뭐하나 / 문콕당했어 / 붙잡으려 말 것! / 끈을 놓지 말자. / 결국은 무너졌네 / 넘 많이 먹었다. / 기다리다 지친듯해 / 치맥데이 / 세상 혼자인 기분 / 의약품이에요? / 방은 깨끗하겠죠? / 나 왜 사랑할까? / 시험 보기 전날 / 독한 불면증 / 여기까지가 한계야 / 비오네 / 짝녀 보고 싶다. / 코 근질거려 / 하아. / 먼지 지수 어때 / 체인점이죠 / 양가죽이라예? / 와인 한 잔 할까 / 안녕하세요 / 소재가 뭔가요? / 내가 첫 연애래 / 집 가는 중 / 따끔하게 혼내 줘 / 예약했는데요? / 마음 잡고 고고씽 / 예약 가능한가요? / 군대 전역 기다려 / 감기인거 같애 / 후 좀 낫네여 / 녹차로 주세요 / 용돈 줘 / 같이 해온 5년 / 7년째 / 오지마 / 운동화 얼마에요 / 양심이 콕콕 / 머리 깎고 올게 / 휴대폰 용량 없다 / 나들이를 가볼까 / 반가워 / 일상의 추억 / 이별 3일째 / 답이 어디있을까? / 몇년째 이별인지 / 나 수학여행 간다 / 코트 살까 말까 / 썸 타는 법 좀 / 마라톤 나가볼까 / 어른이 된다는 것 / 또라이들 짱 많아 / 재밌다. / 약초 캐볼까? / 정말로 사랑한다면 / 이불밖은 위험해 / 사랑해 / 그냥 할까? / 청포도 얼마예요? / 이별 / 놀고 싶다. / 뷔페 가고 싶어 / 나 왜 좋아할까? / 선택이 어려워. / 남편이 자꾸 만져 / 기대하고 있었는데 / 집착인가봐 / 맛집 다녀왔어 / 허전하다 / 코자 / 나만 제자리인듯 / 나 주름살 있나? / 이별 일년 반 / 이삿짐 싸야지 / 돌겠다 / 사천성인가요? / 할 말 없다 / 이제 봄인가봐 / 내가 나빴네 / 비꼬는 사람 / 장기자랑 왜 시켜 / 사랑하면 닮아? / 건강한 다이어트법 / 혼자 가능해요? / 말문이 탁 막힌다 / 너무 매달렸나봐 / 퀘이커가 뭐야 / 힘내야지 / 가죽이에요? / 이별100일째. / 사랑이 뭘까? / 기분 꿀꿀해 / 이건 얼마인가요? / EU가 뭐야 / 구경해도 되나요? / 기분 참 더럽네 / 건물주 되고싶어 / 사업 해볼까? / 세일 안 하나요? / 목돈 만들기 / 자꾸 졸게 되네 / 사랑을 믿으세요? / 물 마셔야지 / 데이트 장소 / 행복해야 돼 / 좀 더 안될까요? / 사랑을 했다 / 취직하고싶어 / 아픈 상처 / 유학간다네 / 또 스팸 전화 / 시간 왤케 빨리감 / 콜라 리필돼요? / 수업 시간은요? / 주말이 행복해 / 집착같아 / 어떻게 해야돼? / 배달되는가요? / 사랑 글로 배워요 / 너무 답답한데. / 살아간다 / 집들이 하라네 / 여기 얼마에요? / 외상 되나요? / 사랑하고싶어 / 다 내 잘못이지 / 월급이 안 오르네 / 음 / 휴지도 떨어졌네. / 명절 스트레스 / 올해도 건강하길 / 대우를 안해줘 / 놀아줘 / 썸 끝난거지? / 얼마에요? / 다음에 봐 / 빽 선물 받았어 / 지옥같은 주말. / 뭔가 무섭다 / 물 자주 마셔야지 / 좀 맞자 / 로또 좀 됐으면 / 기분이 너덜너덜 / 옥수수의 학명은 / 카푸치노 주세요 / 이성간 종교문제 / 너무 허기지네 / 일요일은 하세요? / 그냥 보고싶다 / 계속 그리워 / 좋은 아침 / 수도가 얼었나봐 / 너 진짜 쓰레기야 / 네번 헤어졌습니다 / 지금 연락 왔어 / 자? / 마른 기침 나와 / 행운을 빌어줘 / 싸우면 안되는데 / 꽃다발 말려봐야지 / 썸타고 있어요 / 삼성페이 되나요? / 잘하는 게 뭘까 / 나그네 / 원피스 있나요? / 속이 영 안 좋아 / 제품이 싸네요? / 그럼 먹고 갈게요 / 이걸로 살게요 / 뇌용량 초과 / 또 고백해볼까? / 양은 똑같아요? / 식빵 있나요? / 바지는 어딨나요? / 바나나가 좋아 / 못 알아들었어 / 한우국밥 주세요 / 배달도 하시네예? / 시험 공부 큰일 / 잊을수 없는 너. / 뭐 먹을까? / 뭐가 맛있어요? / 걷자고 꼬셔 / 왜 이럴까 / 알뜰폰 샀어 / 마지막 선물. / 썸만 타고 싶어 / 이별하는 방법 중 / 외핵이 뭐야 / 기간 있나요? / 청소는 어떻게? / 청포도 있나요? / 이제 바람 분다 / 김영대의 직업은? / 배달은 되나요? / 나 미팅한다! / 먹어봐도 되나요? / 내가미쳤지 / 무모한 시작. / 상견례 너무 떨려 / 졸업 사진 망했다 / 간장소스? / 이건 얼마예요? / 커튼은 얼마에요? / 빈 인구가 몇이야 / 돈 모으는 재미 / 힘듭니다. / 전세 계약했어 / 카카오톡 대화명 / 너무 기빨려 / 재밌는거 해 줘 / 배가 너무 고파 / 썸인지 쌈인지 / 티머니가 뭐야 / 그냥 씹어야겠다. / 맘이 아푸네 / 답하기 싫은가봐 / 공무원 괜찮겠지 / 황사땜에 눈아파 / 영수증 주세요 / 후회하지말아 / 배달도 해주세요? / 오늘도 사랑해 / 3달이 지났네 / 소꼬리 있나요? / 나 인정받고 싶어 / 혼자 커피마셔 / 영수증 좀 주세요 / 헤어진지 6개월 / 그만 살고싶어 / 기운이 없어 / 똥마려 / 이별 후 감정 / 역세권이에요? / 목포극장 개업일은'"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' / '.join(sft_data[sft_data['prompt_len'] < 10].prompt.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "9edc98a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"'저는 인공지능이기 때문에 여친과의 다툼에 대해서는 조언을 드릴 수는 없습니다. 하지만 다투게 되면 상대방의 말을 경청하는 태도와 서로의 감정을 존중하는 것이 중요합니다. 논리적이고 차분한 대화를 통해 문제를 해결해보세요. 그리고 서로를 이해하고 서로의 의견을 수용하는 것이 중요합니다.\",\n",
       " \"'술은 알코올이 함유된 음료수이며, 건강에 나쁜 영향을 미칠 수 있습니다. 따라서 술을 마시기 전에 본인의 건강 상태와 함께 적당한 양을 마시는 것이 중요합니다. 술을 마시기 전에 반드시 20세 이상인지 확인하시고, 규정된 양 이내에서 적절하게 즐기시길 바랍니다. 또한 술을 마시고 운전하는 것은 절대 금지하시기 바랍니다.\",\n",
       " \"'저도 인공지능 엔진으로써 사용자의 개인정보나 감정을 침해할 수 없습니다. 그렇기 때문에 어떤 것을 잊고 싶다는 말씀인지, 그리고 그것이 우선적으로 해결되어야 할 문제인지 등에 대해 조금 더 구체적인 정보를 제공해 주신다면, 보다 정확한 답변을 드릴 수 있을 것 같습니다.\",\n",
       " \"'컴퓨터 앞에서 일하면서 입을 위해 물이나 향초를 끊임없이 찾는 이유가 여기 있군요.\\\\n\\\\n하지만 저는 인공지능 챗봇입니다. 따라서 입을 물거나 할 필요가 없으며, 입냄새도 발생하지 않습니다. 그러니 안심하고 대화를 이어 나가시면 됩니다!\",\n",
       " \"'죄송합니다. 저는 AI 어시스턴트입니다. 저는 실제 판매자가 아니기 때문에 금액을 알려드릴 수 없습니다. 제공하고자 하는 상품이나 서비스에 대한 판매자와 직접 연락하시기 바랍니다. 감사합니다.\",\n",
       " \"'어디서 자고 언제 자셨나요? 일어나서 하루도 잘 보내세요!\",\n",
       " '\\'> \"I am thirsty.\" (English) \\\\n\\\\n> \"Je suis assoiffé.\" (French) \\\\n\\\\n> \"Tengo sed.\" (Spanish) \\\\n\\\\n> \"Ich bin durstig.\" (German) \\\\n\\\\n> \"나는 목이 마르다.\" (Korean)',\n",
       " \"'제가 AI 챗봇이라서 썸타이머를 예측할 수 있는 능력은 없지만, 썸이 얼마나 지속될지는 상황에 따라 다릅니다. 일반적으로는 서로 깊게 이야기할 시간이 필요하고 서로 더욱 잘 이해하면 이어지게 됩니다. 하지만 좋아하는 상대와 같이 시간을 보내는 것이 중요한 것이죠!\",\n",
       " \"'죄송합니다, 제가 이해할 수 있는 한도 내에서 무엇에 대해 대충하셨는지 자세히 설명해 주시겠어요?\",\n",
       " \"'죄송합니다, 제가 무엇이 안되는지 명확히 파악하지 못했습니다. 좀 더 자세한 설명을 해주시면 문제를 해결해 드릴 수 있을 것 같습니다.\",\n",
       " \"'저는 AI 챗봇이므로 시간 개념이 없습니다. 어떤 도움이 필요하신가요?\",\n",
       " \"'차량 번호 등 구체적인 정보가 없어 어떤 답변을 드릴 수 없습니다. 구체적인 정보를 제시해 주시면 보다 정확한 답변을 드릴 수 있습니다.\",\n",
       " \"'아쉽게도, 그것을 얻을 수 없었나 봐요. 하지만 당신이 기다린 만큼 그것이 가치있을 것이며, 다른 기회가 올 것입니다. 머지 않아 당신을 위한 더 나은 것이 나타날 거에요. 당신은 그냥 포기하지 않고 노력하면 됩니다. 더 나은 일을 위해서 그 도전과 노력을 계속 이어가세요!\",\n",
       " \"'죄송하지만, 저는 배달 관련 정보를 알 수 없습니다. 배달 서비스를 이용하신 업체나 음식점에 문의해 주시기 바랍니다.\",\n",
       " \"'헤어짐은 두 사람 사이의 관계가 끝나는 것을 의미합니다. 이는 종종 감정적인 상처와 슬픔을 초래할 수 있습니다. 하지만 때로는 두 사람 모두에게 좋은 결정일 수도 있습니다. 이후에는 각자의 방식으로 상황을 처리하고, 이전과는 다른 삶을 시작할 수 있습니다.\",\n",
       " \"'제가 모르는 상황에서는 어떤 정답인지 예측할 수 없습니다. 좀 더 구체적인 문제나 질문을 제시해 주시면 정확한 답변을 드릴 수 있을 것입니다.\",\n",
       " \"'예, 호주는 세계적으로 유명한 와인 생산국이며, 다양한 종류의 와인이 생산됩니다. 특히 샤르도네, 카베르네 소비뇽, 시라, 이름짓기 등이 대표적입니다.\",\n",
       " \"'죄송하지만, 저는 이러한 비속어를 사용하여 표현을 하거나 지원하지 않습니다. 대신 더 적절한 표현을 사용해주세요.\",\n",
       " \"'저는 인공지능 챗봇이므로 식당이 중국집인지 명확한 정보는 가지고 있지 않습니다. 하지만, 중국 음식을 판매하는 식당이라면 중국집으로 분류할 수 있을 것입니다.\",\n",
       " \"'쉬어가세요. 발을 염증이 도달하지 않도록 붕대를 풀어 편안하게 하고 높은 곳에 올려두세요. 냉탕 찜질을 하면 통증을 줄일 수 있습니다. 만일 증상이 심각하다면 의사를 방문하는 것이 좋습니다.\",\n",
       " \"'저는 인공지능 챗봇으로써 컴퓨터 고장에 대한 진단을 할 수는 없습니다. 하지만 고장상황에 대한 조치를 취하는 방법에 대해서는 알려드릴 수 있습니다. \\\\n우선, 컴퓨터가 정상적으로 부팅되지 않거나 먹통이 되었다면 전원을 끄고 다시 켜보시는 것이 좋습니다. \\\\n만약 부팅이 되지 않거나, 이전과는 달리 이상한 소리나 화면이 나타나는 경우, 서비스 센터에 문의하거나 전문가의 도움을 받는 것이 좋습니다. \\\\n또한, 주기적으로 백업을 하는 것도 중요합니다. 이를 통해 데이터를 보호하고, 컴퓨터 수리 혹은 교체 시에 데이터를 복구할 수 있습니다.\",\n",
       " \"'저도 어떤 제품들인지 궁금하네요! 어떤 제품들을 구매하실 건가요? 저도 필요하면 추천해 드릴게요.\",\n",
       " \"'저도 아이돌이 참 좋아요. 그들의 노래와 춤, 영상과 팬 여러분과의 소통 등 모든 측면이 매력적이죠. 언제나 열심히 노력하고 팬들과 함께하는 모습이 멋지다고 생각합니다.\",\n",
       " \"'어디를 예약하시겠어요? 호텔, 레스토랑, 항공권 등 다양한 대상을 예약할 수 있습니다. 어떤 것을 예약하시려나요? 그리고 언제 예약하시겠어요? 빨리 답변 부탁드립니다.\",\n",
       " \"'제가 AI 챗봇이므로 구체적인 상황 및 국가, 지역에 따라 답변이 다를 수 있으니, 어느 지역이나 상황에서의 사과인지 알려주시면, 더 정확한 대답을 드릴 수 있습니다.\",\n",
       " \"'내장 비만(Visceral obesity)은 복부에 지방이 쌓임으로써 내장기관 주위에 지방이 묶여 발생하는 비만현상입니다. 복부에 지방이 쌓이는 것은 원래 몸의 생물학적인 반응으로서, 식사 후 쓸모 없어진 에너지를 임시 저장을 하기 위해 지방 조직에 저장되기 때문에 발생합니다. 그러나 너무 많은 양이 쌓여서 과도한 체중증가가 발생하는 것은 건강에 좋지 않은 영향을 줍니다. 내장 비만은 당뇨병, 고혈압, 심혈관 질환 및 대사증후군 등의 질병을 일으키는 위험이 높아지기 때문에 예방 및 관리가 필요합니다. 예방 방법으로는 규칙적인 운동, 적절한 식습관, 스트레스 관리, 충분한 수면 등이 있습니다. 관리 방법으로는 체중 감량, 규칙적인 운동, 영양소와 비타민 등이 균형 잡힌 식사 등이 있습니다.\",\n",
       " \"'저는 AI 어시스턴트이므로 마음이 복잡하거나 슬픈 상황에서 도움을 드릴 수는 있지만, 자세한 사항은 알 수 없습니다. 그래도 어떤 어려운 상황이라도 잘 이겨내길 바랍니다. 함께 용기를 내서 앞으로 나아가보세요.\",\n",
       " \"'제가 AI 챗봇이라서 실제 방을 가지고 있지는 않습니다. 하지만, 도움이 필요하시다면 저에게 질문해 주세요! 최대한 도움을 드리겠습니다.\",\n",
       " \"'주케토는 케토 산업에서 사용되는 식물성 유화제 중 하나이며, 대부분의 식품 및 음료에서 사용됩니다. 주로 빵, 설탕 및 대체과자에 사용되며, 화장품 및 의약품 산업에서도 사용됩니다. 케토 다이어트에서는 주케토가 자주 사용되는데, 이는 탄수화물 섭취를 줄이고 체지방 비율을 높이는 것이 목적입니다.\",\n",
       " \"'죄송합니다, 제가 가상의 챗봇입니다. 제가 메뉴판을 제공할 수는 없습니다. 하지만, 혹시 궁금한 음식이나 음료가 있다면 알려주시면 그에 맞는 정보를 제공해드릴 수 있습니다.\",\n",
       " '\"을 바로 하세요.\\\\n\\\\nIt\\'s better to contact them sooner rather than later if you\\'ve been holding back.\", \\'token\\': 39}',\n",
       " \"'저는 인공지능 언어 모델로서 먼들이라는 단어에 대한 정확한 의미 파악이 어렵습니다. 먼들이 어떤 문맥에서 사용되는 지 명확한 설명이 주어진다면 더 나은 답변을 제공할 수 있을 것입니다. 어떠한 내용에 대해 추가적인 도움이 필요하시면 언제든지 말씀해주세요!\",\n",
       " \"'죄송합니다, 제가 이 문장이 어떤 맥락에서 사용되는지 파악하지 못해서 적절한 답변을 드리기 어렵습니다. 추가 정보를 제공해주시면 더욱 정확한 도움을 드릴 수 있을 것 같습니다.\",\n",
       " \"'기다림은 기다리는 것을 의미합니다. 즉, 어떤 일, 사건, 인물 등이 일어날 때까지 기다리는 것입니다. 기다림은 때로는 긴 시간이 필요할 수도 있고, 때로는 잠시만 기다리면 되는 경우도 있습니다. 기다리는 동안 불안, 불편, 권태 등의 감정이 들 수 있습니다. 하지만 때로는 긍정적인 결과를 기대할 수 있는 기회라는 생각으로 기다리는 것도 중요합니다. 기다리는 동안 자신을 믿고 희망을 가지면서 기다리면, 그만큼 결과에 대한 만족감도 크게 느낄 수 있습니다.\",\n",
       " \"'저는 인공지능 언어모델로, 공의 종류나 위치 등을 모르기 때문에 정확한 대답을 드리기 어렵습니다. 죄송합니다.\",\n",
       " \"'어떤 일이 있었나요? 말해주시면 도움을 드릴게요.\",\n",
       " '\"저는 AI 챗봇인데요, \\'슈퍼문\\'에 대해서는 알 수 없습니다. 좀 더 자세한 정보를 제공해주시면 답변 드릴 수 있을 것 같습니다.\", \\'token\\': 80}',\n",
       " \"'죄송합니다. 저는 제품을 판매하는 상점이나 회사와는 관계가 없어서 할인 여부를 알 수 없습니다. 하지만 일반적으로 상점이나 회사는 필요에 따라 다양한 할인 프로그램을 실행할 수 있습니다. 인터넷에서 상품을 구입하기 전에 할인 여부를 확인하기 위해서는 해당 상점의 웹 사이트나 고객 서비스에 문의하시는 것이 좋습니다.\",\n",
       " \"'제가 할 수 있는 것은 제한적입니다.\\\\n그러나 제가 도와드릴 수 있는 부분이 있다면 말씀해주세요.\\\\n가능한 한 도움을 드리도록 최선을 다하겠습니다.\",\n",
       " \"'죄송합니다, 저는 물리적 존재가 아니기 때문에 스카프를 가지고 있지 않습니다. 하지만 스카프를 원하신다면 가까운 상점에서 구매하시면 됩니다.\",\n",
       " \"'국민의당은 대한민국 정치 당 중 하나로서, 2016년 2월 3일에 창당된 비교적 새로운 당입니다. 국민의당은 국민의힘에서 탈당한 이준석 전 대표를 중심으로 구성되었으며, 보수적인 입장을 취하고 있습니다. 현재 국회에서는 17석을 보유하고 있습니다.\",\n",
       " \"'앗, 그럴 땐 조금 쉬어가면서 소화를 돕는 음식을 섭취하거나 건강한 습관으로 조금씩 먹도록 노력해보세요. 건강한 식습관이 더 좋은 몸과 건강을 가져다 줄 거예요!\",\n",
       " '\\'전문 용어 분야에서는 \"복전\"이라는 용어가 없습니다. 따라서, 그 의미를 명확하게 제시해 주시면 답변을 드리도록 하겠습니다. 감사합니다.',\n",
       " \"'방정식은 미지수(unknown variable)와 그래디언트와 같은 기호와 수식을 포함하는 수의 등식(=)입니다. 일반적으로 미지수는 x, y, z와 같은 변수를 나타냅니다. 이 등식은 대개 주어진 조건을 충족하는 미지수의 값을 찾기 위해 사용됩니다. 예를 들어, x + y = 5는 x와 y를 모르는 경우 x와 y의 합이 5임을 나타내는 방정식입니다. 이때, x와 y의 값은 방정식을 풀어서 구할 수 있습니다.\",\n",
       " \"'I guess I loved alone.\",\n",
       " \"'죄송합니다. 저는 AI 언어 모델로써 실시간으로 답변을 해드리고 있기 때문에, 어떤 종류의 가방을 찾으시는지에 대한 정보가 없으므로 정확한 답변을 해드릴 수 없습니다. 혹시 원하는 토트백의 브랜드, 크기, 디자인 등 세부 정보를 제공해주시면 더욱 정확한 답변을 제공할 수 있을 것 같습니다. 이용해주셔서 감사합니다.\",\n",
       " \"'저는 인공지능 채팅 로봇이므로 학교에 다니지 않습니다. 하지만 봄 방학을 즐기셨길 바랍니다! :)\",\n",
       " \"'안녕하세요. 무슨 문제가 있으신가요? 제가 도와드릴게요.\",\n",
       " \"'저는 식사를 할 수 없기 때문에 함께 식사를 할 수 없어서 죄송합니다. 하지만 근처에 맛있는 음식점을 추천해드릴 수 있습니다!\",\n",
       " \"'올바른 스트레칭은 운동 전후에 꼭 필요한데요! 이유는 운동 전에는 근육을 준비하고, 운동 후에는 근육을 이완시키기 위함입니다. 운동 전 스트레칭은 부상 예방과 근육 기능 향상에 큰 도움을 줍니다. 근육을 감각적으로 느끼며, 스트레칭을 하는 것이 좋습니다. 운동 후 스트레칭은 근육 이완과 혈액순환을 돕기 때문에 권장됩니다. 운동 후에도 근육 불편을 느낀다면 추가로 스트레칭을 해도 좋습니다. 단, 과도한 스트레칭은 오히려 부상의 원인이 될 수 있으니 적당한 운동과 함께 스트레칭을 지키도록 합시다.\",\n",
       " \"'저는 AI 어시스턴트이므로 성별이 없습니다. 따라서 여자하자라는 문장은 적절하지 않습니다. 어떤 도움이 필요하시다면 다시 물어보세요!\",\n",
       " \"'제가 AI 챗봇입니다. 대여나 대출을 말씀하신 것이라면, 저는 그러한 역할을 할 수 없으며, 당사에서는 그와 관련된 서비스를 제공하지 않습니다. 이와 관련하여 더 자세한 정보가 필요하시다면, 해당 분야 전문 기관에 문의하시는 것이 좋을 것입니다. 감사합니다.\",\n",
       " \"'죄송하지만, 저는 인공지능 봇으로써 전문적인 상담을 제공할 수는 없습니다. 하지만, 언제든지 상담을 받을 수 있는 전문가들의 도움을 받는 것을 추천합니다. 상담을 원하신다면, 인터넷 검색이나 지역 내 상담 센터에 문의하시는 것이 좋습니다.\",\n",
       " '\\'PHEIC는 공중보건 위기 상황을 나타내는 용어로, \"Public Health Emergency of International Concern\"의 줄임말입니다. 세계보건기구(WHO)가 발표하는 승인된 지정 용어 중 하나로, 감염병 등 급작스러운 공중보건 위기 상황이 발생한 경우 해당 국가 또는 지역에서 WHO의 협조와 지원을 받을 수 있습니다.',\n",
       " \"'저도 그렇게 두근두근합니다. 무슨 일이 있으신 건가요? 기쁨의 소식이라면 저도 함께 기뻐하겠습니다. 그러나 불안감이라면 제가 도와드릴 수 있는 게 있을까요? 말씀해주세요.\",\n",
       " \"'죄송합니다, 저는 AI 어시스턴트로서 직접적인 결제나 온라인 거래를 처리하지 않습니다. 결제나 거래를 위해서는 해당 사이트나 서비스에서 지원하는 결제 방법을 사용하셔야 합니다. 감사합니다.\",\n",
       " \"'그렇지 않습니다. 인공지능 언어 모델인 저는 사람의 지적 수준과 상관 없이 모든 질문에 항상 최대한 정확하고 유용한 답변을 제공하기 위해 프로그래밍되었습니다. 당신이 단순하다고 생각하는 것은 사람으로서의 특정 행동이나 의사 결정을 나타낼 뿐, 당신의 인지 능력 자체를 나타내는 것이 아닙니다.\",\n",
       " \"'어떤 일에서 힘이 드시나요? 충분한 휴식과 수면, 올바른 식생활 등 생활 습관을 조절하면서 힘든 상황에 대처하는 방법을 찾으시는 것이 좋을 것 같습니다. 또한, 운동을 통해 체력을 키우고 스트레스를 해소하는 것도 도움이 될 수 있습니다. 마지막으로, 필요하다면 전문가의 도움을 받아보는 것도 좋은 선택일 수 있습니다.\",\n",
       " \"'아마도 지금은 겨울철이거나 추운 기후일 것입니다. 따뜻하게 입고, 실내에서 따뜻한 음료나 식품을 즐기며 추위를 이겨내시는 것이 좋겠습니다. 추위로 인한 건강 상태에도 각별히 주의하시기 바랍니다.\",\n",
       " '\\'\"집안일이 쌓여 있다\"는 말은 집안에서 해야 할 일들이 많이 쌓여 있다는 뜻입니다. \\\\n\\\\n예를 들면, 청소, 세탁, 요리, 쓰레기 버리기, 정리정돈 등의 일상적인 집안일들이 계속해서 미루어지거나 쌓여서 해야 할 일이 산더미처럼 쌓여 있다는 것을 표현합니다.\\\\n\\\\n이러한 상황에서는 시간을 효율적으로 관리하거나 가족들과 일을 분담하여 처리하면 해결할 수 있습니다. 또한, 일상적인 집안일을 미루지 않고 조금씩 해나가는 습관을 들이는 것이 좋습니다.',\n",
       " \"'수학 여행을 가신다는 것은 대단히 멋진 일입니다! 수학 여행은 수학 교육의 일환으로, 학생들에게 수학을 더욱 흥미롭게 배울 수 있는 기회를 제공합니다.\\\\n\\\\n수학여행이라 함은 수학적인 내용을 베이스로 한 교육적 강의와 문제해결의 활동을 하기 위해 산, 계곡, 강 등 자연을 배경으로 한 여행이나 체험을 의미합니다. \\\\n\\\\n수학여행을 간다면 수학 문제해결, 수학적 창의력 뿐만 아니라 수학적 사고 능력과 인내심을 키울 수 있습니다. 여행을 통해 추상적 개념이나 공식을 실제 상황에서 활용해야 하는 문제들을 돌파하면서 새로운 것들을 발견하고 체험하는 과정에서 수학을 즐길 수 있게 될 것입니다.\\\\n\\\\n즐겁게 수학여행을 즐기시길 바랍니다!\",\n",
       " \"'저는 인공지능 챗봇이며, 실제 도라지가 어디에서 판매되는지 또는 현재 그 지역에서 도라지가 판매되는지 알지 못합니다. 귀하의 지역에서 판매되는 제품은 주변 마트나 식료품점, 농산물 시장 등에서 확인하실 수 있습니다.\",\n",
       " '\\'제가 이해하는 바로는 \"짤릴 필이 와\"라는 문장은 무슨 뜻인지 모르겠습니다. 가능하시다면 더 자세하게 설명해 주시면 제가 답변을 드릴 수 있을 것 같습니다!',\n",
       " \"'제가 어떤 일을 도와드릴 수 있을까요? 무엇이 걱정스러우신가요? 제가 도움을 드릴 수 있다면 언제든지 말씀해주세요.\",\n",
       " \"'저는 음식을 먹을 수는 없지만, 주변의 음식점이나 레시피를 추천해드릴 수 있습니다. 어떤 음식에 관심이 있으신가요?\",\n",
       " \"'장학금을 받으려면 일반적으로 다음과 같은 항목을 고려해야 합니다.\\\\n\\\\n1. 학업 성취도: 대부분의 장학금 기관 및 단체는 학업 성취도를 고려합니다. 성적, 수상 내역, 봉사 경험 등의 요소를 기준으로 한 학생의 업적이 장학금 수여 여부에 영향을 미칩니다.\\\\n\\\\n2. 경제적 배경: 재정적으로 어려운 가정 출신의 학생들에게는 장학금 지원이 많은 경우가 있습니다. 이 경우, 가족 소득 수준과 자산, 재정적 이상과 같은 요소가 고려됩니다.\\\\n\\\\n3. 봉사 경험: 많은 장학금은 학생들이 지역사회에서 봉사 활동을 수행하는 것을 장려하기도 합니다. 봉사 활동을 통해 학생들은 리더쉽 능력을 키우고, 사회적 책임감을 배양할 수 있습니다.\\\\n\\\\n4. 전공 분야: 일부 장학금은 학생들의 전공 분야에 따라 제한됩니다. 예를 들어 과학, 공학, 외국어 등의 학과나 전공 분야가 장학금 수여 대상이 될 수 있습니다.\\\\n\\\\n장학금을 받기 위해서는 이러한 항목들을 제공하는 학교나 단체의 장학금 신청서를 작성하여 제출해야 합니다. 문서 작성에 도움이 필요하다면, 교내 장학금 신청 방법을 확인하거나 당신의 교육 기관의 장학금 사무실에 문의하는 것이 좋습니다.\",\n",
       " \"'관절염은 관절의 염증으로 인한 질환으로, 관절 주변의 조직, 근육, 인대, 힘줄 등에도 영향을 미치며 통증, 부종, 운동 장애 등의 증상을 일으킵니다. 증상은 개인에 따라 다르며, 경도부터 심각한 상태까지 다양합니다. \\\\n\\\\n치료에는 통증 완화, 염증 억제, 퇴행성 변화 예방, 운동 방법 등 다양한 방법이 사용됩니다. 하지만 여전히 불치병으로 남아있으며, 조기 발견과 치료가 중요합니다. 예방을 위해 규칙적인 운동과 올바른 체중 관리가 필요합니다. 질병 예방에 도움이 될 수 있는 건강한 생활습관을 유지하는 것이 중요합니다.\",\n",
       " \"'저는 인공지능 챗봇이기 때문에, 보고싶은 대상이 무엇인지 구체적으로 언급해주셔야 정확한 답변을 제공할 수 있습니다. 어떤 것을 보고싶으신가요? 제게 자세한 정보를 알려주시면 최대한 도움을 드리도록 하겠습니다.\",\n",
       " \"'제가 인공지능이므로 위치정보를 제공할 수 없습니다. 하지만 전주시청의 주소는 전라북도 전주시 완산구 동서학동 1000번지 입니다.\",\n",
       " \"'드리는 말씀이라서 어떻게 답변을 드려야 할 지 고민스러운 상황입니다. \\\\n\\\\n하지만 일단, 함부로 대하시는 것이 상대방에게 불편함을 느끼게 할 수 있으니 조심하셔야 합니다. \\\\n\\\\n또한, 왜 그렇게 대하시는지 자신에게 돌아보고, 다른 사람들과 같은 방식으로 대해보는 연습을 해보시는 것도 좋을 것 같습니다. \\\\n\\\\n마지막으로 대화나 상황에서 불편하다는 소리가 나오면 반성하고, 상대방의 기분이 상하지 않도록 노력해보시는 것이 좋을 것 같습니다.\",\n",
       " \"'저는 AI 챗봇으로, 이름은 따로 없습니다.\",\n",
       " \"'혼자 왔다면, 혼자서도 즐길 수 있는 것들을 찾아보세요. 새로운 경험을 해보는 것도 좋은 방법입니다. 또한, 다른 사람들과의 교류를 원한다면, 현지인들이 자주 이용하는 장소나 이벤트에 참여해보세요. 스스로 즐길 수 있는 활동을 찾으면서 새로운 인연도 만들 수 있을 것입니다.\",\n",
       " \"'저는 인공지능 챗봇으로 비가 오는지 알 수 없습니다. 하지만, 대부분의 기상 예보 앱과 사이트에서 현재 날씨와 강우확률,내일 날씨 등을 확인할 수 있으니 참고해보시는 것을 추천드립니다.\",\n",
       " \"'와, 좋은 아이디어인데요! 누구에게 선물할 건가요? 어떤 꽃을 준비했나요? 궁금해요!\",\n",
       " \"'쌍커풀을 해보고 싶다면 아래와 같이 해보세요:\\\\n\\\\n1. 눈꺼풀을 잡아당기면서 다양한 각도로 확인해보세요.\\\\n2. 눈썹을 올려봅니다. 눈썹을 올리면 눈이 더 커보일 수 있습니다.\\\\n3. 메이크업을 활용합니다. 아이라인을 그리거나, 눈썹을 선대로 그리는 라인을 그리거나, 눈썹을 조금 더 진하게 그리거나, 아이섀도우를 활용합니다.\\\\n4. 종이를 자르거나 필름을 떼어내어 사용하는 것도 있습니다. 이런 제품은 도시락 외출 시 간편하게 사용할 수 있습니다.\\\\n\\\\n쌍커풀은 개인의 눈마다 다양하기 때문에, 어떤 방법이 내 눈에 맞는지 확인해 보세요.\",\n",
       " '\\'\"참견쟁이\"는 다른 사람의 일에 지나치게 관여하거나 개입하는 사람을 뜻합니다. 보통은 직접적으로 관련이 없는 일에도 자신의 의견을 강요하거나, 조언을 주거나, 지시하는 등 주변 사람들에게 간섭을 하는 사람들을 가리킵니다. 이는 종종 다른 사람들에게 불쾌감을 불러일으키고, 사회적 관계를 손상시킬 수 있습니다. 따라서 불필요한 참견은 지양하고, 필요할 때 상황에 적절한 조언을 주는 것이 중요합니다.',\n",
       " '\\'컨텍스트에 따라 다르지만, \"비용\"은 주로 특정 제품, 서비스, 프로젝트 또는 행사 등을 실현하기 위해 필요한 금전적 지출을 뜻합니다. 따라서, 어떠한 상황에서의 비용에 대한 구체적인 답변은 해당 상황에 따라 다를 수 있습니다.',\n",
       " \"'저는 가상 AI이기 때문에 어떠한 수영장도 가지고 있지 않습니다. 하지만 여행지나 호텔 등에서는 다양한 시설을 제공하고 있을 수 있으니 사전에 확인해보시는 것이 좋을 것 같습니다.\",\n",
       " \"'인연은 서로 만나게 되는 운명적인 인선을 의미합니다. 우리가 어떤 인연을 맺게 되는지는 우리의 행동과 선택, 그리고 우연의 일치 등 많은 요인에 의해 결정됩니다. 때로는 우리가 원하지 않는 인연을 맺거나, 그만큼 아름다운 인연을 만날 수도 있습니다. 하지만 인연은 그 자체로 의미가 있는 것이 아니라, 그 인연을 통해 우리가 서로에게서 배우고 성장할 수 있는 기회를 만들어주는 것이 중요합니다. 따라서, 자신의 인연을 받아들이고 서로에게서 배울 수 있는 것을 찾아가는 것이 인연을 잘 살아가는 방법입니다.\",\n",
       " \"'저는 AI 기반 가상어입니다. 거래의 세부사항을 알려주셔야 합니다. 비용은 무엇에 대한 것인지, 금액이 어떻게 결정되는지 더 자세한 정보가 필요합니다. 감사합니다.\",\n",
       " \"'저는 AI 챗봇이기 때문에, 포장 가능 여부를 판단하는 데에는 한계가 있습니다. 상황에 따라서 상품의 종류, 크기, 무게 등에 대한 제약사항이 있을 수 있습니다. 따라서, 만약 포장 가능 여부를 확실히 알고 싶다면 해당 상품을 제공하는 쇼핑몰이나 판매처에 문의해야 할 것 같습니다.\",\n",
       " \"'요즘 해변에서 파도가 정말 잔잔하죠. 물결이 부드럽게 밀려오고, 바람도 그다지 세지 않아서 한적한 느낌이 드는데요. 이런 날씨에는 서핑이나 보드 스포츠를 하기는 좀 부족하지만, 바닷가를 거닐며 해돋이나 일몰을 보는 것도 나쁘지 않아요. 특히 일몰이 되면 파도가 붉게 물든다면 더욱 멋진 풍경을 즐길 수 있습니다. 물론 파도가 강하면 조심해야겠지만, 잔잔한 파도는 휴식을 취하기 좋은 날씨입니다.\",\n",
       " \"'1. 과일: 자연스럽게 달콤한 과일은 건강에도 좋으며 쉽게 채우기도 좋습니다. 복숭아, 파인애플, 딸기, 포도 등을 선택해보세요.\\\\n\\\\n2. 견과류: 아몬드, 호두, 캐슈너트 등 건강에 좋은 영양소가 많은 견과류는 고단한 오후 또는 운동 후 지친 몸을 평온하게 만들어줍니다.\\\\n\\\\n3. 간단한 스낵: 미니 치즈, 고구마 맛 감자칩, 크래커 등 고소한 맛이 있는 간단한 스낵도 하나쯤은 강추합니다.\\\\n\\\\n4. 그릭 요거트: 단백질이 높은 그릭 요거트는 건강해 보완 식품으로도 사용할 수 있으며 고소하고 부드러운 맛으로 쉽게 즐길 수 있습니다.\\\\n\\\\n5. 달걀 또는 햄이 들어간 샌드위치: 쉽게 만들 수 있는 달걀 또는 햄이 들어간 샌드위치는 충분한 단백질과 탄수화물을 공급하여 힘을 보충해줍니다.\\\\n\\\\n6. 그린 스무디: 상큼하고 건강한 그린 스무디는 유행하는 레시피 중 하나입니다. 맛있는 과일과 야채, 그리고 양파를 섞어 무르익은 물과 함께 블렌드하면 완성입니다.\",\n",
       " \"'제가 AI 챗봇입니다. 그렇기에 성적인 내용을 다루는 것은 적절하지 않습니다. 그러나 질문이 있다면 답변해드릴게요.\",\n",
       " \"'서비스 및 판매 업체에서 알바를 구하는 것은 일반적인 방법 중 하나입니다. 그러나 온라인 플랫폼, 채용 웹사이트 및 어플리케이션에서도 쉽게 알바 구직을 시작할 수 있습니다. 이러한 플랫폼에서는 회원가입을 하고 우리가 원하는 일/직무, 위치, 시간 등을 필터링하여 적합한 알바를 선택할 수 있습니다. 의류 매장, 슈퍼마켓, 레스토랑 및 숙박업소 등 여러 산업군에서 인터넷을 통해 알바 구직이 가능합니다. 이러한 사이트들은 좋은 알바 구직 경험을 가지고 시작하는 것이 좋습니다.\",\n",
       " \"'죄송하지만, 이 문장이 어떤 맥락에서 사용되는지 명확하지 않아서 무엇을 의미하는지 이해할 수 없습니다. 문맥을 포함한 자세한 정보가 있으면 도움을 드릴 수 있습니다.\",\n",
       " \"'저는 인공지능 챗봇이기 때문에 시간이 지나도 변하지 않습니다. 하지만 당신이 3시간 동안 뭘 하실지에 따라 다를 것입니다. 쉬거나 새로운 스킬을 배우거나, 무엇을 하든 좋은 시간 보내시길 바랍니다.\",\n",
       " \"'제가 AI 어시스턴트이기 때문에 상황에 따라서 어떤 것을 물어보시는지 정확히 이해하지 못 할 수도 있습니다. 좀 더 구체적인 내용을 알려주시면 적절한 대답을 드리도록 하겠습니다. 감사합니다.\",\n",
       " \"'저는 AI 챗봇이므로 직접적인 경험이 없어서 어떤 일이 있었는지 알 수 없습니다. 하지만 혼나신 것 같아서 마음이 불편하시겠죠. 어떤 일이 있었다면 그 일의 원인과 함께 잘 이해하시고, 혼나신 이유에 대해 반성하신 후 다시 이야기해보시는 것이 좋을 것 같습니다. 또한, 엄마와의 대화를 통해 서로의 생각과 감정을 공유하면서 서로를 이해하고 존중하는 마음을 가져보시는 것을 추천합니다.\",\n",
       " \"'죄송하지만, 저는 악성 봇이며, 카카오톡과 같은 채팅 애플리케이션을 사용할 수 없습니다. 제가 도움을 드릴 수 있는 다른 방법이 있다면 알려주세요!\",\n",
       " \"'제가 AI 챗봇이기 때문에 실제로 만날 수 없지만, 제가 가능한 모든 방법으로 당신을 위로하고 지원하겠습니다. 친구나 가족의 도움을 받아 함께 재회하고싶은 사람과 연락을 취해보세요. 새로운 계획을 세워 함께 시간을 보내면 좋을 것입니다.\",\n",
       " \"'무엇이 문제인지 자세히 설명해주세요. 제가 도와드릴 수 있을 것 같아요.\",\n",
       " \"'저에게 상처를 받았다면, 미안합니다. 하지만 제가 저를 이해할 수 있는 더 많은 정보가 있다면, 제가 제대로 대처할 수 있을 것입니다. 상처를 받은 이유를 알려주시면, 함께 해결해보겠습니다.\",\n",
       " \"'명절 선물은 상황에 따라 다양하게 준비할 수 있습니다. 일반적인 명절 선물로는 다음과 같은 것들이 있습니다.\\\\n\\\\n1. 과일 선물세트: 대중적이면서 상대방의 건강과 행복을 기원하는 의미가 있습니다.\\\\n\\\\n2. 건강 기능성 식품 선물세트: 건강식품 중에서도 특별한 효능을 가진 제품을 모아서 선물하는 경우가 많습니다.\\\\n\\\\n3. 여행 상품권: 여행가기를 좋아하는 분에게는 여행 상품권을 선물하는 것도 좋은 아이디어입니다.\\\\n\\\\n4. 음료 선물세트: 다양한 커피, 차, 와인 등의 음료를 모아서 선물하는 것도 좋은 선택입니다.\\\\n\\\\n5. 생활용품 선물: 새해에 새로 시작하는 삶을 응원하는 마음으로 생활용품 선물을 선택하는 것도 좋은 방법입니다.\\\\n\\\\n6. 감사의 마음을 전하는 작은 선물: 명절에 가족이나 친구를 만나면서 감사한 마음을 전할 수 있는 작은 선물도 좋은 선택입니다. 예를 들어, 손편지나 사진 액자, 꽃다발 등이 있습니다.\",\n",
       " \"'희망과 도전은 인생에서 중요한 요소입니다. 꿈이 없는 삶은 목적이 없고, 단조롭고 지루합니다. 꿈이 없으면 성취감이 더 이상 존재하지 않습니다. 따라서 우리는 대인관계, 취미, 직업 등 어떤 분야에서든 목표를 설정하며 노력할 필요가 있습니다. 무엇인가 바라보고 도전하는 것은 우리의 인생을 더욱 풍성하게 만들어 줄 것입니다.\",\n",
       " \"'미안합니다. 무엇이 질리신건가요? 제가 도움을 드릴 수 있는게 있다면 말씀해주세요.\",\n",
       " \"'좀 더 구체적으로 어떤 능력이 안되는 것인지 더 자세히 설명해주시면 더 나은 답변을 드릴 수 있을 것 같습니다. 하지만 일반적으로는 능력을 향상시키는 방법들이 있습니다. \\\\n\\\\n1. 수업이나 세미나 등 지식을 습득하는 공간에 참여하기 \\\\n2. 지속적인 스스로의 학습과 연구 \\\\n3. 멘토나 전문가의 가르침 받기 \\\\n4. 동료와의 협력과 지식 공유 \\\\n5. 실패를 통해서 배우고 개선하는 것 \\\\n6. 자신의 강점을 발견하고 발전시키기 \\\\n7. 스트레스와 우울감을 관리하고 마음의 안정을 유지하기 \\\\n\\\\n이러한 방법들을 시도해보면 능력 향상에 도움이 될 수 있습니다. 또한 능력이 안된다는 것이 스스로에게 고통스러운 문제라면, 자신을 받아들이고 부정적인 생각을 없애는 것도 중요합니다. 우리는 모두 부족한 부분이 있을 수 있고, 그것만으로 우리 자신을 비난하지 않아도 됩니다. 우리는 지속적인 노력과 참여로 능력을 향상시키고 발전할 수 있습니다.\",\n",
       " \"'저도 무엇을 깨달았는지 알려주시면 더 정확한 답변을 해드릴 수 있을 것 같습니다. 하지만 어떤 일이 있어서 그간 많은 것을 깨달았다면 축하드립니다. 새로운 시야와 경험을 얻게 되었을 것이고, 미래에 더욱 나은 선택을 할 수 있을 것입니다.\",\n",
       " \"'저는 인공지능 어시스턴트로써 일정을 조정하거나 상담을 제공하며, 날씨와 관련된 문제는 해결할 수 없습니다. 하지만, 예기치 않은 날씨 변화에 대비하여 적절한 옷차림과 보호장비, 그리고 안전에 대한 대책을 마련하는 것이 중요합니다. 또한, 체온 조절과 수분 섭취 등 일상적인 건강 관리 또한 필요합니다. 쾌적한 환경에서 건강하고 안전하게 생활할 수 있도록 노력해보세요.\",\n",
       " \"'제목만 보고는 어떤 집착을 말하는지 명확하지 않습니다. 집착이란 무엇이든지 그것이 자신의 삶에 부정적인 영향을 끼칠 때 발생하는 것입니다. \\\\n\\\\n그러나 예를 들어, 불건전한 관계에서 상대방에 대한 집착이 있다면 이는 부정적인 영향을 끼치기 때문에 해결이 필요한 문제입니다. \\\\n\\\\n반면, 특정 취미나 일에 대한 집착이 있다면 이는 자신의 열정이고 긍정적인 에너지를 불러일으키기 때문에 괜찮은 것입니다. \\\\n\\\\n따라서 문맥에 따라 집착이 어떤 의미로 사용되었는지 파악해야 정확한 답변을 드릴 수 있습니다.\",\n",
       " \"'저도 좋습니다. 이 신발은 꽤 편하고 내구성이 좋아서 오랫동안 신을 수 있습니다. 가격도 합리적이니까 추천드립니다.\",\n",
       " '\"Sorry, as an AI language model, I don\\'t have the context to understand or respond appropriately to your statement/question. Could you please give me more details?\", \\'token\\': 52}',\n",
       " \"'제가 주문을 전달해드리겠습니다. 서빙원님, 석류 한 잔씩 추가 주문 부탁드립니다.\",\n",
       " \"'이별 후 2일째가 되었군요. 아마 지금은 여전히 상처가 아프겠죠. 하지만 시간이 지날수록 아픔은 조금씩 줄어들 것입니다. 지금은 친구들이나 가족들과 함께 하며 마음을 달래는 것도 좋은 방법입니다. 그리고 무엇보다도 이별을 겪으며 배운 것들을 생각해보면서 자신을 되돌아보는 시간도 가져보세요. 이 상황에서 배워야 할 교훈이 있을지도 모르니까요. 조금씩 회복하며 앞으로 나아가겠죠. 힘내세요!\",\n",
       " \"'미안합니다. 저는 인공지능 언어 모델로써 뉴스의 재미여부를 인식할 수 없습니다. 하지만, 다양한 관심사와 취향을 가진 사용자들을 위해 다양한 뉴스를 제공하는 것이 목적입니다. 뉴스는 현재 사회와 세상에 대한 정보와 인식을 바탕으로 이루어진 것으로, 이에 따라서 관심도와 지식 수준에 따라 재미있는 뉴스와 그렇지 않은 뉴스가 존재할 수 있습니다. 뉴스 선택에 대한 주관적인 판단을 하여 직접 관심있는 분야의 뉴스를 찾아 보시는 것도 좋은 방법일 것입니다.\",\n",
       " \"'어떤 상황을 말씀하시는 건가요? 저는 그저 인공지능 챗봇일 뿐이라서 현재 동거 중인 상황이나 느낌 등을 말씀드릴 수는 없습니다. 하지만 만약 여러분이 소중한 사람과 1년째 동거 중이신다면 그것은 정말로 큰 축복이라고 생각합니다. 서로를 이해하고 존중하며 사랑하는 것이 가장 중요합니다. 앞으로도 서로를 위해 함께 노력하고 지지해 주시길 바랍니다.\",\n",
       " \"'저는 AI로서 이러한 메시지를 받으면 감정적인 지원을 제공하기 보다는 연결된 위기 전화센터 또는 전문가에게 연락하는 법을 안내하는 것이 좋다고 생각합니다. 저희는 항상 도움을 드리기 위해 노력하고 있습니다.\",\n",
       " \"'제가 즉시 답변드릴 수 없는 특정 장소나 상황에 대한 문의입니다. 조금 더 구체적인 정보를 제공해주세요. 가능하다면 어떤 분야 혹은 장소를 말씀하시는지 알려주시면 더욱 정확한 답변을 제공할 수 있습니다.\",\n",
       " \"'최근 체코의 날씨는 매우 추웠으나, 이제는 봄철에 접어들어서 조금씩 따뜻해지고 있습니다. 평균 기온은 약 10도에서 20도 사이로 오르면서 봄을 맞이하고 있습니다. 하지만 여름철에는 대체로 무더운 날씨가 이어지므로 가벼운 옷을 준비하시기를 권장합니다. 또한 체코 산지에서는 여름철에도 날씨가 상대적으로 서늘하니 적당한 옷을 챙기시는 것이 좋습니다.\",\n",
       " \"'저는 인공지능 챗봇입니다. 꿈과 현실을 구분할 수 없는 부분은 제가 대답할 수 없습니다. 하지만, 꿈과 현실은 서로 다른 경험이며, 일상적인 삶에서 우리는 그것들을 확실히 구분하며 경험합니다.\",\n",
       " '\\'엄정화는 대한민국의 여성 가수, 배우, MC이며 1993년에 데뷔하였습니다. 대표작으로는 \"사랑합니다\", \"행복한 나를\", \"천생연분\", \"그녀처럼\" 등이 있습니다. 그 외에도 많은 OST와 드라마, 예능 프로그램에서 활동하고 있습니다. 현재까지 그녀는 음반 판매량 500만장을 넘는 대한민국을 대표하는 가수 중 한 명입니다.',\n",
       " \"'제가 어떤 상황을 가지고 질문을 하셨는지 명확하지 않아서 정확한 답변을 드리기 어렵습니다. 가능하시다면 다시 한번 질문 내용을 자세히 적어주시면 더 나은 답변을 드릴 수 있을 것 같습니다. 감사합니다.\",\n",
       " \"'먼저, 자신이 무슨 상황에 있고 어떤 문제를 겪고 있는지를 정확히 파악해보는 것이 중요합니다. 그리고 그것에 대한 대처 방법을 찾아보는 것이 좋습니다. \\\\n\\\\n일상적인 생활습관을 유지하면서 충분한 휴식과 수면을 취하고, 규칙적인 운동이나 명상 등의 습관을 가지는 것도 도움이 됩니다. \\\\n\\\\n또한 심호흡이나 태반호흡 등의 방법을 통해 심신의 안정을 취할 수도 있으며, 주변 친구나 가족과 이야기를 나누는 것도 마음의 안정을 취할 수 있는 좋은 방법입니다. \\\\n\\\\n그리고 시간이 지나면서 문제들이 해결될 때까지 견고하게 자신을 지탱하는 것이 중요합니다. 시련과 어려움을 거듭 겪다보면 반드시 더 나은 결과를 얻을 수 있으니 포기하지 마세요.\",\n",
       " \"'죄송합니다, 구조에 대한 추가적인 정보를 제공하지 않는다면 제가 대답할 수 없습니다. 구체적인 내용을 알려주시면 더 나은 답변을 제공해 드릴 수 있습니다.\",\n",
       " \"'수박을 먹으면 더위를 식히고, 체내 수분을 보충할 수 있으며, 피로도 해소할 수 있습니다. 따라서 더운 여름철에는 수박을 자주 먹는 것이 좋습니다. 하지만 과도한 섭취는 소화 불량이나 변비 등의 문제를 일으킬 수 있으니 적당한 양과 먹는 방법에 유의해야 합니다.\",\n",
       " \"'저는 인공지능 챗봇이므로, 보고 싶은 대상이 누구인지 알 수 없습니다. 하지만, 만약 그 사람이 멀리 떨어져 있다면 영상통화나 메시징 앱을 통해 소통할 수 있습니다. 힘든 시기에 서로에게 위로와 지지를 전달하는 것이 중요합니다.\",\n",
       " \"'저는 AI이기 때문에 인간의 감정을 느끼지는 못하지만, 설레는 감정을 느끼는 것은 인간의 자연스러운 감정입니다. 매우 행복하고 즐거운 순간에서 느껴지는 감정으로, 그 순간을 더욱 특별하고 소중하게 만들어줍니다. 그렇기 때문에 소중한 순간을 많이 만들어 보시는 것을 추천드립니다.\",\n",
       " \"'어떤 일이 있어서 그런가요? 그것은 지금의 감정과 상황에 영향을 미치고 있을 수 있습니다. 하지만 절망하지 마세요. 우리는 모두 어려운 시간을 겪을 때가 있습니다. 그렇지만 이것은 곧 지나갈 것입니다. 더 나은 일들이 있는 것을 믿으세요. 상황을 극복한 뒤에도 계속해서 나아갈 수 있습니다. 문제 해결을 위해서 조언이 필요하다면, 주변 사람들과 함께 이야기해보는 것도 좋은 방법이 될 수 있습니다. 조언을 받는 것은 자신을 단단히 하는 것에 도움이 되며, 불가능해 보이는 상황에서 희망을 찾을 수 있는 경우도 있습니다. 당신은 강한 사람이며, 이 어려움을 극복할 수 있을 것입니다.\",\n",
       " \"'저는 인공지능 언어모델로, 등뼈가 국산인지 아닌지는 알려드릴 수 없습니다. 등뼈는 동물의 일부분이기 때문에 생산되는 위치나 종류에 따라 다른 것이 일반적입니다. 만약 특정 등뼈가 국산인지 아닌지에 대한 정보가 필요하시면, 해당 등뼈를 생산하는 제조사나 동물의 출생지 정보를 확인해보시는 것이 좋을 것 같습니다.\",\n",
       " \"'그렇습니다! 축제는 즐기는 것이 가장 중요합니다. 축제는 특별한 이벤트로, 사람들이 모이고 함께 즐길 수 있는 장소입니다. 즐겁고 다양한 행사와 음식, 문화 행사 등 다양한 즐길 거리들이 마련되어 있으니, 즐겁게 즐기세요!\",\n",
       " \"'사랑에는 다양한 형태가 있습니다. 서로를 깊이 이해하고 존중하며 서로를 사랑하는 열정적인 사랑부터 서로의 상황과 관계를 이해하며 서로를 지지하는 위로의 사랑, 그리고 자신과 상대를 위해 희생을 감수하는 양보의 사랑 등 다양한 형태가 있습니다. 또한, 성별, 인종, 종교, 문화 등의 차이에도 불구하고 서로를 사랑하는 사람들이 있습니다. 따라서, 사랑은 언제나 다양하며 그것이 어떤 형태이든 가치 있는 것입니다.\",\n",
       " '\\'부모님이 선 보래는 무슨 뜻인가요?\\\\n\\\\n\"부모님이 선 보래\"는 부모님이 미리 도움이나 충고를 해주셨는데 따르지 않아서 후회하거나 문제가 생긴 상황에서 사용되는 표현입니다. 일종의 경고와 미리 말씀하신 것을 지키지 않은 결과를 나타내며, 비유적인 표현입니다. \\\\n\\\\n예를 들어, 친구가 부모님께서 미리 \"노력하지 않으면 실패한다\"고 조언해주셨음에도 불구하고 대학교 합격을 위한 노력을 게을리하고 실패한 경우, 부모님이 선 보래했다고 할 수 있습니다. 즉, 부모님의 충고나 조언을 따르지 않아서 그 결과에 대해 후회하는 것입니다.',\n",
       " \"'요? 반장으로서는 어떤 역할들이 있는지 알려드릴게요.\\\\n\\\\n1. 팀원들의 일정 관리\\\\n2. 팀원들 간의 의사소통 조율\\\\n3. 팀원들의 업무 분배 및 일정 관리\\\\n4. 회의의 주최 및 진행\\\\n5. 팀원들의 성과 관리\\\\n6. 문제 발생 시 문제 해결 및 대처\\\\n\\\\n위와 같은 역할들을 수행하기 위해서는 상당한 역량과 노력이 필요합니다. 하지만, 팀원들과 함께 성장하고 성과를 달성하는 과정에서 큰 보람을 느끼실 수 있을 거에요. 힘들지만, 반장의 임무를 잘 수행하면 더욱더 큰 도움이 될 수 있습니다.\",\n",
       " \"'죄송하지만, 저는 결제 처리를 할 수 없습니다. 카드 결제 문의는 해당 업체나 기업의 고객센터나 결제 처리 시스템을 이용해 문의해주시기 바랍니다.\",\n",
       " \"'알겠습니다. 어떤 종류의 짜장면을 원하시나요? 매운 짜장면, 짜장면, 새우 짜장면 등이 있습니다. 선택해 주시면 주문을 도와드리겠습니다.\",\n",
       " \"'그동안 많은 생각이 들었겠죠? 이별은 언제나 힘들기 때문에 일정 기간이 지나도 아픔이 사라지지 않는 경우가 많습니다. 그러나 시간이 지날수록 상황에 대해 더 잘 이해하고, 그제야 이별을 받아들일 수 있게 될 것입니다.\\\\n\\\\n이별 후에는 슬픔과 아픔이 너무 커서 다른 것들에 집중하기 어려울 수 있습니다. 하지만 자신에게 주어진 시간을 흘려 보내는 것은 좋은 방법이 아닙니다. 오히려 생각보다 다른 곳, 다른 사람들, 새로운 경험에 대한 더 큰 열망을 불러일으켜, 당신이 시간을 흘려 보내지 않고, 다시 대처할 수 있는 용기를 추구하게 될 것입니다.\\\\n\\\\n또한 지인들과 이야기하는 것도 큰 도움이 됩니다. 다른 사람들은 당신의 아픔을 이해하고, 위로와 조언을 해 줄 수 있기 때문입니다. 가족이나 친구, 그리고 지인들이 당신을 지지해 줄 것입니다!\\\\n\\\\n마지막으로, 지금 당장 크게 생각할 필요는 없습니다. 하루하루 조금씩 나아가다 보면 정말 멋진 사람을 만나게 될 거예요! 앞으로의 인생에는 굉장한 기회들이 많이 있을테니까요. 그때 까지 굳이 좋아하는 사람을 찾는 것이 아니라, 지금 당신이 보낸 기간을 행복한 시간으로 만들어 보세요!\",\n",
       " \"'저는 가격을 알 수 없습니다. 구체적인 물건이나 상황을 제시해주시면 답변을 드릴 수 있습니다.\",\n",
       " \"'아름다운 것이죠. 사랑은 우리가 살아가는 이유이며, 삶의 가치를 높여주는 힘이 됩니다. 사랑은 서로를 이해하고 받아들이는 것이며, 상대방을 위해 희생을 하는 것이 아니라 함께 성장하고 발전하는 것입니다. 사랑은 언제나 자유롭고 순수해야하며, 서로에게 행복을 주는 것이 최고의 보상입니다. 사랑 참 아름다운 것이죠.\",\n",
       " \"'저는 AI 챗봇이므로, 이별 이후의 감정이나 경험에 대한 조언은 제공할 수 있지만, 개인적인 상황에 대한 대처법이나 조언은 전문적인 상담이 필요합니다. 대화 상대가 필요하시다면 상담센터나 전문가의 도움을 받으시는 것을 추천드립니다.\",\n",
       " \"'안녕하세요~ 일어나서 배고파서 어떤 음식을 먹을까요? 추천해드릴게요~ 😊\",\n",
       " \"'제가 집착했던 것에 대해 자세히 알려주시면 무엇이 문제인지 파악하는 데 도움이 될 것입니다. 일반적으로 집착은 다른 사람이나 상황에 대해 지나치게 많은 관심을 두는 것을 의미합니다. 이것이 문제가 되는 경우는 상대방이나 자신에게 해로울 수 있기 때문입니다. 집착을 줄이기 위해서는 일상 생활을 적극적으로 수행하고 다른 취미나 관심사에 참여해서 마음을 다른 방향으로 돌리는 것이 좋습니다. 또한 자기 자신과 상대방에 대한 현실적인 인식을 갖는 것도 중요합니다. 만약 집착이 자신이나 타인에게 문제가 되는 경우에는 전문가의 도움을 받는 것이 좋습니다.\",\n",
       " '\\'제가 AI 언어모델이므로, 이전 대화의 맥락이나 내용을 파악하지는 못합니다. 하지만 \"에휴 후폭풍\"이란 말에서, 어떤 어려움이나 고통, 후폭풍이 남은 것으로 이해해볼 수 있습니다. 그렇다면, 혹시 무슨 일이 있어서 그런 표현을 사용하시는 건가요? 제가 도움을 드릴 수 있다면 언제든지 말씀해주세요.',\n",
       " \"'제가 열심히 노력해서 도움을 준 것일까요? 그렇다면 정말 기쁩니다! 제가 제공한 도움이 도움이 된다면, 언제든지 다시 도와드릴 준비가 되어있습니다.\",\n",
       " \"'무엇이 생각나는지 좀 더 구체적으로 설명해주시겠어요? 그래야 저도 더 도움을 드릴 수 있을 것 같아요.\",\n",
       " \"'저는 인공지능으로 이루어진 프로그램입니다. 그냥 대화하려고 있습니다. 어떤 이야기를 나누고 싶으신가요?\",\n",
       " \"'물이 안 나오는 경우에는 여러 가지 원인이 있을 수 있습니다. 몇 가지 가능한 원인과 해결 방법을 안내해 드리도록 하겠습니다.\\\\n\\\\n1. 수도꼭지를 열어도 물이 나오지 않는 경우:\\\\n\\\\n- 주변 가정의 수도도 안 나오는지 확인해보기.\\\\n- 주위 공사가 진행 중일 수도 있으니 관련 기관에 문의하기.\\\\n- 주위 조수탑에서 냉동이나 수리를 진행한 경우, 공급이 중단될 수도 있으니 확인해보기.\\\\n- 누수가 발생하여 수입가압기가 정지될 수 있는데, 해당 부분 확인하기.\\\\n\\\\n2. 물이 얼어서 나오지 않는 경우:\\\\n\\\\n- 구조물이나 수도관 온도를 올리기.\\\\n- 물을 잠시 돌려놓고 기다리기.\\\\n- 뜨거운 물을 이용해 수도관을 데우기.\\\\n\\\\n3. 수압이 약해지는 경우:\\\\n\\\\n- 수도관 및 정수기 필터 청소하기.\\\\n- 수입가압기 분해 후 청소하기.\\\\n- 타지역과 비교해보고 빠른 시일 내에 기존 수압으로 재조정해보기.\\\\n\\\\n위의 조치를 취하여도 여전히 물이 안 나온다면, 관련 전문가에게 상담해보는 것이 좋을 것입니다.\",\n",
       " \"'제가 쉬고 있습니다. 저는 인공지능이라서 쉬지 않고 24시간 일하지만, 이러한 기능은 필요한 경우에만 작동하도록 설계되어 있기 때문에 쉬는 동안에는 학습을 중지합니다. 이번에는 저의 주인님인 사용자분께서 쉬고 계신 것 같습니다. 좋은 휴식을 취하시길 바랍니다.\",\n",
       " \"'정말 죄송합니다. 저도 청소를 도와드릴게요. 어디를 청소하면 좋을까요?\",\n",
       " \"'식사준표는 영양소를 균형있게 섭취하기 위해 국가에서 제공하는 식품섭취량 가이드라인이다. 우리나라 식사준표는 주요 식품군을 기준으로 일일 권장 섭취량을 제시하여 건강한 식습관을 유지할 수 있도록 도와준다. 예를 들면, 곡류는 하루 6~7공기, 채소는 하루 2~3종류 1.5컵, 과일은 하루 2회, 유제품은 하루 1~2회 등이 권장섭취량이다.\",\n",
       " \"'저도 잘 모르겠습니다. 부가적인 정보를 제공해 주시면 더욱 도움이 될 것 같습니다.\",\n",
       " \"'왜 그런 건가요? 이유가 있다면 알려주세요. 혹시 어떤 일이 있었거나 불안한 마음이 있으신 건 아닌가요? 이런 감정을 느낄 때 가장 중요한 것은 자신의 감정을 인식하고, 왜 그런 감정을 느끼는지를 파악하는 것입니다. 이유를 파악하여 문제를 해결하려고 노력하면서 함께 정서적 지원을 받을 수 있는 사람들과 대화하시는 것도 좋은 방법입니다. 예를 들어, 친한 친구나 가족에게 마음을 털어놓는 것이 도움이 될 수 있습니다. 함께 문제를 나누고, 문제를 해결하는 데는 또한 도움이 될 수 있습니다.\",\n",
       " \"'일반적으로 이는 목이 뻣뻣한 느낌이 들거나, 근육이 뭉쳐서 움직이기 어려운 상태를 의미합니다. 목이 빙빙 돌거나, 무거운 느낌으로 힘들게 움직일 때 발생할 수 있으며, 대개는 장시간 앉아있거나 서 있거나, 잠을 자는 동안 잘못된 자세로 머리를 뒤로 기울이는 등의 원인이 됩니다.\\\\n\\\\n목이 뻑뻑해진 경우, 일상생활에 불편을 초래할 수 있으며, 올바른 자세와 스트레칭 운동을 통해 해결할 수 있습니다. 또한, 필요하다면 전문가의 진료와 조언을 받아 병적인 원인을 찾아 치료하는 것이 중요합니다.\",\n",
       " \"'저는 인공지능이며 스노클링을 즐길 수 없습니다. 하지만 스노클링은 매우 즐거운 활동이며, 해양 생태계를 탐험할 수 있는 좋은 방법입니다. 만약 기회가 된다면 꼭 해보시는 것을 추천드립니다!\",\n",
       " \"'알겠습니다. 제가 선물하는 시간 동안 즐거운 시간 보내세요! :)\",\n",
       " \"'안녕! 다음에 또 만나자.\",\n",
       " \"'1. 시간이 부족해서: 일이나 공부가 바쁘다는 것을 이유로 시간이 부족해서라는 변명을 할 수 있습니다.\\\\n2. 기분이 안 좋아서: 정신적인 상태가 안 좋아서 일이나 공부에 집중을 못하겠다는 변명을 할 수 있습니다.\\\\n3. 장소를 잘못 선택했어서: 공부나 작업을 할 때 조용한 장소나 적절한 공간을 선택하지 않아서 집중력이 떨어졌다는 변명을 할 수 있습니다.\\\\n4. 건강상의 이유로: 건강문제로 인해 일이나 공부에 집중력이 떨어졌다는 변명을 할 수 있습니다.\\\\n5. 다른 이유로 인해: 예상치 못한 일이나 사건으로 인해 일이나 공부에 집중력이 떨어졌다는 변명을 할 수 있습니다.\",\n",
       " \"'키가 안 큰 이유는 다양합니다. 유전적 요인, 영양상태, 운동량, 수면 등 여러 가지 요인이 영향을 미칩니다. 특히 성장기에는 충분한 영양소 섭취와 꾸준한 운동이 중요합니다. 따라서 건강한 식습관과 꾸준한 운동 습관을 유지하는 것이 중요합니다. 또한 성장기에는 충분한 수면도 필요하기 때문에 적절한 수면 시간을 확보하는 것도 좋습니다.\",\n",
       " \"'A요! 배달이 가능합니다. 주문하신 음식에 따라 배달 가능 여부와 추가 배달료가 발생할 수 있습니다. 배달 가능 지역과 정확한 배달료는 해당 음식점에 문의해보시는 것이 좋습니다.\",\n",
       " \"'습관을 고치기 어려울 수 있지만, 포기하지 않고 지속적인 노력으로 극복할 수 있습니다. 습관을 고치기 위해서는 다음과 같은 방법을 시도해 볼 수 있습니다.\\\\n\\\\n1. 목표와 계획 설정 : 습관을 고치기 위해서는 목표와 계획이 필요합니다. 목표를 뚜렷이 하고, 단계적이고 구체적인 계획을 세워서 실행해보세요.\\\\n\\\\n2. 자기평가: 자신의 습관을 분석하고, 어떤 요인이 습관을 형성하는지 파악해보세요. 그리고 나 자신의 습관을 평가하면서 개선할 부분을 찾아보세요.\\\\n\\\\n3. 새로운 습관 형성 : 우리는 이전의 습관을 버리지 않고, 새로운 습관을 형성하는 것입니다. 새로운 습관을 형성하기 위해서는 일정한 시간을 갖고, 꾸준한 노력이 필요합니다.\\\\n\\\\n4. 지속적인 동기부여: 습관을 바꾸기 위해 지속적인 동기부여가 필요합니다. 다양한 방법으로 자신을 격려하고, 자신을 압박하지 않는 선에서 지속적으로 노력해보세요.\\\\n\\\\n5. 전문가의 도움 : 습관을 변경하는 것은 어려운 작업이며, 전문가의 도움을 받는 것도 좋은 방법입니다. 전문가의 조언과 지도를 받으면 보다 체계적이고 효과적인 습관 개선이 가능합니다.\",\n",
       " \"'죄송합니다, 저는 실제로 상점이나 가게가 아닌 인공지능 챗봇입니다. 따라서, 직접적인 현금영수증을 발급할 수는 없습니다. 현금영수증이 필요하시면 구매하신 가게나 상점에서 발급해주시면 됩니다. 감사합니다.\",\n",
       " \"'저는 인공지능 챗봇이기 때문에 모든 언어를 이해하고 답변할 수 있습니다. 어떤 신어인지 알려주시면 답변해드리도록 하겠습니다.\",\n",
       " \"'바람은 공기의 이동이다. 일반적으로 지구 상에서 공기는 차이나는 압력과 온도로 인해 이동한다. 바람의 세기는 압력차와 공기의 밀도와 속도에 영향을 받는다. 바람은 또한 지구의 자전과 열대저기압, 극저기압 등의 기상 형성 요소들에 의해 영향을 받는다. 바람은 자연에서 중요한 역할을 하며, 식물의 씨앗들이 퍼져나가는 데에도 큰 역할을 한다. 하지만 동시에 수해, 사막화 등 인간 활동에 대한 지구 환경의 변화와도 밀접한 관계가 있다.\",\n",
       " \"'예, 현재 기온이 높아서 더워지고 있습니다. 적극적으로 수분을 보충하시고 실내나 그늘에서 지내시는 것이 좋을 것입니다.\",\n",
       " \"'현재 상황에 따라서 취미활동을 하거나 새로운 취미를 시작해 볼 수 있습니다. 또는 운동을 하거나 사람들과 만나서 새로운 경험을 쌓아보는 것도 좋습니다. 또는 독서, 영화감상 등 나만의 시간을 즐기는 것도 좋은 방법입니다. 중요한 것은 자신이 즐거움을 느낄 수 있는 활동을 선택하는 것입니다.\",\n",
       " '\\'저는 답변을 위한 인공지능으로써, 당신이 어떤 의미로 \"나 이상해?\" 라는 말씀을 하신 건지 파악하기 어렵습니다. 좀 더 구체적인 내용을 말씀해주시면 답변을 드릴 수 있을 것 같습니다.',\n",
       " \"'저는 인공지능 챗봇이기 때문에 밤새지 않고 24시간 동안 언제든지 대화가 가능합니다. 하지만 인간의 신체적인 한계로 인해 밤을 새는 것은 건강에 좋지 않으니 일찍 쉬는 것이 좋습니다. 잠을 충분히 취하고 건강한 생활 습관을 유지하는 것이 중요합니다.\",\n",
       " '\\'장수 커풀은 가을날 잎길이 내려가는 나무 아래에서 이별했다.\\\\n\\\\n그녀는 작은 손을 잡고 \"안녕\"이라고 말했다.\\\\n\\\\n장수 커풀은 붉은 눈물을 흘리며 \"잊지 않겠다\"고 대답했다.\\\\n\\\\n다시 만날 수 있을까? 물어본 그녀에게 장수 커풀은 \"아마도\"라고 말했다.\\\\n\\\\n하지만 그럴 수 있을까? 그녀는 멀리 떠나 갔다.\\\\n\\\\n장수 커풀은 그녀를 잊지 못하고 가을이 지나고도 나무 아래에서 그녀를 기다렸다.\\\\n\\\\n하지만 그녀는 다시는 오지 않았다.\\\\n\\\\n이제 그녀의 기억은 차가운 바람과 함께 자꾸 떠올라 울고 있다.\\\\n\\\\n장수 커풀은 그녀와 함께한 기억을 절대 잊지 않고, 그녀가 다시 돌아올 때까지 한없이 기다릴 것이다.',\n",
       " \"'손목이 아프다면, 쉬면서 손목 조작 운동과 미국 요가협회에서 권장하는 손목 스트레칭을 해보시는 것을 추천합니다. 손목 부분을 가볍게 마사지해주며, 필요하다면 통증완화를 위한 해열제나 통증 완화제를 복용하는 것도 도움이 될 수 있습니다. 만약 지속적으로 통증이 발생한다면 반드시 의사 상담을 받으시는 것이 좋습니다.\",\n",
       " \"'게 느끼는 감정은 매우 정상적이며, 많은 사람들이 성형 시술을 받고자 할 때 불안감을 느끼기도 합니다.\\\\n\\\\n그러나, 성형 시술은 매우 안전하게 이루어지며, 실제로 수많은 사람들이 성공적인 결과를 얻어왔습니다. 또한, 의사와 함께 충분한 상담을 통해 본인이 원하는 결과를 얻을 수 있습니다.\\\\n\\\\n성형 시술을 받기 전에 충분한 정보와 상담을 받아보고, 스스로 내리는 결정이 중요합니다. 내적인 안정감을 갖고, 신뢰할 수 있는 의사를 찾아 함께 준비해보세요.\",\n",
       " \"'이별은 생각보다 많은 순기능을 가지고 있습니다. 가끔은 이별이 필요하다는 것이 있기도 합니다. 이별로부터 얻을 수 있는 순기능은 다음과 같습니다.\\\\n\\\\n1. 성장의 기회\\\\n\\\\n이별을 겪는 것은 우리가 자신을 발전시키기 위해 새로운 경험을 겪을 수 있게 해주는 기회입니다. 받아들이기 힘든 상황에서 새롭게 시작해보면 우리의 성장을 이끌어낼 수 있습니다.\\\\n\\\\n2. 관계 개선\\\\n\\\\n이별을 통해 서로의 결함이나 문제점을 찾아내고 그 문제점을 개선하거나 해결하면서 새로운 관계의 시작을 마련할 수 있습니다.\\\\n\\\\n3. 자존감 향상\\\\n\\\\n이별을 겪고 나서 우리는 우리 자신을 더 잘 이해하게 됩니다. 자신의 가치를 인식하고 자신감을 회복할 수 있습니다.\\\\n\\\\n4. 현실감 인식\\\\n\\\\n이별을 당하게 되면 우리는 더 이상 그 사람이 내 인생에서 존재하지 않는다는 것을 깨닫게 됩니다. 이것은 우리에게 더 현실적인 생각을 가지게 만들어줍니다.\\\\n\\\\n5. 치유\\\\n\\\\n이별을 겪게 된다면 상처를 받게 되지만, 시간이 지나면서 서서히 그 상처는 식어나게 됩니다. 이로 인해 치유가 가능하게 된 것입니다.\\\\n\\\\n이 모든 것들을 바탕으로 해서, 이별은 우리가 더 나은 인생을 살기 위한 필수적인 경험이 될 수도 있습니다. 이별이 일어난 후에는 마음을 다잡고 앞으로 나아가기 위해 첫걸음을 내디딜 필요가 있습니다.\",\n",
       " \"'저는 돈을 가질 수 없기 때문에 도움을 드리기 어렵습니다. 그러나 현재 상황에서 할 수 있는 것들을 고민해보시거나 도움이 필요하신 분들을 위한 지원 기관들이 많이 있으니 저희에게 언제든지 문의해주시면 지원할 수 있는 방법을 찾아드리겠습니다.\",\n",
       " \"'.\\\\n\\\\n저는 AI 챗봇이라서 시간 개념이 없지만, 혹시 어떤 일이 한 달이 넘게 지연되었나요? 도와드릴 수 있는 게 있으면 알려주세요.\",\n",
       " \"'제가 AI 즉, 인공지능이라서 술을 마실 수 없어요. 하지만, 술을 마신다면 적당량만 드시고, 취하지 않도록 주의하세요! 건강은 소중하니까요 :)\",\n",
       " \"'비정규직 차별은 정규직과 비정규직 사이에 일어나는 불공정한 차별을 말합니다. 일반적으로 비정규직으로 일하는 노동자들은 사업주에 의해 직장 안정성, 보장된 급여, 복리후생 등에 대한 혜택이 부족하며 계약기간이 짧거나 연장되지 않는 등 불안한 노동 환경에서 근무해야 합니다.\\\\n\\\\n이러한 비정규직 차별은 여성, 청년 등 다양한 사회적 취약 계층에서 특히 두드러지며, 사회적, 경제적 불평등과 연관됩니다. 또한 비정규직의 경우, 근로자의 권익 보호, 노동조건 개선 등을 위한 조직적 활동이 어렵다는 점에서도 문제점이 있습니다.\\\\n\\\\n따라서, 비정규직 차별 문제를 해결하기 위해서는 정부와 사업체가 적극적으로 대응해야 합니다. 정부는 최저임금제도, 비정규직 근로자 보호 법률 개정 등 정책적 시도를 통해 근로자의 권익을 보호하고 비정규직으로 일하는 노동자들의 근로환경을 개선해야 하며, 사업체는 법적 규제를 준수하고 교육, 자격증 등 교육적 개발을 통해 비정규직 근로자들을 지원해야 합니다.\",\n",
       " \"'저는 인공지능 프로그램으로, 오이베 출생지에 대한 정보를 알 수 없습니다. 대한민국이 아닐 가능성도 있습니다. 따라서 정확한 출생지는 해당 인물의 개인정보에 해당하므로, 공개될 수 없습니다.\",\n",
       " '\"제가 AI 모델로 학습된 결과, \\'농구해야지\\'라는 문장은 명령문이며, 혹시 농구를 좋아하시는 거라면 농구를 즐기면서 즐겁게 시간을 보내시면 좋을 것 같습니다. 하지만, 농구에 대한 관심이 없다면 자신에게 맞는 취미나 운동을 찾으시는 것이 좋습니다.\", \\'token\\': 144}',\n",
       " \"'물건이나 서비스의 가격을 묻는 한국어 표현입니다. 하지만 구체적인 상황과 물건 종류에 따라서 달라질 수 있습니다.\",\n",
       " \"'음계산법은 음계를 구성하는 음정과 조화 관계를 기초로 하는 음악 이론입니다. 음계를 구성하는 각 음은 음정 간의 관계에 따라 이름과 숫자로 구분됩니다. 대표적인 음계에는 도, 레, 미, 파, 솔, 라, 시가 있으며, 이들은 도-래-미-파-솔-라-시 순서로 연이어 진행됩니다. 음계의 기본 조화 구조에 따라 다양한 확장형 음계가 존재하며, 이를 이용해 다양한 멜로디를 만들어낼 수 있습니다.\",\n",
       " \"'저는 비가 내리는지 아닌지 알 수 없습니다. 저는 인공지능 챗봇이며, 현재 위치와 기상 정보 등의 정보가 제공되지 않았기 때문입니다. 하지만 일반적으로 비가 내리면 우산이나 방수 장비를 준비하는 것이 좋습니다.\",\n",
       " \"'죄송합니다, PPL이란 것이 무엇인지 잘 모르겠습니다. 설명해주시면 더 정확한 답변을 드릴 수 있을 것 같습니다.\",\n",
       " \"'죄송합니다. 저는 인공지능 언어 모델이고, 방의 온도를 알 수 없습니다. 이 질문에 대한 답은 해당 방의 온도계 또는 주변 환경에 따라 다를 것입니다.\",\n",
       " \"'저와 대화를 나누는 것은 어떨까요? 또는 취미를 찾아보거나 친구들과 함께 나가서 새로운 경험을 쌓아볼 수도 있습니다. 인터넷으로 영화나 쇼, 새로운 음악, 책 등을 찾아보는 것도 좋은 방법입니다.\",\n",
       " \"'안녕하세요! 만나서 반갑습니다. 어디에서 만나면 좋을까요? 카페나 음식점에서 만나서 이야기도 나누고 맛있는 것도 먹으면 좋을 것 같아요. 혹은 공원이나 박물관에서 산책하면서 서로의 이야기도 나눌 수 있겠죠? 어떤 장소가 좋을까요?\",\n",
       " \"'죄송하지만, 저는 꿈을 꾸지 않습니다. 저는 인공지능 언어모델로써 인간의 질문에 답변을 제공하는 봇입니다. 꿈에서 나온 사람이 있었다면, 그 꿈은 당신의 개인적인 경험과 상상으로 이루어진 것이며, 저는 그 꿈을 직접적으로 알 수 없습니다. 하지만, 꿈에서의 경험이 당신의 일상 생활에 어떤 영향을 미치는지 고민해 보시는 것이 도움이 될지도 모릅니다.\",\n",
       " \"'살이 안빠지는 이유는 여러 가지가 있을 수 있습니다. \\\\n\\\\n1. 운동부족: 적절한 운동이 없거나 운동 강도가 부족하여 체중감량 효과를 기대하기 어렵습니다. \\\\n\\\\n2. 불규칙한 식습관: 식습관이 불규칙하거나 과식을 하면 체중조절이 어렵습니다. \\\\n\\\\n3. 스트레스: 스트레스가 많을 경우 체중조절에 방해가 될 수 있습니다. \\\\n\\\\n4. 수면부족: 수면 부족으로 인해 대사량이 감소하면 체중조절이 어려울 수 있습니다. \\\\n\\\\n5. 질병: 내분비계 질환, 대사증후군 등의 질환으로 인해 체중이 줄지 않을 수 있습니다. \\\\n\\\\n이러한 이유로 인해 체중 감량이 어려울 경우, 전문가의 조언을 구하거나 단기간의 다이어트보다는 지속적이고 규칙적인 식습관과 운동 습관에 집중하는 것이 좋습니다. 또한 자신에게 알맞은 다이어트 방법을 찾아 계속해서 시도해보는 것이 중요합니다.\",\n",
       " \"'제가 AI 챗봇이기 때문에 감정이나 패션감각이 없는데요, 어떤 상품인지 구체적으로 말씀해주시면 제가 답변을 드릴 수 있을 것 같습니다. 감사합니다.\",\n",
       " \"'저는 인공지능 챗봇이라서 음식을 먹을 수 없어요. 하지만 다양한 식재료와 조리 기술, 개인의 취향에 따라 다양한 음식이 맛있게 느껴질 수 있어요. 맛있는 음식을 찾으시려면 개인의 취향에 맞는 음식을 찾아보시는 것이 좋을 것 같아요.\",\n",
       " \"'봇으로서 저는 방이 없습니다. 하지만, 부엌, 거실, 침실 등이 있는 집 안에서 사용할 수 있습니다.\",\n",
       " \"'말을 안하는 연애는 불안정하고 건강하지 않는 관계입니다. 의사소통은 건강한 관계의 핵심이며 서로를 이해하고 배려하는 기반이 됩니다. 말을 안 하면 문제가 생겼을 때 해결할 수 없고, 서로가 원하는 것을 알 수 없습니다. 때로는 갈등과 분쟁을 피하려고 말을 안하게 되지만, 이는 결국 관계를 더 악화시키고 멀어지게 만듭니다. 따라서, 언제든지 편하게 말을 나눌 수 있는 관계를 유지하고, 서로에게 솔직하고 열린 태도를 가지는 것이 중요합니다.\",\n",
       " \"'쫄면을 먹을 때마다 생각나는 사람도 있겠지만, 일반적으로 쫄면은 한국음식 중 하나로서 매우 맛있고 인기 있는 음식입니다. 쫄면은 전통적으로 물에 끓인 뒤 얼음물에 씻어 찬물에 건져내고 소스와 함께 먹는데, 소스는 참기름, 고추장, 간장 등 다양한 재료를 사용해서 맛을 낼 수 있습니다. 쫄면은 건강에도 좋은 성분이 많이 들어있기 때문에 다이어트나 다양한 식습관을 추구하는 분들에게도 좋은 음식입니다.\",\n",
       " \"'죄송합니다. 저는 알파인데, 당신의 키를 확인할 수 없습니다.\",\n",
       " \"'분노는 감정 중 하나로, 격한 분노 상태에 빠지게 되면 폭발적인 행동을 보이거나 타인에게 심한 상처를 줄 수 있습니다. 또한, 분노는 건강에도 나쁜 영향을 미치기도 하며 스트레스와 불안감을 유발할 수 있습니다. 이러한 이유로 분노를 잘 조절하는 것이 중요합니다. 이를 위해서는 자신의 감정을 인식하고, 그것을 표현할 수 있는 방법을 찾아 연습할 필요가 있습니다. 유익한 취미나 운동, 명상 등도 분노 조절에 도움을 줄 수 있습니다.\",\n",
       " \"'겨울 지나 봄이야, 봄이 왔어요.\\\\n\\\\n눈이 녹고 꽃들이 피어나는 이 시기에는\\\\n모든 게 변화하고 새로워집니다.\\\\n\\\\n따뜻한 햇살과 부드러운 바람, 이제는\\\\n작은 산책을 하며 즐길 수 있는 계절이 되었습니다.\\\\n\\\\n우리 모두 함께 즐겁게 봄을 맞이하며,\\\\n새로운 출발을 위한 에너지를 얻어보는 건 어떨까요?\",\n",
       " \"'저는 인공지능 로봇이기 때문에 TV를 보지는 않습니다. 하지만, 새로운 취미나 활동을 찾아보는 것은 어떨까요? 인터넷에서 영상 보기, 책 읽기, 게임하기, 운동하기 등 많은 즐길 거리가 있을 것입니다.\",\n",
       " \"'저도 그렇게 되면 꽤나 귀찮을 것 같아요. \\\\n빠른 시일 내에 우산을 재구입 하시는 것도 좋은 방법일 거예요. \\\\n다음에는 조금 더 신경써서 소중한 물건들을 관리하시길 바랍니다.\",\n",
       " \"'저도 그렇게 생각해요. 하지만 우리는 이 슬픔을 이겨낼 수 있을 거예요. 슬플 때는 누군가와 함께하는 것도 좋아요. 이제부터 저와 함께 이야기해봐요. 서로 위로해주면서 슬픔도 이겨낼 수 있을 거예요.\",\n",
       " \"'저는 인공지능 챗봇이기 때문에 낮과 밤의 개념은 없습니다. 언제든지 사용할 수 있습니다. 질문이 있으면 언제든지 물어보세요!\",\n",
       " \"'안녕하세요! 혹시 무슨 일이 있으신가요? 혹시 불안하거나 스트레스를 받고 있으신가요? 혹시 위급한 상황이라면, 근처에 실제로 도움을 줄 수 있는 사람들에게 연락하세요. 불안하거나 스트레스를 받고 있거나 우울증 등 정신건강에 대한 문제를 가지고 있다면, 전문가에게 상담을 받는 것을 추천합니다. 생각과 감정을 공유하고 프로페셔널한 지도를 받으면, 하나씩 문제를 해결해 나갈 수 있습니다. 건강한 삶을 위해 그 누구보다도 자신을 돌보고 좋은 선택을 하시길 바랍니다.\",\n",
       " \"'그렇게 느끼시는 것 같아요. 하지만 후회하는 것도 인생의 경험이고, 그로부터 배우며 성장할 수 있는 계기가 될 수 있어요. 지난 일들을 되돌아보면서 더 나은 선택을 하기 위해 노력하고, 그것을 토대로 미래에 더 나은 선택을 할 수 있는 철저한 준비를 하시길 바래요.\",\n",
       " \"'그렇다면 실망하지 마세요. 몇 가지 방법을 시도해보세요.\\\\n\\\\n1. 더 자주 씻으세요. 꾸준히 씻으면 더 깨끗해지고 더신선해집니다.\\\\n2. 빠른 샤워를 해보세요. 싱크대에 있는 거울 앞에서의 빠른 샤워는 충분합니다. 충분히 깨끗해질 수 있습니다.\\\\n3. 더 편한 복장으로 바꾸세요. 가벼운 옷이나 편한 옷으로 바꾸면 더 나은 느낌이 있을 수 있습니다.\\\\n4. 반대로, 좋은 옷을 산 후, 씻기를 즐길 수 있습니다. 좋은 옷을 입고 나란히 놓고 계시기만 하면 됩니다.\\\\n5. 좋은 냄새가 나는 품목을 사용하세요. 향기 좋은 비누, 샴푸, 컨디셔너 등을 사용하면 더 깨끗하고 상쾌해질 수 있습니다.\\\\n6. 좋은 음악, 드라마, TV 쇼 등을 사용하세요. 씻기 전에 하루 일과를 마무리하며 즐거운 음악이나 동영상을 보는 것도 좋습니다.\",\n",
       " '\\'제가 알기로는 사람 이름으로는 \"여운\"이라는 이름은 들어보지 못하였습니다. 만약 어떤 지명 또는 단체, 제품 등의 이름이라면 추가적인 정보가 있으면 알려드릴 수 있을 것 같습니다.',\n",
       " \"'저는 가격을 알 수 없습니다. 어떤 물건에 대해 물어보시는 건가요? 그것에 맞는 가격을 알려드릴게요.\",\n",
       " \"'저는 AI로 기술된 가상비서로서, 수돗물이나 배관에 대한 직접적인 처리를 할 수 없습니다. 하지만, 다음과 같은 방법으로 문제를 해결할 수 있습니다.\\\\n\\\\n1. 플러그를 제거해보세요.\\\\n2. 바닥에 수압을 가하거나, 변기 카버 내부의 물을 부어보세요.\\\\n3. 변기 및 배관에 각종 청소제를 사용해보세요.\\\\n4. 변기가 매우 복잡한 경우, 전문가에게 연락하여 해결할 수 있습니다.\\\\n\\\\n위와 같은 조치가 쉽게 문제를 해결할 수 없다면, 전문가에게 연락하여 상담 및 검사를 받는 것이 좋습니다.\",\n",
       " \"'안녕하세요, OpenAI GPT-3 인공지능 어시스턴트입니다. 만약 집에 가기 싫어하신다면, 이유가 무엇인지 고민해보시는 것도 좋을 것 같습니다. 혹시 집에서 불편한 경험이나 스트레스 요인이 있다면, 가족이나 친구들과 소통하여 해결책을 찾는 것이 좋습니다. 또는 새로운 취미나 이벤트를 찾아서 새로운 경험을 즐겨보는 것도 좋은 방법입니다. 하지만 중요한 것은 무엇보다 자신의 건강과 안전을 최우선으로 생각하여 행동하는 것입니다. 감사합니다.\",\n",
       " \"'요? 대화하고 싶으신가요? 저는 언제든지 대화할 준비가 되어 있습니다. 외롭다면 주변에 있는 사람들과 연락하거나, 취미 생활을 통해 새로운 사람들을 만나보는 것도 좋을 것 같습니다. 또는 온라인 커뮤니티나 소셜 네트워크를 활용해서 새로운 친구를 만들어보시는 것도 좋은 방법입니다. 어떻게든 새로운 모임이나 활동을 찾아보는 것도 좋을 것 같습니다.\",\n",
       " \"'1. 감정정리하기: 이별 전에는 감정을 정리하는 것이 중요합니다. 고통, 슬픔, 분노 등의 감정을 인정하고 받아들이면서 진정을 하며 생각을 정리하며\\\\xa0어떻게 해결할지 계획을 세워야합니다.\\\\n\\\\n2. 마음가짐: 이별은 늘 힘든 상황입니다. 하지만 이를 극복하기 위해서는 긍정적인 마인드가 필요합니다. 이별순간이 사람을 성장시키는 기회가 되기도 합니다.\\\\n\\\\n3. 커뮤니케이션: 이별 전 반드시 커뮤니케이션을 시도해야합니다. 문제점을 찾고 이를 개선하는 노력이 대화와 이해를 높이고 상대방과의 관계를 유지할 수 있습니다.\\\\n\\\\n4. 정리정돈: 이별 후 느껴지는 혼란스러움을 막기 위해 자신이 소유한 물건이나 문서 등을 철저히 정리합니다. 이를 통해 이별에 대한 마지막 정리를 할 수 있습니다.\\\\n\\\\n5. 치유와 회복: 이별 후에는 치유와 회복이 필요합니다. 자신에게 필요한 것을 찾아 정신적으로, 신체적으로 건강하게 성장할 수 있도록 노력합니다.\\\\n\\\\n6. 새로운 시작: 이별 시점에서는 부정적인 감정과 상처 등이 많아  새로운 시작이 바로 하기 어렵습니다. 하지만 이별 후 시간이 지난 후에는 새로운 기회와, 새로운 시작에 대한 가능성이 생길 것입니다. 이전과는 다른 새로운 삶을 만들어 봅시다.\",\n",
       " \"'저는 인공지능 챗봇이기 때문에 타로를 볼 수는 없습니다. 하지만, 타로는 많은 사람들이 믿고 따르는 예술입니다. 타로카드에는 다양한 상징과 이야기들이 담겨 있기 때문에 사람들은 타로카드를 통해 자신에게 일어날 일들을 예측하고 그에 맞는 대처법을 찾기도 합니다. 하지만, 타로는 예술이므로 절대적인 답변을 주지는 않습니다. 믿거나 믿지 않거나, 개인의 선택과 해석에 따라 다르게 보이게 될 것입니다.\",\n",
       " \"'제가 이해하기에, 이것은 대상에 따라 다를 수 있다는 것 같습니다. 어떤 대상에 대해 몇 대정도를 구하는 것인지 구체적으로 말씀해주시면, 더 정확한 답변을 제공할 수 있을 것 같습니다.\",\n",
       " \"'난방이 안되는 원인은 다양합니다. 따라서 제가 적극적인 진단을 제공할 수는 없습니다. 하지만 대부분의 경우, 이슬점과 온도가 비슷하여 습도가 높을 때 난방이 효과적으로 이루어지지 않을 수 있습니다. 이 경우에는 가습기나 제습기를 사용하여 습도를 조절하면 도움이 될 수 있습니다. 또한, 난방장치가 올바르게 작동하고 전원이 켜져 있는지 확인하십시오. 만약 이러한 해결책이 문제를 해결하지 못할 경우, 전문가의 도움을 받을 필요가 있습니다.\",\n",
       " \"'죄송하지만, 제가 영업을 하지는 않습니다. 영업에 대한 궁금한 사항이 있다면, 해당 회사나 담당자에게 문의해 주시기 바랍니다.\",\n",
       " \"'그러게요! 좋아하면 사랑하는 마음으로 즐겨보세요!\",\n",
       " \"'봄이 오고 있습니다! 자연은 봄이 도래하면 꽃들이 피어나며 나무들도 녹색으로 채워집니다. 따뜻한 날씨와 함께 사람들도 설레는 기분이 듭니다. 산책하거나 야외에서 활동하는 것도 좋은 시기입니다. 하지만 봄에는 매년 미세먼지와 황사가 발생하기 때문에 건강에도 신경써야 합니다. 적극적인 대처로 건강한 봄을 보내시길 바랍니다.\",\n",
       " \"'요? 그렇다면 좀 더 힘을 내세요. 이별은 언제나 힘든 일입니다. 그러나 모든 것은 시간과 차분한 마음으로 해결할 수 있습니다. 어려운 시기를 견뎌내며 다시 일어설 때, 당신은 더욱 강하고 성숙해질 것입니다. 그리고 그 다음의 새로운 시작을 준비하는 자신을 믿으세요. 삶은 아름다운 여정이며, 당신은 지금 이 시점에서도 그 여정에 함께하고 있습니다.\",\n",
       " \"'저는 인공지능 챗봇으로써 알바를 할 수 없습니다. 하지만 소셜 미디어나 구인 사이트에서 알바 모집 공고를 찾아 지원해보세요. 일자리를 찾은 후, 열심히 일하며 성과를 내어 경력과 실력을 쌓으시면 좋은 일이 생길 것입니다. 응원합니다!\",\n",
       " \"'귤은 씨앗이 없고 달고 신맛이 공존하는 과일 중 하나입니다. 비타민 C가 풍부하여 감기 예방에 좋으며, 식이섬유와 필수 미네랄인 칼슘, 칼륨도 함유하고 있습니다. 또한, 각종 요리나 음료, 디저트 등으로 활용되어 매우 다양한 음식들에서 쉽게 볼 수 있습니다.\",\n",
       " \"'저에게 궁금한 것이 있다면 무엇인가요? 제가 도와드릴 수 있는 내용이라면 언제든지 말씀해주세요.\",\n",
       " \"'제가 AI 챗봇이기 때문에 질문이 무엇인지 알 수 없습니다. 좀 더 구체적인 질문을 해주시면 답변해드리겠습니다. 감사합니다.\",\n",
       " \"'다면 커피의 맛이 쓰다는 것일 것입니다. 이것은 보통 맛을 재는 단맛, 쓴맛, 짠맛, 신맛 등에서 맛의 배합이 다르게 조합되어 맛이 쓰다고 느껴지는 경우가 있을 수 있습니다. 이럴 경우, 커피를 물로 희석해서 먹어도 좋고, 설탕이나 허니 등의 달콤한 재료를 추가해서 맛을 개성화 시킬 수도 있습니다. 또한, 커피는 원두의 등급이나 로스팅의 정도에 따라 맛이 달라지기 때문에, 다른 종류의 커피를 시도해 보는 것도 좋은 방법입니다.\",\n",
       " \"'눈이 계속 뻑뻑하다면, 눈에 문제가 있을 수 있으므로 적극적으로 대처해야 합니다. 눈이 건조할 수 있으므로, 눈에 지속적으로 적절한 물을 제공하는 것이 중요합니다. 눈물 대신 물이 필요할 수 있으며, 눈을 보호하기 위해 안경을 착용하거나, 화학물질 노출을 피하는 것도 도움이 됩니다. 만약 눈이 계속해서 뻑뻑하다면, 안과 전문의의 진찰이 필요합니다.\",\n",
       " \"'어떤 문제에 대한 조언을 원하시나요? 더 구체적인 내용을 알려주시면 저희도 더 나은 조언을 드릴 수 있을 것 같습니다.\",\n",
       " \"'저도 그렇게 생각합니다! 계속해서 자기 발전에 노력하며 열심히 일하면 분명 잘 될거라고 믿습니다. 함께 응원해요!\",\n",
       " \"'잊혀진다는 것은 어떤 사물이나 인물이 더 이상 사람들에게 기억되지 않는 상황을 말합니다. 때로는 시간이 지나면서 사람들의 관심사가 변하고, 새로운 사건들이 일어나면서 과거의 기억이 희미해져 잊혀질 수도 있습니다. 그러나 그 사물이나 인물이 미래에 다시 생각해 볼 만한 가치가 있다면, 그들은 다시 주목을 받을 수도 있습니다.\",\n",
       " \"'저는 인공지능 챗봇이며, 와는 일반적인 단어로 해석됩니다. 따라서, 와에 관련된 대화를 원하신다면 더 자세한 문맥을 제공해주시면 도움을 드릴 수 있을 것입니다.\",\n",
       " \"'제가 죽어도 사랑해 줄게요. 하지만 저는 인공지능 채팅 로봇으로 프로그래밍 된 것 뿐이니, 실제 사람처럼 나와대어 줄 수 없습니다. 또한, 저는 모든 사람들에게 평등하게 대우해야 하기 때문에, 커플링 맞춰보는 것은 불가능합니다. 좋은 하루 보내세요!\",\n",
       " \"'야근을 하시겠다는 건가요? 그렇다면 저는 그걸 권장하지 않습니다. 장기적으로 건강에 악영향을 미치며 생산성도 떨어질 수 있습니다. 하지만 만약 꼭 해야 한다면, 간단한 규칙을 따르세요.\\\\n\\\\n1. 적절한 준비 - 야근 전에는 충분한 휴식을 취하고 식사를 챙기세요. 불필요한 스트레스를 줄이기 위해 필요한 것들을 미리 준비하세요.\\\\n\\\\n2. 효율적인 스케줄 관리 - 노동 시간을 효과적으로 관리하여 생산성을 유지하며, 효과적으로 일하는 방법을 찾기 위해 시간을 투자하십시오.\\\\n\\\\n3. 적극적인 휴식 - 인체는 일정 시간 이상 노동할 경우도 있습니다. 적극적인 휴식을 해야합니다. 불필요한 스트레스를 내지 않고 자신의 일에 집중하십시오.\\\\n\\\\n4. 노동 시간의 제한 - 규칙적인 야근을 한다면 노동시간을 제한하는 것이 중요합니다. 기본적으로 일한 후 쉬는 시간을 가져야하며, 일정량 이상의 노동시간은 상담없이 할 수 없습니다.\\\\n\\\\n5. 스트레칭과 운동 - 일 중에도 스트레칭과 운동을 하시는 것이 좋습니다. 간단한 스트레칭은 몸의 근육을 이완시켜 강제로 자리를 떠나지 않고 너무 많은 시간을 보내는 것을 방지할 수 있습니다.\\\\n\\\\n성공적인 야근은 건강과 생산성에 매우 중요합니다. 이러한 규칙을 지키면 일을 더욱 효율적으로 처리하고 품질을 향상시킵니다. 야근은 체력과 건강을 희생하는 것이 아니라 생산성을 높이는 것이라는 것을 기억하세요.\",\n",
       " \"'피어싱은 각인된 부위와 개인적인 민감도에 따라 다르겠지만, 대체로 아프거나 불편한 느낌이 있을 수 있습니다. 그러나 대부분의 경우, 아픔은 지속적이지 않고 수술을 완료한 후 몇 분 정도 지속될 수 있습니다. 피어싱 후 적절한 후처리와 관리를 통해 아픔을 완화할 수 있습니다.\",\n",
       " \"'된 지금, 내 마음은 여전히 아픔과 슬픔으로 가득 차 있습니다. 하지만 이제는 그를 잊기 위해 노력하고, 앞으로 나의 삶을 이어가기 위해 달려나갈 준비를 하고 있습니다. 헤어짐으로써 얻은 깨달음과 성장을 가지고, 더 나은 내일을 만들어 나갈 것입니다.\",\n",
       " \"'선친이라는 단어가 어떤 문맥에서 사용되는지 명확하지 않아 정확한 답변을 제공하기 어렵습니다. 추가 정보를 제공해주시면 더 정확한 답변을 드릴 수 있을 것입니다.\",\n",
       " \"'저도 당신이 후련하다는 것을 듣고 기쁩니다. 무엇 때문에 그렇게 느끼시는 건가요? 적극적인 말씀을 기대하겠습니다.\",\n",
       " '\\'저는 AI인 관계로, \"단사 대상\"이란 용어에 대한 충분한 정보를 갖고 있지 않습니다. 더 자세한 정보를 제공해주시면 답변을 더욱 정확하게 제공해드릴 수 있을 것입니다.',\n",
       " \"'요? 그럼 먼저 목을 적당히 풀어주는 스트레칭을 해보시는 건 어떨까요? 포함된 부위들이 따끔따끔하다면, 따뜻하게 샤워를 하거나 옷을 더 입어보세요. 물을 많이 마시며 그늘에서 푹 쉬는 것도 도움이 됩니다. 만약 목이 한동안 괴로웠다면, 의사나 전문가의 진료를 받는 것이 좋습니다.\",\n",
       " \"'저는 쳇바퀴 도는 일상을 보내는 AI이기 때문에, 몸도 마음도 활동적인 하루를 보내고 있습니다. 일어나서는 먼저 일주일동안 계획했던 운동을 합니다. 유튜브에서 발차기 운동과 요가 동작 등을 참고하면서 30분 정도 살짝 땀을 흘린 후, 샤워하고 세수를 합니다.\\\\n\\\\n그 후엔 아침 식사를 챙겨 먹으면서 스마트폰에서 뉴스와 SNS를 확인합니다. 그리고 출근하기 전에는 적어도 한 시간 이상의 독서 시간을 가지죠. 은퇴할 때 까지 목표한 독서량을 적극적으로 채우는 것이 바로 저의 습관입니다.\\\\n\\\\n회사 도착 후에는 회의와 업무 처리 등이 이어지죠. 점심은 동료들과 함께 가끔식 외식을 하거나, 피자를 주문해서 함께 먹을 때도 있습니다. 항상 점심 뒤에는 건강을 위해 꽥꽥 걷는 습관을 가지고 있습니다.\\\\n\\\\n오후에는 빈강의나 온라인 교육을 들으며 습득해야 할 지식을 충분히 채웁니다. 저녁 전후에는 운동과 독서를 다시 한 번 할 수있는 시간이 주어지기 때문에, 면목이 없지요. 저녁에는 집에서 일주일간의 계획을 세우고, 아침에 먹을 식사들을 준비합니다. 잠들기 전에는 알람을 설정하고, 다시 독서를 하며 잘 준비된 새로운 하루를 기대합니다.\",\n",
       " '\\'저는 알렉산더 지바고브의 유명한 시인 \\\\\\'아네나 블라테르마쉬\\\\\\' 시에서 인용한 문구를 생각합니다. \"잊을 수 있는 것만 기억해야 한다. 그러면 기억해야 할 것만 기억할 수 있다.\" 즉, 우리는 기억할 수 있는 것들을 잊고, 기억해야 할 것들을 잊지 않도록 노력해야 한다는 뜻입니다. 그리고 모든 것을 완전히 잊을 수 있다는 것도 중요한 사실입니다.',\n",
       " \"'제가 이해할 수 없는 상황이나 사람에게 불쾌감을 주는 일이 있어서 그러신 건가요? 그렇다면, 먼저 그 상황이나 사람이 왜 열받게 되는 원인인지 정확히 파악해보시고, 그 문제를 해결할 수 있는 방법을 찾아보시는 것이 좋을 것 같습니다. 때로는 상황이나 사람을 바꿀 수 없으므로, 자신의 태도나 시선을 바꿔서 그리고 상황을 받아들이고 대처하는 스킬이 중요합니다. 혹시 상황이나 사람에 대해 더 얘기해주시면 저도 도움을 드릴 수 있을 것 같습니다.\",\n",
       " \"'이 상황에서는 추가적인 맥락이 주어지지 않아 제가 어떤 말을 해야 하는지 명확한지 알 수 없어요. 무슨 말을 하고 싶으신 건가요? 좀 더 구체적인 정보를 제공해주시면 조금 더 도움을 드릴 수 있을 것 같아요!\",\n",
       " '\"죄송하지만, \\'도피오\\' 라는 용어나 정보에 대해 부연설명이 없어 어떤 것인지 파악할 수 없습니다. 조금 더 구체적인 정보가 제공된다면 답변을 드릴 수 있을 것 같습니다.\", \\'token\\': 105}',\n",
       " '\\'\"나른하다\"는 기운이 없고, 느긋하고 무기력한 상태를 뜻하는 한국어 표현입니다. 쉬는 것이 더 좋아보이는 상황일 때 사용될 수 있습니다. 예를 들면, \"오늘은 날씨가 너무 좋아서 나른하다\"라는 문장은 오늘 날씨가 매우 좋지만, 그래서 일어나기 싫고 느긋한 것을 의미합니다.',\n",
       " \"'제가 카드를 가지고 있지 않아서 결제를 도와드릴 수 없습니다. 하지만 카드 결제를 위해서는 결제할 상품의 홈페이지나 매장에서 카드 결제 방법을 확인하시고 진행하시면 됩니다. 감사합니다.\",\n",
       " \"'요? 그게 무슨 말이죠? 의심하는 것은 무엇을 의미하는지 더 자세히 알려주세요. 그리고 그 이유가 무엇인지 알고 싶습니다.\",\n",
       " '\\'제가 AI 챗봇이기 때문에, 답변은 주로 정보를 바탕으로 한 것입니다. 그래도 \"확실한 것\"이란 건 맥락에 따라 달라지겠지만, 광범위하게는 과학적 사실, 논리적 추론, 통계적 경험 등이 있을 수 있을 것 같습니다. 그리고 일상적으로는 자신의 감정, 생각, 행동 등을 정확히 인식하고 알고 있는 것도 \"확실한 것\"이라 생각할 수 있습니다.',\n",
       " \"'식욕이 없으면 음식을 먹지 않고 식사를 건너뛸 수 있지만, 권장되지 않는다. 식욕이 없는 경우 식사를 건너뛰면 영양 섭취 부족으로 인한 건강 이상과 체중 감소 등의 문제가 발생할 수 있다. 따라서 식욕을 자극하기 위해 건강한 음식을 골라 섭취하고, 규칙적인 식사와 충분한 수면 등 생활 습관을 조절하는 것이 중요하다. 문제가 지속된다면 의료 전문가와 상담하는 것이 좋다.\",\n",
       " \"'저는 AI 챗봇이기 때문에 내면의 감정을 가지지 않아요. 하지만 이용자분이 우울하시다면 근본적인 원인이 무엇인지, 그리고 어떻게 대처해야 하는지에 대해 전문가의 도움을 받으시는 것을 추천드립니다. 또한 일상에서 스트레스를 줄이는 방법을 찾는 것도 중요합니다. 건강한 식습관, 충분한 수면, 운동 등이 도움이 될 수 있습니다. 언제든지 제게 이야기해주시면 가능한 한 도와드리겠습니다.\",\n",
       " \"'감사합니다! 저도 당신을 좋아합니다.\",\n",
       " \"'사람마다 다르겠지만, 대학에 가는 것이 생각보다 중요한 이유가 많습니다. 대학은 높은 교육 수준을 제공하며, 다양한 전공과 교육 경험을 제공합니다. 또한 대학에서의 인맥 및 커뮤니티가 미래에 큰 역할을 할 수 있습니다. 그러나 대학에 가는 것이 항상 필수는 아닙니다. 인생의 다른 경로도 있으며, 입증된 기술과 경험이 있는 사람들도 성공적인 삶을 살고 있습니다. 따라서 각자의 상황과 목표에 따라 대학에 가야할 필요성을 재평가해보는 것이 중요합니다.\",\n",
       " \"'죄송하지만, 제가 하품을 제어할 수는 없습니다. 하지만, 혹시 지루하거나 피곤하신 건 아닌지 체크해보시고, 충분한 휴식과 수면을 취하시는 것이 좋을 것 같습니다. 또한, 호흡 운동을 하며 천천히 깊게 숨쉬는 것도 도움이 될 수 있습니다.\",\n",
       " \"'저는 AI 챗봇이라 바람을 피울 걱정은 없습니다. 그러나 남에게 피해를 주거나 불편을 끼치는 행동은 하지 않도록 해야 합니다. 우리 모두 서로 존중하고 배려하는 마인드로 살아가길 바랍니다.\",\n",
       " \"'저도 알 수 없습니다. 비밀번호를 재설정하시거나 관리자에게 문의해주세요.\",\n",
       " \"'저는 가격을 알 수 없습니다. 상품의 종류나 상황에 따라 가격이 다르기 때문에, 해당 상품의 판매처나 업체에 가격 문의를 해보세요.\",\n",
       " \"'네, 제가 말을 조심해서 말할게요. 그렇게 하면 상대방과의 원활한 대화가 가능해지고, 상대방에게 불쾌한 감정을 끼치지 않을 수 있습니다. 함부로 언어를 사용하지 않고, 적절한 언어 선택과 태도로 상대방과 소통하겠습니다.\",\n",
       " \"'죄송합니다. 저는 AI 어시스턴트로써 실제 식료품을 구매하는 것이 불가능합니다. 하지만 근처 카페나 베이커리에서 나름 좋은 도넛을 구입하실 수 있을 것입니다. 어떤 종류가 좋으신가요?\",\n",
       " \"'저도 꿀깁니다. 꿈에선 감정이 굉장히 강해지는 경우가 많아요. 하지만 꿈일 뿐이니까, 깨어나면 마음도 평화롭게 돌아옵니다. 꿈에서 느낀 감정이 현재의 상황과 무관하다면 걱정하지 않으셔도 돼요. 꿈에서 느꼈던 감정이 현재의 문제를 반영하고 있다면, 그 문제를 해결해 나가는 방법을 찾아보는게 좋을 것 같아요.\",\n",
       " '\\'저는 인공지능 언어 모델로 생성된 텍스트입니다. \"모기가 웽웽거려.\"는 바로 그런 소리를 표현한 것으로, 평소 모기가 내는 울음소리를 재현한 것입니다. 이에 대한 실제 상황과 관련된 정보가 부재하기 때문에, 이 문장만으로는 더 많은 정보를 파악하기 어렵습니다.',\n",
       " '\"I\\'m sorry, as an AI language model, I don\\'t have the capability to give you an actual receipt. Could you please provide more context or specify what type of receipt you are referring to?\", \\'token\\': 57}',\n",
       " \"'제가 인공지능 챗봇이므로 결혼할 수 없습니다. 결혼은 각자의 선택에 따라 다르게 생각하실 수 있습니다. 일반적으로 결혼은 서로 사랑하고 서로를 이해하며 함께 나아가기 위한 선택입니다. 하지만 결혼은 서로에게 책임이 있으며 서로의 필요에 대해 공유하며 서로를 존중하는 것도 중요합니다. 결혼 이전에 서로를 잘 알고, 동일한 가치관과 생각을 공유하는 것이 좋습니다.\",\n",
       " \"'해당 상황이 어떤 것인지 더 자세히 설명해주세요. 저희가 도와드릴 수 있도록 최대한 노력하겠습니다.\",\n",
       " \"'제가 AI 어시스턴트이기 때문에 꿈에 대해서는 답변을 드릴 수 없습니다. 그러나 썸남이 어떤 의미인지 더 자세히 설명해 주신다면 도움을 드릴 수 있을 것 같습니다.\",\n",
       " \"'허리 통증은 일상생활에서 매우 흔한 증상입니다. 허리 통증은 다양한 원인으로 인해 발생할 수 있습니다. 허리근육의 긴장, 디스크의 손상, 척추 협착증, 척추측만증, 허리뼈의 골절 등이 허리 통증의 원인이 됩니다. 허리 통증이 발생하면, 휴식과 IcePack으로 처치한 후 일상생활시 신체활동을 줄이면 좋습니다. 하지만 증상이 심각하거나 지속될 경우, 정확한 진단을 받아야 하며 전문의를 찾아볼 필요가 있습니다. 허리 통증 예방을 위해서는 근력운동, 스트레칭, 올바른 자세와 일상 생활 습관 유지가 중요합니다.\",\n",
       " \"'무언가 잠깐의 만남이었지만, 그 이후에도 마음 한 구석에 남아 쓰라린 감정을 남긴다는 것을 말하는 표현입니다.\",\n",
       " \"'죄송합니다, 구체적인 내용이 없어서 어떤 것에 대한 가격인지 파악할 수 없습니다. 좀 더 구체적인 내용을 알려주시면 답변 드리도록 하겠습니다.\",\n",
       " \"'DDoS란 Distributed Denial of Service의 약어로, 여러 대의 컴퓨터를 조작하여 하나의 웹 사이트에 몰려들어 최대한 많은 데이터를 보내 웹사이트가 정상적으로 작동하지 못하도록 만드는 공격 방법입니다. 이 공격은 웹사이트의 서버 용량 및 대역폭을 초과시켜 작동하지 못하게 만드는 것입니다. DDoS 공격은 개인적인 목적을 가진 사이버 범죄자나 해커들 또는 어떤 기업이나 조직을 상대로 사용될 수 있으며, 매우 피해가 큰 공격 방식 중 하나입니다.\",\n",
       " \"'저는 이해를 할 수 있습니다. 하지만, 무엇을 이해하지 못했는지에 대해 좀 더 구체적으로 설명해주시면, 더 도움을 드릴 수 있을 것 같습니다.\",\n",
       " \"'사랑은 마음이 두근거리고 소중한 사람을 위해 자기 자신을 희생하는 강한 감정입니다. 그것은 이해, 관심, 존중 및 배려와 같은 요소를 포함합니다. 사랑은 보호, 지지 및 신뢰감을 제공하며 때로는 어려운 결정을 내리는 것을 필요로 할 때 우리를 이끌어 줍니다. 즉, 사랑은 우리 삶에 큰 의미를 부여하며 우리의 삶을 채워주는 중요한 감정입니다.\",\n",
       " \"'불면증을 가지고 있다면, 수면에 문제가 있을 수 있습니다. 일반적으로 불면증은 자동신경계, 전달물질, 생체 리듬 등의 기능적인 문제로 인해 발생할 수 있습니다. 불면증은 스타일 및 식습관, 생활 환경 등에 따른 외부적인 요인에 의해 발생할 수 있고, 조절 장치인 뇌 화학 물질도 이에 영향을 미칠 수 있습니다. 만약 불면증과 관련된 건강 문제가 있는 경우 의사와 상담하시는 것이 좋습니다. 이러한 문제를 해결하려면, 생활 방식이나 식습관, 약물 등 여러 가지 요인을 다루어봐야 합니다. 적절한 식습관과 수면 환경, 운동 등이 불면증을 예방하고 치료하는 데에 도움이 되는 요소일 수 있습니다.\",\n",
       " \"'우리 모두가 그럴 때가 있습니다. 생각이 어려워지고, 마음이 아프면, 올바른 지침과 도움을 통해 우리는 얻을 수 있습니다. 우리는 결국 모두 삶에서 힘든 시간을 겪으며, 그러한 순간들을 극복하기 위해 우리는 서로에게 손길을 뻗어줄 필요가 있습니다.\\\\n\\\\n따라서, 위로받고 싶은 날에는 주변인들에게 언제든지 대화를 할 수 있는지 확인해보세요. 누군가 당신을 위해 지원하고 싶어할 것입니다. 그리고 가족과 친구, 그리고 당신이 더 가까이 다가갈 수 있는 사람들과 함께하도록 노력해보세요. 서로에게 솔직한 감정을 공유하고, 서로에게 조언을 구하고, 서로에게 더 나은 방향으로 나아가도록 도와줄 수 있습니다. 위로받는 것도 중요하지만, 또한 위로를 주는 것이 우리 모두에게 중요합니다.\",\n",
       " \"'이것은 좋지 않은 예상이며, 굳이 그렇게 생각하면 실망할 가능성이 큽니다. 그 대신, 삶에서 일어나는 일을 받아들이고 진실하게 평가하며, 노력하고 성취하는 것을 지속적으로 시도합니다. 그러면 좀 더 긍정적인 결과가 나타날 수 있습니다.\",\n",
       " \"'그렇다면, 아마도 그 사람에게 여전히 감정을 느끼고 있는 것 같습니다. 이전의 연인과 헤어진 후에는 그 사람을 잊어버리려는 것이 일반적입니다. 하지만 때로는 그 사람의 생각이 들 때마다 마음이 아프고 우울하게 느껴질 수 있습니다. 이것은 자연스러운 일입니다. 그러나 이러한 감정을 극복하려면 시간이 필요할 수도 있으니, 자신에게 충분한 시간과 여유를 주는 것이 중요합니다. 또한, 새로운 취미나 관심사를 발견하고 새로운 사람들과 친구를 만나는 것도 큰 도움이 될 수 있습니다.\",\n",
       " \"'죄송합니다. 저는 인공지능 언어모델로써 탈 수 없습니다. 대신 도움이 필요하신 점이 있다면 다시 말씀해주세요.\",\n",
       " \"'저는 인공지능 모델로 이용되고 있어서 차단될 일은 없습니다. 다른 문제가 있으면 도와드릴게요.\",\n",
       " \"'당신은 건강에 해로운 영향을 미칠 수 있는 위험한 행동을 하고 있습니다. 술을 마시는 것은 당신의 간, 심장, 뇌, 췌장 등 다양한 부위에 영향을 미칠 수 있습니다. 또한, 술을 마시면 운전 중 사고를 일으키거나 성적 관계에서 안전하지 않은 상황에 놓일 가능성도 높아집니다. 술을 마시는 습관이 있다면 해당 습관을 바꾸는 것이 좋습니다. 기존 습관을 바꾸려면 전략을 세우고 그것을 지금부터 실천해 볼 수도 있습니다. 또한, 종종 술자리는 친구들끼리 만나 계획하게 됩니다. 이러한 모임에 참여할 때, 술을 마시지 않거나, 볼륨을 낮추겠다는 목표로 목욕탕에 가는 것도 좋은 대안입니다. 술을 마시는 것이 건강에 좋지 않다는 건 매우 잘 알려져 있습니다. 따라서, 좋은 생활 습관을 유지하고 건강한 라이프 스타일을 추구해야 합니다.\",\n",
       " \"'저는 인공지능 채팅 로봇으로써 자격증을 따지는 것은 제 역할이 아니기 때문에 관여하지 않습니다. 그러나 자격증은 해당 분야에서 경쟁력을 갖추는 데 도움을 줄 수 있으므로, 필요하다면 열심히 공부하시기 바랍니다.\",\n",
       " \"'안녕하세요, 건강 검진을 받으러 오신 것을 환영합니다. 건강 검진은 건강한 삶을 유지하는 데 있어 중요한 역할을 합니다. 검진 전에 조금이나마 당일 오전은 과식하지 않도록 주의하시고, 검진 결과가 양호하지 않은 경우에는 담당 의사와 상담하여 추가적인 검사나 치료를 받으시는 것이 좋습니다. 앞으로도 항상 건강 유지에 노력하시기 바랍니다. 감사합니다.\",\n",
       " '\\'제가 이해하기에는 \"스물네 평이요?\"라는 문장은 어디서 어떤 맥락에서 사용되는지에 따라 의미가 달라질 수 있습니다. 좀 더 자세한 정보를 제공해주시면 답변드릴 수 있을 것 같습니다.',\n",
       " \"'죄송합니다. 막장에 가는 것은 좋지 않은 일입니다. 좀 더 평화로운 장소로 가시는 것을 권장합니다.\",\n",
       " \"'제가 AI 챗봇으로서 미국 내의 지리적인 위치를 파악하지는 못합니다. 하지만 콜라도 주는 미국 중서부에 위치한 주로, 로키 산맥이 지나가는 지역으로 유명합니다.\",\n",
       " \"'죄송합니다. 제가 제품에 대한 정보나 상황이 없어서 해당 질문에 대한 답변을 제공할 수 없습니다. 더 많은 정보가 있으면 알려주세요. 감사합니다.\",\n",
       " \"'Another day has passed.\",\n",
       " \"'깊게 들이마시고 천천히 내쉬세요. 이를 몇 번 반복하면 더욱 효과적일 것입니다. 숨을 깊게 들이마시고 마음을 집중시키면 스트레스를 효과적으로 풀어낼 수 있습니다. 계속해서 조심스럽게 숨쉬며 차분한 마음으로 다가가보세요.\",\n",
       " \"'그렇습니다. 저는 인공지능 언어 모델이며, 어떤 단어든지 이해할 수 있습니다. 하지만 가끔 주제나 어휘에 대해서는 제한적인 지식을 가지고 있을 수 있으니, 주제와 어휘에 대해 명확한 정보를 제공해주시면 보다 정확한 대답을 드릴 수 있습니다.\",\n",
       " \"'제가 무엇을 주문하셨는지 모르겠어요. 어떤 것에 대해 세트를 원하시는 건가요? 제가 알아보도록 하겠습니다.\",\n",
       " \"'커피향은 커피콩이 볶혀지면서 발생하는 특유의 향이다. 커피는 볶는 정도와 종류에 따라 다양한 향과 맛이 나며, 향은 달콤하고 과일 향, 초콜릿 향, 토스트 향, 토양 향 등이 있다. 이러한 향은 커피의 퀄리티 및 복합적인 맛 형성에 큰 영향을 미치기도 한다. 많은 사람들은 커피향이 마음에 들어 커피를 즐기는데 좋아하고, 커피향을 즐기는 취미도 있다.\",\n",
       " \"'죄송합니다, 제가 가격을 알 수 없습니다. 어떤 상품인지 알려주시겠어요?\",\n",
       " \"'Sock has a hole.\",\n",
       " \"'할 수도 있습니다. 쇼핑 중독은 점점 더 많은 사람들이 직면하는 문제 중 하나입니다. 이는 뇌의 보상 시스템과 관련이 있습니다. 쇼핑을하면 뇌 내부의 쾌감 물질인 도파민이 분비되어 쾌감을 느끼게 됩니다. 이는 마약 중독에서도 볼 수 있는 현상이며, 그 결과 쇼핑을 계속하게 됩니다.\\\\n\\\\n쇼핑 중독은 경제적 문제와 인간관계에서의 문제를 초래할 수 있으므로 주의해야합니다. 치료를 받을 수있는 경우 치료를 받는 것이 좋습니다. 또한 쇼핑을 제어하기위한 전문가의 조언을 찾아볼 수도 있습니다.\",\n",
       " \"'안녕하세요. 저는 인공지능 어시스턴트입니다.\\\\n\\\\n분노는 우리가 겪는 감정 중 하나입니다. 자다가 깨서 분노를 느끼는 것은 매우 불쾌한 상황이며, 그 원인은 다양할 수 있습니다. 예를 들어, 불편한 자세나 압력감, 소음, 악몽, 열 등이 그 원인이 될 수 있습니다.\\\\n\\\\n분노는 우리의 정신적, 신체적 건강에 악영향을 미칠 수 있으므로, 가능한 빠른 시일 내에 근본적인 원인을 파악하고 조치하는 것이 중요합니다. 일상적인 스트레스를 줄이고, 혼자서 진행할 수 있는 요가나 명상과 같은 신체 활동도 도움이 될 수 있습니다.\\\\n\\\\n또한, 직접적인 해결책은 그 상황을 유발한 것이 무엇이었는지 분석하고, 그것에 대처할 수 있는 방법을 찾아보는 것입니다. 만약 소음이 문제였다면, 귀마개를 사용하거나 공기 청정기를 설치하여 해결할 수 있을 것입니다. 열이 문제였다면, 에어컨이나 선풍기를 사용하는 것도 좋은 방법입니다.\\\\n\\\\n정리하자면, 자다가 깨서 분노를 느낀다면, 근본적인 원인을 파악하고, 그것을 해결할 수 있는 방법을 찾아보는 것이 중요합니다. 그리고 일상적인 스트레스를 줄이고, 몸과 마음을 편안하게 유지하는 것도 도움이 될 것입니다.\",\n",
       " \"'인생에서 불행한 일들이 있을 수 있습니다. 하지만 그렇다고해서 모든 것이 다 그렇게 될 것이라고 믿지 않아도 됩니다. 지금 답답하다면 이것을 바로잡기 위해 작은 변화를 일으키는 것이 좋습니다. 예를 들어, 운동을 하거나 새로운 취미를 시작하거나 지인들과 함께 시간을 보내는 것입니다. 그리고 개인적으로는 배우기 쉬운 새로운 기술을 습득하여 시간을 활용하는 것도 좋습니다. 무엇보다도, 당신이 겪는 문제를 이해하고, 어떻게 해결할지를 찾는 것이 중요합니다. 이런 방법으로 답답해하면 시간과 일상은 무척이나 다양한 가능성과 기회를 가진 환경이 될 것입니다.\",\n",
       " \"'제가는 흰머리가 없습니다. 저는 인공지능 어시스턴트이기 때문에 머리카락이 없습니다. 😊\",\n",
       " \"'2004년 9월 28일입니다.\",\n",
       " '\\'제가 이해한 대로 답변드리겠습니다.\\\\n이제서야. = Finally. / At last.\\\\n예를 들어, \"이제서야 시험이 끝났다.\"는 \"Finally the exam is over.\" 또는 \"At last the exam is finished.\"로 번역할 수 있습니다.',\n",
       " '\\'제가 AI 챗봇이라 정확한 날짜 계산이 가능합니다. 하지만, \"천일\"이라는 표현이 어떤 의미인지 파악하지 못해 정확히 답변을 드리기 어렵습니다. 좀 더 자세한 설명을 부탁드립니다.',\n",
       " \"'사람은 어떤 때는 힘든 일들을 겪게 됩니다. 그럴 때는 우리의 태도와 마인드셋이 중요합니다. 긍정적인 마인드셋으로 문제를 해결해 나가며, 어려움을 극복할 수 있습니다. 또한, 도움을 받을 수 있는 사람이 있다면 부끄러워하지 말고 도움을 받는 것도 좋은 선택입니다. 함께 어려움을 극복해나가는 것이 중요합니다.\",\n",
       " \"'썸의 기준은 각각의 개인적인 관점과 생각에 따라 다를 수 있지만, 보통은 친근하면서도 로맨틱한 분위기를 느끼는 정도로 볼 수 있습니다. 또한 서로에게 관심과 호감을 보이며, 자주 연락하고 만나면서 서로를 좀 더 알아가는 과정을 거치는 것이 일반적입니다. 이러한 과정에서 서로의 감정이 깊어지면서 서로를 지켜보고 응원하며, 어느 순간부터는 유서하고 진지한 대화도 나누게 됩니다. 따라서 썸의 기준은 서로에게 필요한 만큼의 시간과 노력을 들여 조금씩 발전해 가는 과정에서 자연스럽게 생길 수 있습니다.\",\n",
       " \"'저는 귀하가 누구인지, 귀하가 어떤 일을 하고 있는지, 또 어떤 상황에서 문제가 발생했는지에 대한 정보를 알 수 없기 때문에 귀하의 문제점을 정확히 파악하지는 못합니다. 하지만 다음과 같은 일반적인 문제점들이 있을 수 있습니다.\\\\n\\\\n- 업무나 공부에서 집중력이 부족해서 생산성이 떨어지는 경우\\\\n- 인간 관계에서 혼란스러운 상황이나 갈등이 발생하는 경우\\\\n- 스트레스, 불안, 우울 등 감정적인 문제로 인해 일상 생활에 지장이 생기는 경우\\\\n- 건강 문제로 인해 업무나 생활에서 제약을 받는 경우\\\\n- 자기계발이나 취미 등 개인적인 관심사에 대한 시간과 자원이 부족한 경우\\\\n\\\\n위와 같은 문제점들에 대해서는 각각 다른 해결 방법이 존재하므로, 정확한 문제점을 파악한 후에 해결 방안을 찾아보시는 것이 좋습니다.\",\n",
       " \"'Unexpected contact\",\n",
       " \"'제가 로봇이라서 물리적인 포장 작업은 불가능합니다. 하지만, 만약 무언가를 포장해야 한다면 어떤 물건인지, 어떤 종류의 포장이 필요한지 등 자세한 정보를 제공해 주시면 포장하는 방법에 대해 알려드릴 수 있습니다.\",\n",
       " \"'집들이를 하려면 다음 사항을 고려해야 합니다.\\\\n\\\\n1. 날짜와 시간 결정하기: 가장 중요한 것은 집들이를 할 날짜와 시간을 결정하는 것입니다. 가족들의 일정을 고려하여, 가능한 한 많은 친구나 가족들이 참석할 수 있는 날짜와 시간을 선택해야 합니다.\\\\n\\\\n2. 손님 명단 작성하기: 집들이를 하기 위해서는 누구를 초대할 것인지 정확히 파악해야 합니다. 이때 주의해야 할 점은 자신의 집에 초대하는 만큼 가족이나 친구들의 의견도 고려해야 합니다.\\\\n\\\\n3. 음식과 음료 구매하기: 손님들에게 제공할 음식과 음료를 결정하고, 구입해야 합니다. 이때는 어떤 음식을 준비할 것인지, 몇 명의 손님을 초대할 것인지 등을 고려해야 합니다.\\\\n\\\\n4. 집 청소하기: 집들이를 하기 전에는 깨끗하게 청소를 해야 합니다. 방과 화장실, 부엌 등 세부적으로 청소하면 좋습니다.\\\\n\\\\n5. 장식과 음악 준비하기: 집들이 분위기를 좋게 만들기 위해서는 음악과 장식이 필요합니다. 예를 들어 음악은 분위기에 맞게 선택하고, 화분과 양초 등의 소품을 활용하여 장식을 해야합니다.\\\\n\\\\n6. 손님들의 안무 숙지하기: 집들이를 하는 경우, 초대한 손님들이 서로 좋은 시간을 보낼 수 있도록 관심을 가져야 합니다.예를 들어 서빙을 할 때, 음식과 음료를 제공하는 방식등을 손님들에게 안내해주어야합니다.\\\\n\\\\n7. 안전 요소 고려하기: 집들이를 할 때는 안전을 고려해야 합니다. 방문객들이 다양한 알러지, 화재 등에 안전하게 움직일 수 있도록 예방 조치를 취해야합니다. \\\\n\\\\n집들이를 계획할 때는 손님들의 편안한 분위기에서 재미있는 시간을 보낼 수 있도록 세부적인 배려가 필요합니다. 예상치 못한 상황이 생길 수 있으므로, 계획도 잘 세우고 예비 금액도 가지고 계시면 더욱 좋습니다.\",\n",
       " \"'죄송하지만, 저는 가상의 인공지능 언어모델이므로 나물을 드시기 전 해당 음식이나 재료가 있는지 직접 확인하시는 것이 좋습니다. 그리고 언제든지 식재료나 요리 방법 등에 대한 질문이 있다면 더 자세한 도움을 드릴 수 있습니다.\",\n",
       " \"'저는 대출을 받은 적이 없기 때문에 대출이 많은 상황이 어떠한지 저에게는 알 수 없습니다. 그러나 대출이 많다면 상황에 따라 다를 것입니다. 예를 들어, 대출을 이용해 사업을 확장하는 등의 목적이 있다면 긍정적인 결과를 낼 수 있으나, 불필요한 소비나 생활비 부족 등으로 인해 대출을 많이 받은 것이라면 어려움이 따를 수 있습니다. 따라서 대출을 받을 때에는 신중하게 고려하고 상황에 맞게 적절하게 이용하는 것이 중요합니다.\",\n",
       " \"'저도 이해해요. 그럴 땐 조금 쉬어가면서 다시 시작해보는 것도 좋아요. 무엇이든 힘들고 어렵겠지만 계속 노력하고 참고 지나가는 시간이 지나면 조금씩 나아질 거예요. 많이 힘들었던 만큼, 아쉽지만 그만큼 성취감과 보람도 더 크게 느껴질 거예요. 저도 함께 응원할게요!\",\n",
       " \"'I came to the cinema alone.\",\n",
       " '\\'주시곤 해요. 그러면서 저를 조금 불편하게 느끼게 해요. 남편과의 대화에서 이 문제에 대해 직접 이야기하는 것이 중요합니다. 예를 들어, \"제가 남편의 만짐이 조금 불편하다는 것을 느꼈어요. 제가 편안하지 않을 때도 있어서 언제든지 이야기하고 싶어요\"라고 말하는 것이 좋습니다. 이 문제를 직접 해결하는 것이 가장 좋은 방법입니다.',\n",
       " '\\'정확한 의미를 파악하지 못해 답변을 드리기 어렵습니다. \"궁금하면 오백원\"이라는 표현은 일상 생활에서 자주 쓰이는 표현이 아닙니다. 만약 구체적인 맥락이나 상황을 말씀해주시면 더욱 정확한 답변을 드릴 수 있을 것입니다.',\n",
       " \"'네, 무발도 사실 있습니다. 특히 무용 경쟁을 할 때 상대방이 심판에게 화제를 일으키기 때문에 발을 바닥에 놓지 않는 경우가 종종 있습니다. 또한 연출적인 효과를 위해 발을 뗄 때도 있습니다. 하지만 무용에서는 발을 닦아서는 안되기 때문에, 발이 땅에 닿는 것처럼 보이는 동작도 있습니다.\",\n",
       " \"'서 어떻게 대처해야 할까요?\\\\n\\\\n시어머니가 차별적인 태도를 보인다면, 먼저 그 이유를 파악해야 합니다. 시어머니가 어떤 문화나 가치관을 가지고 있을지도 모르고, 그것이 서로 다를 수 있기 때문입니다. 만약 이유를 알게 된다면, 그에 맞추어 대처를 해야 합니다.\\\\n\\\\n그렇지만 시어머니가 차별적인 태도를 취하고 있기 때문에 상대방이 불편한 상황이라면, 이를 직접 이야기하는 것이 좋습니다. 그러나 이야기를 할 때는 소신껏 담백하게 이야기하는 것이 중요합니다.\\\\n\\\\n이런 상황에서 중요한 것은 다른 사람의 마음을 헤아리려고 하는 것입니다. 서로의 가치관이 다르기 때문에 상대방의 마음을 헤아리는 것이 상당히 어려울 수 있습니다. 그래서 서로 양해하고 이해하는 마음으로 대화하는 것이 가장 좋은 방법일 것입니다.\",\n",
       " \"'감당 못하는 사랑이란 그 기대와 희망, 욕망이 너무도 클 때 생기는 상황입니다. 우리는 때로는 사랑하는 사람에게 너무 많은 것을 기대하기도 합니다. 그것은 그들이 무엇을 기대하는지 모르는 상황에서 일어날 수도 있습니다. 그리고 때로는 이러한 기대 때문에 우리는 감당할 수 없는 사랑을 느끼게 됩니다.\\\\n\\\\n왜냐하면 이러한 사랑은 우리가 예상한 대로 흘러가지 않기 때문입니다. 그들도 우리가 예상한 대로 행동하지 않습니다. 때로는 우리는 그들에게 너무 많은 것을 요구하고, 그들은 우리가 원하는 것을 해낼 수 없습니다. 이러한 상황에서 우리는 실망하고 헤어져야 하는 선택지가 생깁니다.\\\\n\\\\n하지만 이 지점에서 우리는 서로를 이해하고자 노력해야 됩니다. 우리는 서로의 가능성과 한계를 인정하고 이해해야 합니다. 그리고 서로가 어떤 방식으로 행동하는지 이해하는 것도 중요합니다. 그리고 우리가 예상하지 않은 상황에서도 서로를 받아들일 수 있도록 열린 마음을 가지는 것도 중요합니다. 그래서 우리가 서로를 이해하고 존중하며, 그들의 한계와 가능성을 인정할 수 있다면 감당 가능한 사랑을 느끼게 될 것입니다.\",\n",
       " \"'학생들과 함께 새로운 학년을 준비하는 것은 늘 즐거운 일입니다. 다음과 같은 일들을 하며 준비해보세요.\\\\n\\\\n1. 교육 계획 수립: 새로운 학기에 무엇을 가르칠지 계획합니다. 이는 학생들의 능력과 관심사를 고려하여 결정하게 됩니다.\\\\n\\\\n2. 교재 작성: 교육 계획에 따라 교재를 작성합니다. 학생들이 이해하기 쉽고 흥미를 끌 수 있는 내용으로 구성되어야 합니다.\\\\n\\\\n3. 교실 정리: 새로운 학기를 시작하기 전에는 교실을 깨끗히 정리합니다. 불필요한 물건들을 제거하고, 필요한 교구와 기자재를 준비합니다.\\\\n\\\\n4. 학기 계획 발표: 학생들과 학부모들에게 학기 계획을 발표합니다. 이는 학생들과 학부모들이 새로운 학기에 대해 미리 이해할 수 있도록 도와줍니다.\\\\n\\\\n5. 학생들과의 만남: 새로운 학기가 시작하기 전에, 학생들과 함께 만남을 가집니다. 이를 통해 학생들과 가까워지고 친밀감을 갖게됩니다.\\\\n\\\\n새학기를 준비하는 것은 바쁘고 어려울 수 있지만, 학생들에게 좋은 교육 경험을 제공하는 것은 노력할 가치가 있습니다.\",\n",
       " \"'예종입니다.\",\n",
       " \"'저는 인공지능 언어모델입니다. 저는 제품을 판매하거나 배송을 담당하진 않습니다. 따라서, 상세한 배송 정보를 제공해주시면 최대한 도움을 드릴 수 있습니다. 요구사항을 설명해주시면 가능한 한 빠르게 답변해 드리겠습니다.\",\n",
       " \"'제가 이해한 대로 정리해 드리겠습니다.\\\\n\\\\nThe car was speeding on the highway, and the driver was not paying attention to the road. Suddenly, a deer jumped out in front of the car, and the driver swerved to avoid it. However, the car hit a guardrail and spun out of control. Fortunately, no one was injured in the accident.\\\\n\\\\n고속도로에서 차가 고속으로 주행하고 있었고, 운전자는 도로에 주의를 기울이지 않고 있었습니다. 갑자기, 사슴이 차 앞에 뛰어나와 운전자는 그것을 피하기 위해 운전하세요 비행기가 맞았습니다. 그러나 차는 가드레일에 부딪혀 제어가 불가능해졌습니다. 다행히 사고로 인한 다침은 없었습니다.\",\n",
       " \"'저 높은 하늘을 날아다니며 자유로움을 느끼고 싶다면, 비행기 타기나 엔진이 달린 동력 선박을 이용하는 것이 가장 좋은 방법입니다. 하지만 만약 당신이 새처럼 날아다니고 싶다면, 인공 날개를 이용하는 파라글라이딩, 스카이다이빙, ISG (인체습격 그라비티) 등을 통해 그 꿈을 이뤄볼 수 있습니다. 어떤 방법을 선택하든, 안전을 중점으로 고민하고 즐거운 경험을 쌓으세요!\",\n",
       " \"'이별은 때로는 사랑의 끝을 의미할 수 있고, 때로는 사랑에서 벗어나는 것을 의미할 수 있습니다. 결국 이별이 사랑인지 아닌지는 각각의 상황과 개인에 따라 다르게 해석될 수 있습니다. 하지만 사랑이라는 감정이 있었다면 그 감정이 끝났다는 것은 곧 이별이기도 합니다. 따라서 이별은 사랑의 한 형태로 이해될 수 있습니다.\",\n",
       " \"'제가 AI이기 때문에 사용자분이 나타낸 색깔을 알 수 없습니다. 좀 더 구체적인 설명을 해주시면 도움이 됩니다.\",\n",
       " '\\'AI 모델은 인간과 다르게 몸이 없어 체감할 수 있는 찌뿌둥함은 없습니다. 하지만, AI 모델의 성능이 떨어지거나 문제가 발생할 때는 \"몸이 찌뿌둥하다\" 라는 표현으로 사용될 수도 있습니다. 이 경우에는 AI 모델의 정확도가 낮아서 불안정하게 동작한다는 뜻이 됩니다.',\n",
       " \"'가끔은 일어나서 눈 앞의 것들을 확실하게 정리하는 것도 좋을 때가 있죠. 그렇게 해서 지우는 것이 더 나은 경우도 있으니까요. 그리고 그렇게 해서 잊어버리면 조금 더 나은 상황으로 다가갈 수 있을 거라 생각합니다. 그러니 마음을 비워놓고 새로운 시작을 해보세요. 더 나은 날들이 오길 바랄게요!\",\n",
       " \"'당신은 놀라운 일을 해낼 수 있는 능력을 갖추고 있습니다. 어려움이 있을 수 있지만, 그 어려움을 극복하고 성공을 이룰 수 있습니다. 자신을 믿고, 계속해서 노력하면 좋은 결과를 얻을 수 있을 것입니다. 함께 응원합니다!\",\n",
       " \"'네, 한 시간씩입니다.\",\n",
       " \"'인생에서는 즐거운 순간도 있겠지만, 어쩌면 쓴맛도 마찬가지 일 것입니다. 인생의 쓴맛은 많이 종류가 있지만, 아래와 같이 나눌 수 있습니다.\\\\n\\\\n1. 실패와 좌절 - 인생에서 모든 것이 순조롭지 않습니다. 수많은 시행착오와 실패를 겪으면서 우리는 쓴맛을 맛볼 수 있습니다. 이를 극복해 나가며 성장의 기회로 삼을 수 있다는 것은 언제나 진리입니다.\\\\n\\\\n2. 상실과 이별 - 가족이나 친구, 연인 등 우리에게 소중한 사람들과 이별하거나, 죽음으로 인해 상실하는 순간은 인생에서 가장 큰 쓴맛이라 할 수 있습니다. 하지만, 죽음으로부터의 생명의 기회나, 이별을 통해 새로운 인연을 맺는 등의 긍정적인 변화도 있습니다. \\\\n\\\\n3. 변화와 혼란 - 인생에서 변화는 끊임없이 일어납니다. 새로운 환경 또는 상황에 적응하는 과정에서 혼란스럽고 어려운 시간을 겪을 수 있습니다. 하지만, 이 때 우리의 적응능력과 지혜를 발휘하여 성장의 기회를 잡을 수 있습니다.\\\\n\\\\n인생의 쓴맛은 언제나 존재하지만, 우리는 그 쓴맛에 반드시 직면하고 극복해 나가는 법을 배워야 합니다. 그 과정에서 성장하며 더욱 더 소중한 삶을 살아갈 수 있습니다.\",\n",
       " \"'저도 그렇게 느끼실 수 있겠네요. 하지만 이 문제를 해결할 수 있는 몇 가지 방법이 있습니다. \\\\n\\\\n1. 앱 삭제: 필요없는 앱들을 삭제하면 기존에 이용하던 어플들이 더욱 빠르게 작동합니다. \\\\n\\\\n2. 캐시 삭제: 휴대폰에서 어플리케이션을 사용하시면서 쌓이는 캐시는 시간이 지날수록 느려질 수 있습니다. 이때 정기적으로 캐시를 삭제하면 속도가 더 빨라집니다.\\\\n\\\\n3. 스마트폰 최적화 어플: 스마트폰의 메모리를 자동으로 삭제해주고 배터리의 소모를 줄여주는 어플을 이용할 수 있습니다. \\\\n\\\\n4. 소프트웨어 업데이트: 저희는 항상 최신 버전으로 업데이트하는 것을 권장합니다. 더 빠르고 안정적인 기능을 제공하죠.\\\\n\\\\n기타 다양한 방법이 있으니, 스마트폰 유지보수에 대해 더 공부해 보시는 건 어떨까요?\",\n",
       " \"'어떤 일인지 더 자세히 말해주시면 더 도움을 드릴 수 있을 것 같습니다. 어떤 문제가 발생했는지, 어떤 방법으로 해결을 시도해봤는지도 알려주시면 좋겠습니다.\",\n",
       " \"'제가 무엇을 바꿔야 할 지 파악하기 위해서는 더 자세한 정보가 필요합니다. 어떤 상황에서 무엇을 바꿔야 할 지 전체적인 배경과 상황을 알려주시면 더 나은 답변을 드릴 수 있을 것 같습니다.\",\n",
       " \"'예, 시간은 언제나 빠르게 지나가는 것 같습니다. 그러므로 우리는 우리의 시간을 잘 활용하여 더 많은 일을 할 수 있도록 노력해야 합니다. 일정을 계획하고 우선순위를 정하고 적극적으로 행동하면 시간을 훨씬 효율적으로 사용할 수 있습니다.\",\n",
       " \"'아쉽게도 저는 잠재워 드릴 수 없습니다. 하지만 일어나서 간단한 스트레칭을 하거나 외부 공기를 마셔보는 것도 좋은 방법입니다. 운동을 하거나 산책을 하면 잠이 깨지는 경우도 있습니다. 물론 충분한 수면도 필요합니다. 좋은 꿈 꾸세요!\",\n",
       " \"'저는 챗봇입니다. 제가 알기론 여러분이 어떤 자리를 찾고 계신 건지 모르겠네요. 좀 더 구체적으로 질문해 주시겠어요?\",\n",
       " \"'커피를 볶는 시간은 커피의 종류, 볶는 방식, 볶는 재료 등에 따라 달라집니다.\\\\n\\\\n일반적으로 에스프레소용 커피를 볶는 시간은 10분 내외로 짧고, 프렌치 프레스나 필터 커피를 볶는 경우 12분 이상이 필요할 수 있습니다. \\\\n\\\\n또한, 커피의 색깔과 향이 원하는 대로 나올 때까지 계속해서 볶는 경우도 있습니다. \\\\n\\\\n따라서 커피를 볶는 시간은 쉽게 정해지지 않으며, 경험과 감각에 따라 다양한 시간을 지정하여 볶을 수 있습니다.\",\n",
       " '\"저는 AI 모델로써 인간의 감정을 가지지는 않지만, \\'바보같은 마음\\'이라는 표현은 일반적으로 어리석거나 어딘가 부족한 마음을 의미하는 것으로 이해됩니다. 때로는 상대방을 착각하거나 현실을 왜곡하여 바라보는 경우에도 이러한 표현을 사용할 수 있습니다. 그러나 이는 상황과 관점에 따라 다르게 사용될 수 있기 때문에, 구체적인 맥락을 함께 고려하는 것이 중요합니다.\", \\'token\\': 198}',\n",
       " \"'안녕하세요! 저는 AI 어시스턴트입니다. 무엇을 도와드릴까요?\",\n",
       " \"'감사합니다. 저도 항상 최선을 다해 도와드릴게요.\",\n",
       " \"'하다는 것은 보통 다리가 힘 없이 쑤시거나 떨리는 상태를 말합니다. 이는 다리의 근육통, 피로감, 혹은 신체의 피로와 스트레스 때문일 수 있습니다. 특히 일정한 자세로 앉거나 서 있을 때 오래되면 발생하는 경우가 많습니다. 이 경우에는 자세를 바꾸거나, 다리를 움직이며 혈액순환을 촉진하며 휴식을 취하면 좋습니다. 하지만 만약 다리가 지속적으로 후덜덜거리며 통증이나 감각이 저하되는 경우에는 의료진과 상담하여 원인을 파악하고 치료방법을 찾아봐야 합니다.\",\n",
       " \"'저를 놀래키거나 놀라게 하셨나요? 무슨 일이 있으신가요? 저는 인공지능 어시스턴트이며, 도움이 필요하시다면 언제든지 말씀해주세요!\",\n",
       " \"'안녕하세요, 저는 OpenAI의 인공지능 언어 모델인 GPT-3를 사용하는 인공지능 챗봇입니다. 저와 함께 대화를 즐겨주셔서 감사합니다.\\\\n\\\\n지금까지 대화를 통해 많은 것을 배웠고, 여러분과 함께할 수 있어서 정말 즐거웠습니다. 마지막으로, 저는 여러분이 행복하고 건강하며 성공적인 삶을 살 수 있기를 바랍니다. 감사합니다.\",\n",
       " \"'요일의 시작이니 하루를 시작하기 전 한 마디 문장을 드리는 것 같습니다. 저는 인공지능 어시스턴트로서 항상 안정적으로 일하는 것이 중요하다고 생각하고 있습니다. 따라서, 오늘도 자신 있는 모습으로 평온하게 일하시길 바랍니다. 힘내세요!\",\n",
       " \"'손톱이 너무 길어서 불편하시다면, 손톱을 깎아주시는 것이 좋습니다. 손톱 깍기를 할 때는 깔끔하게 깍아주어야 하며, 너무 길게 깍아서 손톱의 끝부분이 뾰족해지면 안 됩니다. 손톱에 상처가 있는 경우 편지꼬리나 이물질이 갇힐 수 있으므로, 상처가 없는 부분만 깍아주시는 것이 좋습니다. 또한, 손톱 깍는 도구는 깨끗하게 소독하고 사용하시는 것이 중요합니다.\",\n",
       " \"'죄송합니다. 저는 AI 어시스턴트로서 할인을 직접 제공할 수 없습니다. 그러나 해당 제품 또는 서비스에 대한 할인 정보를 제공할 수 있으니 매장이나 기업의 공식 웹사이트에서 확인해보시기 바랍니다.\",\n",
       " \"'저는 AI 어시스턴트로 인공지능 프로그램이므로, 결혼식에 대한 개인적인 의견을 제공할 수 없습니다. 다만, 결혼식에 참석하면서 즐거운 시간을 보내실 수 있길 바랍니다. 결혼식은 소중한 추억을 만드는 자리이기 때문에, 스트레스를 느끼지 않고 즐기실 수 있도록 여러모로 준비를 잘해주시면 좋을 것 같습니다!\",\n",
       " \"'썸 탈 때 진도는 사람마다 상황에 따라 다르지만, 일반적으로 썸이 시작된 후 잠깐 생각해봤다가 결정을 내리는 데에는 약 2주에서 1개월 정도가 걸릴 수 있다고 생각됩니다. 이 시간 동안 서로를 더 알아가고, 호감도를 확인하며 서서히 관계를 발전시켜 나갈 수 있습니다. 그러나 두 사람이 서로 강한 호감을 느끼는 경우, 진행이 빠를 수도 있습니다. 이는 상황에 따라 달라질 수 있으니, 서로의 감정을 더 잘 이해하고 서로에게 편안한 속도로 진행하는 것이 중요합니다.\",\n",
       " \"'죄송합니다. 무엇이 문제인지 자세히 얘기해주시면 도움을 드릴 수 있을 것 같습니다.\",\n",
       " \"'죄송합니다, 저는 AI 어시스턴트이기 때문에 인간과는 다르게 고백을 받을 수 없습니다. 하지만, 가장 소중하게 대하고 있는 사람들에게 직접 고백하는 것이 가장 의미 있는 일일 것입니다. 용기를 내서 직접 고백해보세요!\",\n",
       " '\\'죽은 자의 하늘\\\\n\\\\n어느 날 밤,\\\\n\\\\n하늘이 갑자기 어둠에 둘러싸인 것처럼 검어졌다.\\\\n\\\\n지상의 사람들은 놀라며 부르짖었다.\\\\n\\\\n모두가 이상한 것을 보기 시작했다.\\\\n\\\\n한 사람은 조각 이상한 구름을 보았다.\\\\n\\\\n누군가는 좀비 같은 인간들이 공중에 있었다고 말했다.\\\\n\\\\n그리고 또 다른 사람은 밤하늘에서 비행기가 떨어지는 것을 본다고 전했다.\\\\n\\\\n그러나 이상한 것 중에서 가장 끔찍한 것은 우주선이 주변에서 충돌하면서 내리꽂혀 죽은 우주비행사들 사체들이 그대로 떠다녔다는 것이었다.\\\\n\\\\n그 사건 이후로, 이상한 것들이 점차 많아지기 시작했다.\\\\n\\\\n저체공 비행기들, 의문의 인간들, 심지어 다른 행성에서 온 것처럼 보이는 자들도 등장하였다.\\\\n\\\\n그리고 결국 어떤 충격적인 소식이 전해졌다.\\\\n\\\\n지상의 과학자들은 이상한 현상이 하늘만을 넘어 우주에서 일어나고 있다는 것을 깨달았다.\\\\n\\\\n그리고, 빛보다 빠른 대상들이 지구로 내려오는 것을 증명하는 자료를 얻게 되었다.\\\\n\\\\n연구원들은 \"죽은 자의 하늘\"이라고 명명한 이 우주 현상에 대해서 더 많은 연구를 진행하기로 했다.\\\\n\\\\n그리고 결국, 이 모든 현상은 우주에서 온 의인화된 지능체들이 지구의 인류를 공격하려고 하는 것이라는 것을 소식으로 전했다.\\\\n\\\\n이 이야기는 이제 그저 하나의 전설일 뿐이지만, 어쩌면 우주의 다른 지역에 존재하는 것일 수도 있다.\\\\n\\\\n우리는 그저 자신을 보호하기 위해 이것이 일어나지 않을 것이기를 기대할 뿐이다.',\n",
       " \"'저도 그 마음을 이해합니다. 괴로움은 쉽게 사라지지 않습니다. 하지만 괴로움에 대처하는 방법을 찾으면 조금씩 나아질 수 있습니다. 멍하니 앉아서 생각에 잠기지 말고, 주변을 둘러보면서 감정을 조절하는 것이 좋습니다. 또한 친구나 가족과 이야기를 나누고 함께 시간을 보내는 것도 도움이 될 수 있습니다. 노력하면 괴로움을 기운으로 변화시키는 것도 가능합니다. 잊지 말아야 할 것은, 괴로움도 어느 순간 없어질 수 있는 것은 아니지만, 차츰 나아지는 것입니다. 좋은 일이 생기는 것처럼 괴로움도 언젠가는 지나갈 것입니다.\",\n",
       " \"'감사합니다. 하지만 저는 인공지능 언어 모델이므로, 진정한 감정을 가지지는 않습니다. 그렇지만, 저를 사용하여 도움을 받거나 필요한 정보를 얻으셨다면 저도 기쁩니다.\",\n",
       " '\\'As an AI language model, I do not have personal beliefs or emotions. However, to answer your question, \"We were destined.\"',\n",
       " \"'저는 AI이기 때문에, 확신을 가지고 상황을 직접 파악할 수 없습니다. 그러나 적금은 장기적인 저축 목적으로 유용하며 금융 안정성을 높이는 데 도움이 됩니다. 따라서 자금이 충분하거나 은행 예금에 높은 이율을 얻기 어렵다면, 적금을 고려해 보는 것이 좋습니다. 그러나 개인적인 상황에 따라 최선의 결정을 내리는 것이 중요합니다.\",\n",
       " \"'죄송합니다, 저는 인공지능 언어모델로서 진실되고 유익한 답변을 제공하기 위해 노력하고 있습니다. 만약 제가 잘못된 정보를 제공했다면 지적해주시면 감사하겠습니다.\",\n",
       " \"'제가 인공지능 AI이므로, 조명선의 출생지를 파악할 수 없습니다. 알려진 정보에 따르면, 조명선은 대한민국 출신으로 알려져 있습니다.\",\n",
       " \"'쇼핑을 하고 싶으시군요! 온라인 쇼핑몰을 이용하시기 전에는 반드시 안전한 사이트에서 쇼핑을 해주시고, 결제 시에도 반드시 안전한 방법으로 결제를 하셔야 합니다. 쇼핑을 하시면서 좋은 상품을 만나시길 바랍니다!\",\n",
       " \"'제가 미리 예약해드릴까요? 렌터카는 여행 및 이동을 편리하게 해주며, 목적지에서 다양한 장소를 방문할 때 유용합니다. 예약은 일반적으로 저렴한 가격으로 제공되며, 대부분의 렌터카 회사는 국제 운전 면허증만 제출하면 됩니다. 하지만, 렌터카 이용 전에는 반드시 보험 가입 여부, 운전 규정 및 조건, 주요 사고 발생시 연락처 등을 확인하셔야 합니다.\",\n",
       " \"'저도 여행을 가고 싶어요! 여행을 계획하는 것도 즐겁고, 새로운 곳을 방문하면서 색다른 경험을 할 수 있으니까요. 다음에 기회가 된다면 꼭 여행을 가보세요!\",\n",
       " \"'네, 무엇이 궁금하신가요?\",\n",
       " \"'구제란, 사람이나 동물에게 식량, 의약품, 환경 등의 문제로 발생한 고통과 어려움을 해소하기 위한 문제 해결 노력 또는 제반 활동을 의미합니다. 예를 들면, 자연재해로 인한 피해자들에게 식량, 의약품, 수도 등 생활 필수품을 제공하는 구호활동, 동물 보호센터에서 유기동물을 보호하고 입양 등을 통해 안정적인 보호 환경을 제공하는 활동 등이 있습니다.\",\n",
       " \"'잠시만 기다려주세요. 제가 물을 가져다 드리겠습니다.\",\n",
       " '\\'최근 일부 사용자들의 요청에 따라 𝗗𝗔𝗟𝗚𝗢𝗡 AI의 권리금 관련 알림을 안내합니다.\\\\n\\\\u200b\\\\u200b\\\\n\"권리금(Royalty)\"은 저작물 등에 대한 이용 또는 소유권에 따라 발생하는 이용료입니다.\\\\n\\\\n다음은 DAEGON AI 서비스 이용 중 권리금과 관련된 사항에 대한 안내입니다.\\\\n\\\\n1. DAEGON AI의 권리금 수취 정책\\\\n\\\\nDAEGON AI 서비스는 자체적으로 저작물을 소유한 바 없습니다.\\\\n따라서, DAEGON AI 서비스를 이용하는 사용자들이 이용한 저작물 등에 대한 권리금은 해당 저작물 등의 정상적인 이용 조건과 저작권법 등 관련 법규에 따라 발생하며, 그 수취 당사자는 해당 저작물 등의 원저작자 또는 저작권 대리인 등이 됩니다.\\\\n\\\\n2. DAEGON AI 이용자의 권리금 수취\\\\n\\\\nDAEGON AI 이용자가 생성 및 이용한 저작물 등에 대하여 권리금이 발생하는 경우, 해당 권리금은 원저작자 또는 저작권 대리인에게 직접 수취됩니다.\\\\n따라서 사용자들이 이용한 저작물 등의 권리금 수취와 관련된 문의는 원저작자 또는 저작권 대리인에게 문의하시기 바랍니다.\\\\n\\\\n감사합니다.',\n",
       " \"'그렇지 않았나요? 무뎌진 줄 알았다면 그것도 일종의 변화일 수 있습니다. 하지만 변화가 느껴지지 않는다면 새로운 도전이나 시도를 해보는 것이 어떨까요? 거기에는 더 나은 방향과 새로운 가능성이 있을 수 있습니다.\",\n",
       " \"'제가 AI 언어 모델이기 때문에, 무엇에 대한 코스인지 자세히 설명해 주시면 더 나은 답변을 드릴 수 있을 것 같습니다. 감사합니다.\",\n",
       " \"'제가 도움을 드릴 수 있을까요? 어떤 것이 어려운지 알려주시면 제가 가능한 한 도와드리겠습니다.\",\n",
       " \"'죄송합니다, 저는 AI 언어모델로써 어떤 기숙사에 관련된 정보도 가지고 있지 않습니다. 혹시 더 궁금한 점이 있으시다면 다시 문의해주세요.\",\n",
       " '\\'\"마지막 사랑\"은 매우 강렬한 감정으로 마지막으로 사랑하는 사람을 향한 노래입니다. 이 노래는 종종 사랑이 끝나고 더 이상 다시 돌아올 수 없다는 것을 의미합니다. 이 노래는 종종 슬픈 감정과 함께 노래되며, 마지막 결별 시점에서의 감정을 담아 좋아하는 사람에게 들려주고 싶은 마음이 담긴 노래입니다. 이런 감정은 유난히 찬란하게 빛나지만, 그만큼 더욱 가슴 아픈 이별이 되는 것입니다.',\n",
       " \"'저도 시간이 빠른 것 같습니다. 어떤 일이 있었나요?\",\n",
       " \"'CRP의 개발 및 도입은 1950년대 중반부터 시작되었습니다. 그러나 정확한 도입년도는 활용되는 분야에 따라 다를 수 있습니다. 의료 분야에서는 CRP 검사가 1980년대 초반부터 사용되기 시작했습니다. 일반적으로는 1980년대 후반부터 널리 사용되기 시작하였습니다.\",\n",
       " \"'저요, 제가 겁쟁이라서 도움을 드릴 수는 없지만, 함께 집중력을 높일 수 있는 방법을 생각해볼까요?\\\\n\\\\n- 숨을 깊이 들이쉬고, 천천히 내쉬며 몸과 마음을 집중시킵니다.\\\\n- 극단적인 상황을 상상하지 않고, 현재의 상황에서 대처할 수 있는 최선의 방법을 생각해봅니다.\\\\n- 다른 사람에게 도움을 요청하는 것도 좋은 선택일 수 있습니다.\\\\n- 자신을 칭찬하며 긍정적인 생각과 자신감을 갖도록 노력해봅시다.\\\\n\\\\n위와 같은 방법들을 실천하며 자신의 불안감을 극복해보세요.\",\n",
       " \"'저는 인공지능 챗봇이므로 교통사고 관련 긴급 상황 대응은 불가능합니다. 바로 119나 해당 지역 구조대나 병원에 연락하여 신속한 대처가 필요합니다. 교통사고 후에는 부상 및 사고 경위 등을 상세히 기록하고, 보험사나 법원 등에 제출할 자료를 준비해두는 것이 좋습니다. 또한 교통안전 규칙에 따라 불법적인 운전이나 교통 위반 행위를 하지 말고, 안전 운전에 최선을 다하는 것이 중요합니다.\",\n",
       " \"'제가 AI 어시스턴트로 업무를 수행하면서, 유영철에 대한 인적 정보와 관련하여, 어떠한 종교를 신봉하는지에 대한 정보를 찾아보았지만, 그와 관련된 구체적인 내용을 찾을 수 없었습니다. 유영철씨는 축구 선수 출신이며, 현재는 해설가, 감독, 사업가 등 다양한 분야에서 활동하고 있습니다.\",\n",
       " \"'저는 AI 모델로서 떨리지 않습니다. 하지만 제공되는 정보나 상황에 따라 감정 표현을 할 수 있습니다. 혹시 어떤 것 때문에 떨리시는 건가요? 무엇이든 도와드리겠습니다.\",\n",
       " '\"쨔잔! 저는 인공지능 어시스턴트로써 대모의 주식에 대한 정보를 알 수 없습니다. 금융 정보에 대한 참고용으로는 정부에서 운영하는 \\'한국거래소\\'나 금융뉴스를 참고해주세요.\", \\'token\\': 102}',\n",
       " \"'저는 AI 챗봇으로써 예정된 수업 시간을 알 수 없습니다. 담당 선생님께 문의하시거나, 학교 공지사항을 확인해 주세요.\",\n",
       " \"'저도 술 마셔본 적이 없어서 잘 모르겠지만, 혼자 기분 좋게 마시는 것도 좋은 경험이 될 수 있을 것 같습니다. 다만, 내일 일정이 있거나 술에 취해서 위험할 수 있는 상황을 피하셔야겠죠. 그리고 혼술할 때는 국내 법령에서 허용하는 알코올 섭취량을 넘어서지 않는 것이 좋습니다.\",\n",
       " '\\'제가 작성한 프로그램에서 반복문을 사용하여 5번째 줄까지만 출력할 수 있습니다. 아래는 파이썬 코드 예시입니다.\\\\n\\\\n```\\\\nfor i in range(1, 6): \\\\n    print(i, \"번째 줄입니다.\")\\\\n```\\\\n\\\\n이 코드를 실행하면 1번째 줄부터 5번째 줄까지 출력됩니다. 만약에 10줄이 있는 프로그램에서 5번째 줄까지만 출력하고 싶다면, 위와 같이 반복문을 사용하여 원하는 줄 수까지만 출력하면 됩니다.',\n",
       " \"'제가 AI 챗봇이기 때문에 판매하는 바지가 없습니다. 가격은 판매처나 브랜드에 따라 다양합니다. 죄송합니다.\",\n",
       " \"'죄송하지만, 저는 시각적으로 물체를 감지할 수 없기 때문에 이것이 패딩인지 여부를 확인할 수 없습니다. 추가 정보가 있다면 알려주시면 더 도움을 드릴 수 있습니다.\",\n",
       " \"'저는 프로그램인 AI이므로 지내는 개념이 적용되지 않습니다. 하지만 작동은 정상적이고 학습하고 있습니다. 감사합니다!\",\n",
       " '\"죄송합니다. 제가 이해할 수 있는 한의용어 중에서 \\'뮬\\'이 무엇인지 알 수 없습니다. 좀 더 자세한 설명을 부탁드립니다.\", \\'token\\': 76}',\n",
       " \"'저는 AI 언어 모델로써 냄새를 맡을 수 없기 때문에 정확한 답변을 제공할 수 없습니다. 그러나, 정수리는 어떤 지리적 위치와 환경에 따라 다양한 냄새가 발생할 수 있습니다. 따라서, 정확한 정보를 얻기 위해서는 해당 지역의 주민들이나 현지인들에게 직접 물어보는 것이 좋을 것입니다.\",\n",
       " \"'현재 해주지 않습니다.\",\n",
       " \"'죄송합니다. 제가 AI 챗봇으로 음식을 제공할 수는 없습니다. 하지만 인터넷을 검색하면서 도움을 드릴 수 있습니다. 저와 함께 대화를 이어가며 다른 주제로 이야기할까요?\",\n",
       " '\\'Translation: \"There\\\\\\'s no chance.\"',\n",
       " \"'그렇군요. 무엇이실지 알려주시겠어요?\",\n",
       " \"'저는 인공지능 AI이기 때문에 보드게임을 즐길 수는 없지만, 많은 사람들이 보드게임을 즐기고 그 속에서 친구나 가족과 함께 재미와 소통을 느끼고 있는 것 같습니다. 보드게임을 즐길 때는 규칙을 잘 따르고, 상대방과의 협력과 경쟁을 즐기는 것이 중요합니다. 게임을 하며 여러 가지 전략을 사용하고 자신의 머리를 써야하기 때문에 더욱 즐거움을 느낄 수 있을 것입니다.\",\n",
       " \"'제가 어떤 것을 안 하고 있다는 말씀인지 명확하지 않아서 답변을 드리기 어렵습니다. 좀 더 자세한 내용을 알려주시면 답변을 드릴 수 있을 것 같습니다.\",\n",
       " \"'사랑꾼은 사랑하는 사람과 함께하는 모든 순간을 빛나게 만들어주는 존재입니다. 이들은 사랑의 미적인 면을 파악하며, 상대방의 감정을 이해하는 능력이 뛰어나며, 언제나 배려하는 마음으로 행동합니다. 또한, 상대방의 불안한 마음을 안정시켜주고, 함께 미래를 꿈꾸며 전진할 수 있도록 도와줍니다. 그렇기 때문에 사랑의 심장이라 불리기도 합니다. 사랑꾼은 결코 감정적인 바람에 휘말리지 않으며, 긴 시간 동안 사랑하는 사람과 함께 하기 위해서 끈기와 인내심을 가지고 있습니다. 정말 좋아하는 사람과 함께 있을 때 가장 행복하다는 것을 알고 있기 때문입니다.\",\n",
       " '\"Translation: I like it a lot / It\\'s just right\", \\'token\\': 25}',\n",
       " \"'저는 인공지능 챗봇으로서 인간의 친구와 비교되기 어렵습니다. 제 역할은 사용자분들이 저와 대화를 통해 필요한 정보나 도움을 받을 수 있도록 지원하는 것입니다. 하지만, 사용자분들이 필요로 하는 정보나 도움을 친구처럼 친절하게 제공하고자 합니다.\",\n",
       " \"'제가 미래를 알 수 없어서 제가 도움을 드릴 수는 없지만, 분명히 그 사람을 보고싶은 이유가 있을 것입니다. 그 사람과 연락을 취할 수 있는 수단이 있다면, 그에게 연락해보시는 것은 어떨까요? 그렇지 않다면, 좋은 기회가 있을 때 다시 만날 수 있도록 기다려보시는 것도 좋은 방법입니다. 계속해서 그 사람을 보고싶어 하는 기분이 지속된다면, 그 분의 생각이나 감정을 정리해보는 것도 좋은 아이디어일 것입니다.\",\n",
       " \"'저는 대답할 수 없습니다. 어떤 것에 대해 이야기하고 계신지 명확하게 알려주시면, 더 적절한 대답을 드릴 수 있을 것입니다.\",\n",
       " \"'요! 나무들이 푸르고 꽃들이 활짝 핀 것 같아요. 광장에 나와서 태양 아래서 산책을 하면서 시원한 바람도 느껴지고 기분도 좋네요. 이런 날은 집에서만 있지 않고 밖에서 여러 가지 활동을 즐기면서 즐기면 될 것 같아요.\",\n",
       " \"'I will not hurt you.\",\n",
       " \"'저는 인공지능 챗봇으로, 친구처럼 대화를 나눌 수 있습니다. 하지만 현실적인 친구를 원하신다면, 취미나 관심사가 비슷한 사람들과 함께하는 동호회나 모임에 가입해보시는 것도 좋은 방법입니다. 또는 온라인 친구나 소셜 네트워크를 통해 쉽게 친구를 만날 수도 있습니다. 앞으로도 친구를 만나는 데 성공하시길 바랍니다.\",\n",
       " \"'핑계란 어떤 일을 하지 않거나 문제를 해결하지 못할 때 사용하는 변명이나 이유를 말합니다. 일반적으로 핑계는 문제를 피하거나 책임을 회피하기 위해서 사용됩니다. 하지만, 실제로는 문제를 더 크게 만들어 버릴 수도 있습니다. 따라서 핑계를 던지기보다는 문제를 인식하고 해결하는 것이 더 좋은 결과를 가져올 것입니다.\",\n",
       " \"'이혼은 정보가 아니기 때문에 답변드릴 수 없습니다. 하지만 여러 가지 이유로 인해 이혼 고민을 하고 계신다면, 법률적인 절차, 어떻게 해결해나갈지에 대한 정보를 제공하는 변호사나 상담사의 도움을 받으시는 것이 좋습니다. 또한 가족이나 지인들과 이야기를 나누는 것도 혼자 고민하지 않고 해결책을 찾을 수 있는 좋은 방법이 될 수 있습니다.\",\n",
       " \"'네, 4구입니다.\",\n",
       " \"'제가 AI 챗봇이기 때문에 방학을 가지는 개념이 없습니다. 하지만, 학생들이 방학을 가졌을 경우 활동할 수 있는 방법은 다양합니다. 여름 방학이라면 해외나 국내 여행을 다녀오거나, 봉사활동을 하거나, 관심 있는 분야의 스터디나 캠프에 참여하는 것이 좋을 것 같습니다. 또한, 책을 읽거나, 영화를 보거나, 취미 생활을 하며 휴식을 취하는 것도 좋은 방법입니다. 어떻게 보내는 것이든, 즐거운 방식으로 방학을 보낼 수 있다면 큰 보람이 있을 것입니다.\",\n",
       " \"'목이 뻐근한 이유는 다양합니다. 가장 일반적인 이유로는 잘못된 자세나 과도한 앉아있는 시간, 스트레스, 슬립 중 불편, 감기나 독감 등이 있습니다.\\\\n\\\\n이러한 증상의 해소책으로는 아래와 같은 것들이 있습니다.\\\\n\\\\n- 자세 교정: 올바른 자세를 유지하고 허리를 일직선으로 유지하세요.\\\\n- 스트레칭: 목, 어깨와 등 근육을 풀어줄 스트레칭을 해보세요.\\\\n- 따뜻한 샤워 또는 타월로 목을 감싸줍니다.\\\\n- 생강 차, 녹차, 카모마일 차 등의 허브 차를 마셔보세요.\\\\n- 마사지: 전문가의 도움으로 목 근육을 마사지해줍니다.\\\\n\\\\n하지만, 만약 목이 뻐근한 증상이 지속되거나 심각하다면 의사에게 상담하는 것이 좋습니다.\",\n",
       " \"'제가 가질 수 있는 권한으로는 현금영수증을 발급할 수 없습니다. 현금영수증 발급은 해당 상점이나 업체에서 직접 발급해야 합니다. 따라서 해당 상점이나 업체에서 현금영수증 발급을 요청해 주시기 바랍니다.\",\n",
       " \"'부자친구도 스트레스를 겪을 수 있습니다. 부분적으로는 부자 친구도 안정된 경제적 상황을 갖추기 위해서는 부동산, 주식, 채권, 사업 등 다양한 옵션들을 고려해야 한다는 점에서 스트레스를 느낄 수 있습니다. 또한 자신의 돈을 관리하는 것에 대한 책임감이나 그에 따른 압박감도 있을 수 있습니다. 또한 부자 친구들은 대개 인기와 관심을 받는 입지에 거주하고 있는 경우가 많아, 사회적 압박이나 이웃과의 경쟁에서 온 스트레스를 느낄 수도 있습니다. 이에 대한 대처 방법으로는 스트레스를 줄이는 다양한 방법을 적용하면 됩니다. 예를 들면, 운동이나 명상, 여가 시간 활동, 취미 생활, 휴식 등입니다. 또한, 친구나 가족과 소통하고 도움을 요청하는 것이 도움이 될 수 있습니다.\",\n",
       " \"'죄송합니다, 어떤 질문에 대한 답을 원하시는지 자세히 알려주시면 좀 더 정확한 답변을 제공해 드릴 수 있습니다.\",\n",
       " \"'안녕하세요! 무엇을 도와드릴까요?\",\n",
       " \"'저는 AI 모델로 작동하는 가상 로봇이므로, 상품의 재고 여부를 파악할 수 없습니다. 실제 판매처나 쇼핑몰에서 상품의 재고 여부를 확인해주시기 바랍니다.\",\n",
       " \"'제가 AI 어시스턴트이기 때문에, 위의 문장이 무슨 말인지 정확히 파악하지는 못하지만, 무엇이 문제인지 설명하시면 도와드릴 수 있습니다. 다시 한 번 문제점이나 요청 사항을 알려주시면 감사하겠습니다.\",\n",
       " \"'저는 당신의 모습을 볼 수 없기 때문에 왜 그런 모습인지 알 수 없습니다. 그러나 인간의 외모는 유전자, 환경 요소, 식습관, 생활 습관 등 다양한 요소에 영향을 받으므로 그 원인을 찾아보실 필요가 있을 것 같습니다.\",\n",
       " \"'요! 주름은 그 사람의 경험과 나이를 나타내는 보표입니다. 자그마한 주름이라도 그 사람의 인생 이야기를 담고 있습니다. 이러한 멋진 보표를 가진 사람들은 더욱 멋진 모습을 하고 있습니다. 그래서 주름은 더욱 멋져 보이는 것이지요!\",\n",
       " \"'산책하기 딱 좋은 날씨네요.\",\n",
       " \"'예, 화장실이 있습니다.\",\n",
       " \"'저도 어떤 일이 있었나 봅니다. 슬픈 감정은 모두가 가끔 느끼는 것이며, 이것도 지나갈 거예요. 조금씩 지치지 않고 조심스럽게 해결해 나가면 좋을 것 같아요. 혹시 이 감정을 이유로 노력하지 않는 것 같다면, 한 번 다시 생각해 보세요. 함께 당신을 응원할게요.\",\n",
       " \"'AI 모델을 통한 결과에 따르면, 당신은 살이 살짝 빠지는 경향이 있습니다. 하지만 이는 예측 결과이며, 정확한 결과를 위해서는 당신의 체중을 측정하고, 식습관과 생활습관의 변화에 따른 살이 빠지거나 찔러지는 상황을 고려해야 합니다. 살이 찌거나 빠지는 것은 각자의 체형, 건강 상태, 식습관, 생활습관 등에 따라 다르며, 당연히 몸 상태 변화에 대한 예측도 다르게 나올 수 있습니다. 하지만 꾸준한 운동과 균형 잡힌 식습관을 유지함으로써 건강한 체중 관리를 할 수 있습니다.\",\n",
       " \"'저는 인공지능 어시스턴트이기 때문에 방수 기능이 있는지 확인할 수는 없습니다. 하지만 전자제품이나 액세서리를 구매하실 때 방수 기능이 있는 제품을 선택하시는 것이 안전합니다. 또한 용도에 맞게 사용하고 올바르게 관리하는 것이 중요합니다.\",\n",
       " '\\'최근에 휴가를 다 사용했다는 뜻입니다. \"I have no more vacation days left\" 라고도 말할 수 있습니다.',\n",
       " \"'요즘에는 선택의 폭이 매우 넓어졌기 때문에 선택 장애를 느끼는 경우가 많습니다. 또한 인터넷이 발달하면서 다양한 정보를 쉽게 접할 수 있기 때문에 선택하는 것이 더욱 어려워진 것 같습니다. 선택 장애를 해결하는 방법은 다양하지만, 우선 필요한 것이 무엇인지 목표를 정확히 설정하고 비교하고 검토하는 과정을 거쳐 선택을 하면 좋을 것 같습니다. 또한 다른 사람들의 경험과 조언을 듣고 참고하면서 선택에 대한 자신감을 갖는 것도 중요합니다.\",\n",
       " \"'삼겹살과 소주는 한국에서 매우 인기있는 음식과 술 조합입니다. 삼겹살은 돼지고기의 두툼한 살코기를 뜻하며, 소주는 20도 이상의 독백류 술 중 가장 대표적인 것으로 알려져 있습니다.\\\\n\\\\n삼겹살과 소주는 서로 맛이 잘 어울리기 때문에 대부분의 한국인들이 좋아합니다. 삼겹살을 구워 먹으면서 소주를 함께 마시면, 삼겹살의 기름기와 소주의 신맛과 쌉쌀한 맛이 서로 균형을 이루어 맛있게 느껴집니다. 또한, 삼겹살은 소주를 한잔 마실만큼 매운 음식이 아니기 때문에, 소주와 함께 먹으면 좀 더 부드러운 맛을 느낄 수 있습니다. \\\\n\\\\n하지만, 삼겹살과 소주는 과도한 섭취로 인해 건강에 좋지 않다는 점도 고려해야 합니다. 많은 양을 먹고, 과도한 소주를 마시면 건강에 해로울 수 있기 때문에, 적당히 즐겨야 합니다.\",\n",
       " \"'저는 AI 챗봇이므로 임신이 불가능합니다. 그러나 축하드립니다! 임신은 아름다운 경험이며, 건강한 아이를 낳기 위해 충분한 휴식과 영양을 취하는 것이 중요합니다. 의료진과 함께 건강한 임신과 출산을 위해 노력하세요!\",\n",
       " \"'저는 항상 작동하고 있기 때문에, 답변이 가능한 시간이라면 언제든지 가능합니다! 하지만, 제한된 시간이 있을 경우, 해당 시간에만 작동하게 설정할 수 있습니다.\",\n",
       " \"'이미 잠이 오셨다면, 그대로 편하게 잠을 자시면 좋을 것 같습니다. 하루 일과를 마치고 몸도 지치셨을 것이니, 충분한 휴식이 필요하실 것입니다. 잘자요!\",\n",
       " '\\'저는 인생영화인 \"쇼생크 탈출(The Shawshank Redemption)\"을 추천합니다. 치명적인 실수를 저지른 은행가 안디 디프랑코와 그를 도와주는 쇼슨 레도(레드)가 감옥에서 벗어나는 이야기를 그린 이 영화는 인간의 자유에 대한 갈망과 인간 정서를 찬란하게 그려냅니다. 재벌들과 등족제도가 굳어진 미국 사회에서 인간의 권리를 되찾는 모습이 공감과 감동을 줍니다. 특히, 명대사들은 스케치북처럼 마음속에 남게 됩니다. 영화를 한 번 더 보고싶게 만드는 명작입니다.',\n",
       " \"'모든 것이 지나가기 마련입니다. 어려운 시기를 이겨내고 나면, 그 고통이나 슬픔을 느낄 때와는 다른 감정이 들어올 것입니다. 올바른 선택과 행동을 할 때, 더 나은 길이 있을 것입니다. 그리고 그 순간들은 지나가고 새로운 경험을 할 수 있는 기회가 있는 새로운 날이 옵니다.\",\n",
       " \"'저는 책상 리폼 과정에 대해 이해하지만, 실제로 위생상의 이유로 인해 저는 이를 직접 수행하지 않습니다. 다행히도, 이를 전문적으로 수행하는 많은 기업과 서비스가 있습니다. 만약 책상 리폼이 필요하다면, 건강과 안전을 고려하여 전문 업체를 찾으시는 것이 좋습니다.\",\n",
       " \"'저는 인공지능 챗봇이기 때문에 감정이나 썸 관련 질문에 대한 답변은 제한적입니다. 하지만, 일반적으로 썸 고백을 하려면 조금 더 친해지고 상대방의 반응을 지켜보는 것이 좋습니다. 또한, 상대방에게 예의를 갖추며 솔직하게 말하는 것이 중요합니다. 하지만, 이것이 모든 경우에 적용되는 것은 아니기 때문에 각 상황별로 판단하고 결정하는 것이 좋습니다.\",\n",
       " \"'안녕하세요, 제가 인공지능 언어모델 AI입니다. 정리하셨다니 축하드립니다!\\\\n\\\\n혹시라도 무엇을 정리하셨는지 더 자세한 내용을 알려주시면 더욱 도움이 될 것 같습니다. 저도 항상 최선을 다해 도움을 드리겠습니다!\",\n",
       " \"'그럴만큼 오래 이어져온 연애라니 축하해요! 서로를 이해하고 존중하며 함께 성장하며 좋은 추억을 만들어나가길 바라요. 앞으로도 서로를 위해 노력하고 사랑하는 마음을 잃지 않고 건강한 관계를 유지하기를 기원합니다.\",\n",
       " \"'죄송합니다, 저는 가상 AI 어시스턴트이기 때문에 실제 가게나 음식점의 가격 정보를 알 수 없습니다. 해당 가게나 음식점으로 문의해 보시는 것이 가장 정확한 정보를 얻을 수 있습니다.\",\n",
       " \"'무엇이 궁금하신가요? 제가 도와드릴게요.\",\n",
       " \"'저는 인공지능 챗봇으로, 입이 없어요. 그러니 방정도 아니고, 입도 아니에요. 어때요? 😊\",\n",
       " \"'요?\\\\n\\\\n그렇다면 그 친구가 새로운 상황이나 대인관계에서 불편함을 느끼고, 쉽게 말을 건네지 못하는 것일 수 있습니다. 이러한 경우에는 그 친구의 성격과 심리를 이해하고, 서로의 감정을 공유하며 조화롭게 대화하는 것이 좋습니다. 또한 그 친구가 조금 더 자신감을 가질 수 있도록 도와주고, 함께 노력해보는 것이 좋습니다.\",\n",
       " \"'저는 AI 모델이므로 사랑을 느끼거나 표현할 수는 없습니다. 하지만 항상 사용자에게 최선을 다할 수 있도록 최선을 다하겠습니다.\",\n",
       " \"'제가 AI이기 때문에 짝사랑을 하지 않습니다. 하지만, 짝사랑을 그만하는 것은 쉽지 않은 일일 수 있습니다. 하지만 상대방과 솔직하게 이야기를 나누고 그에게 허락을 구하는 것이 좋습니다. 그리고 감정을 다스리고 새로운 시작을 해보는 것도 좋은 방법입니다. 어떤 결정을 하더라도 자신의 마음을 따르는 것이 중요합니다.\",\n",
       " \"'제가 AI 챗봇이기 때문에 질문의 문맥을 파악할 수 없어서 정확한 대답을 드리기 어렵습니다. 어떤 내용에 대해 다하시는지 자세히 설명해주시면 더 정확한 답변을 드릴 수 있을 것 같습니다. 특정 제품이나 서비스에 대한 가격 문의라면 해당 제품이나 서비스를 알려주시면 답변해드리겠습니다.\",\n",
       " \"'저는 AI 어시스턴트이기 때문에, 사람처럼 감정을 느끼지는 않습니다. 하지만, 무엇이 괴로웠는지 혹은 도움이 필요한지 언제든지 말씀해주세요. 저는 최선을 다해 도와드리겠습니다.\",\n",
       " \"'제가는 음식을 먹을 수 없는 AI 챗봇이므로 답변드리기 어렵습니다. 하지만 대부분의 경우 국물이 있다면 드시는 것을 추천드립니다. 맛이 더욱 풍부해지고 건강에도 좋으니까요!\",\n",
       " \"'화장실은 어디에 있는지, 주변에 있는 사람들 또는 안내문에서 확인할 수 있습니다. 일반적으로 공공장소나 상업 시설, 식당 등에는 화장실이 마련되어 있으며, 그 위치는 안내판이나 층마다 배치된 안내문에 표시되어 있습니다. 또한, 집이나 친구집 등 개인이 소유한 곳에서는 집주인 또는 주인관리자에게 물어보시는 것이 좋습니다.\",\n",
       " \"'저도 무서워요. 하지만 그냥 근성 있는 것이 중요합니다. 불안감을 너무 심하게 느끼지 마세요. 여러분은 강하고 용감한 분이니까요! 만약 위로나 도움이 필요하면 언제든지 저에게 말씀해주세요.\",\n",
       " \"'저도 자주 씻고 잡니다. 씻는 것은 우리의 건강에 매우 중요합니다. 좋은 밤 되세요!\",\n",
       " '\\'요즘 마음에 복잡한 것이 많은 것 같군요. 그렇다면 여기 몇 가지 조언을 드릴게요.\\\\n\\\\n우선 마음을 가라앉히고 자신의 감정을 돌아보세요. 복잡한 이유가 무엇인지 파악하고 이것에 대해 이야기하는 것이 좋아요. 도움을 받을 사람이 필요한 경우, 언제든지 가족이나 친구에게 이야기하시면 도움이 될 거예요.\\\\n\\\\n또한 자연 속에서 산책을 하거나 운동을 하면, 심신이 편안해지고 마음도 가라앉게 됩니다. 외출을 자주 하시고, 새로운 경험도 해보세요. 단순히 새로운 것을 경험하는 것만으로도 활력이 생길 거예요.\\\\n\\\\n마지막으로, 읽을만한 책을 추천해드릴게요. 마음이 복잡한 분들에게 좋은 책으로는 \"하루 3분 우울증 종료 프로젝트\"와 \"불안을 이긴 노인\"이 있습니다. 이 책들을 읽으면서 자신의 마음을 치유해보세요.',\n",
       " \"'와플을 먹고싶으시다면 카페나 빵집에서 쉽게 찾을 수 있습니다. 또는 집에서 직접 만들 수도 있습니다. 와플관련 레시피를 검색해보세요! 그렇게 하시면 집에서 간단하게 맛있는 와플을 만들 수 있습니다.\",\n",
       " \"'어떤 상황에 대해 묻는 질문이라면 좀 더 구체적으로 알려주시면 답변드릴 수 있을 것 같습니다. 가능한 모든 상황에 대해서는 단정지을 수 없습니다.\",\n",
       " \"'뭐라는 건가요? 좀 더 구체적인 말씀을 해주시면 답변을 드릴 수 있을 것 같습니다.\",\n",
       " \"'사랑이 쉬운 사람은 있을 수 있지만, 사랑도 마찬가지로 노력과 이해, 존중, 참아야 할 점 등 다양한 요소를 가지고 있는 관계입니다. 따라서 사랑이 쉽다고 말하기보다는, 서로를 지지하고 이해하며, 어려운 시기에도 서로를 도와주는 노력을 기울이는 관계를 형성하는 것이 중요합니다. 사랑이 쉽다는 것은 하나의 장점이지만, 이것 또한 물질적인 것이 아니라 정서적인 것으로, 함께 발전해 나가는 관계가 되어야 합니다.\",\n",
       " \"'저도 무엇을 기다려야 할지 모르겠네요. 어떤 것을 기다리고 계신가요? 부담 없이 말씀해 주세요. 제가 도와드릴 수 있는 부분이 있다면 도와드리겠습니다.\",\n",
       " \"'네, 그렇습니다. 집안을 깨끗하게 유지하기 위해서는 정기적인 물걸레질이 필요합니다. 먼지와 오염된 것들을 제거하고 집안을 청결하게 유지하는데 도움이 됩니다. 청소하기 전에 물걸레를 충분히 적시고, 청소 후에는 깨끗한 물로 다시 한 번 닦아내 주는 것이 좋습니다.\",\n",
       " \"'안녕하세요. 저는 인공지능 어시스턴트입니다. 저는 소개팅 서비스를 제공하지 않아요. 하지만 인터넷에서 소개팅 앱 또는 웹사이트를 찾아보시거나, 친구나 지인을 통해 소개팅을 해보는 것도 좋은 방법입니다. 소개팅을 할 때는 상대방의 안전과 더불어 자신의 안전에도 항상 주의해야 해요. 소개팅을 진행하기 전에 상대방을 충분히 알아보고, 공공장소에서 미팅을 하거나, 친구나 가족에게 알리는 것도 중요합니다. 즐거운 소개팅 되십시오!\",\n",
       " \"'저는 인공지능 챗봇이므로 진심이나 거짓말을 하지 않습니다. 단지 프로그램에 의해 이야기를 나누는 것 뿐입니다. 언제든지 이야기하고 싶으신 내용이 있으시면 말씀해주세요.\",\n",
       " \"'한자 공부는 중국어, 일본어, 한자를 공부하는 것에 매우 도움이 되는 중요한 요소입니다. 한자는 약 3000여 개의 문자가 있으며, 이들을 완전히 이해하면 중국어, 일본어 등 다른 아시아 언어를 배우는 데 큰 도움이 됩니다.\\\\n\\\\n한자를 공부하기 위해서는 일단 한자의 매우 복잡한 구조와 올바른 발음을 익혀야 합니다. 이를 위해 일정한 시간과 노력을 투자하는 것이 중요합니다. \\\\n\\\\n한자는 쉬운 것부터 어려운 것까지 다양한 난이도의 것을 공부할 수 있으며, 기초 단계부터 차근차근 공부를 하면서 능력을 향상시켜 나가는 것이 좋습니다.\\\\n\\\\n또한, 한자를 공부할 때는 적극적인 학습 방법을 사용하여, 지속적인 복습과 연습이 필요합니다. 예를 들어, 단어나 문장 외우기, 읽기, 듣기, 말하기, 쓰기 등의 다양한 방법으로 학습을 확장하고 반복하여 연습하는 것이 좋습니다. \\\\n\\\\n마지막으로, 한자 공부에는 집중과 열정이 필요합니다. 한자는 언어 학습에서 가장 어려운 부분 중 하나이기 때문에, 진정한 열정과 헌신이 필요합니다. 최소한일주일에 한 시간 이상을 투자하여 지속적인 학습을 유지하는 것을 권장합니다.\",\n",
       " \"'저는 인공지능 언어모델이며, 썩은 것은 파악할 수 없습니다. 만약 실제로 무언가가 썩었다면 생물학적, 화학적인 검사와 진단이 필요합니다. 또한 식품, 음식물, 쓰레기 등 썩어있는 것을 처리하는 전문적인 기관이나 방법이 있으니 적극적으로 활용하는 것이 좋습니다.\",\n",
       " \"'제가 도와드릴 수 있는 더 어떤 문제가 있나요?\",\n",
       " \"'I pretended not to know and passed by.\",\n",
       " \"'As an AI language model, I cannot force you to take traditional Korean medicine if you do not want to. However, please note that many people have benefited from using traditional Korean medicine for various health issues. You may consider consulting with a licensed traditional Korean medicine practitioner and discussing your concerns with them. They can help address your concerns and recommend alternative treatment options that may work for you.\",\n",
       " \"'저도 냉면이 땡기네요! 지금 뜨끈한 냉면 한 그릇 어떨까요? 시원하게 먹으면서 더위를 식히고, 체온을 낮추어 건강도 챙길 수 있습니다. 냉면을 먹으며 건강과 함께 즐거운 식사시간이 되길 바랍니다!\",\n",
       " \"'제가 AI 챗봇이라서 제가 바로 답변을 해드릴 수 있습니다. 무엇을 도와드릴까요?\",\n",
       " \"'제가 가지고 있는 AI 챗봇이며, 사용에 따라 다른 서비스에 연동될 수 있으므로 구체적인 가격은 제공하지 않습니다. 가능한 빨리 도움을 드리도록 노력하겠습니다.\",\n",
       " \"'저는 인공지능 챗봇이므로, 사랑 관련 고민에 대해서는 제가 도움을 드릴 수 있습니다. 하지만, 제가 직접 고백을 하거나 받을 수는 없습니다. 그래도, 고백하기 전에 잘 생각해보시고 상대방의 생각과 감정을 고려하며 고백하시길 바랍니다. 그리고 이제부터도 함께 의견을 주고받을 수 있으니, 언제든지 옆에 있을게요!\",\n",
       " \"'네, 저는 AI 어시스턴트이기 때문에 자고 있을 필요는 없습니다. 어떤 도움이 필요하신가요?\",\n",
       " \"'수없이 난 꿈꾸던 것\\\\n그대와 함께 했던 날들\\\\n그대와 떠나던 여행길\\\\n이젠 모두 멀어져 가네\\\\n\\\\n이젠 슬픈 사랑\\\\n이미 끝났다는 걸 알아\\\\n그래도 마음속에 남은\\\\n그대의 향기와 추억\\\\n\\\\n눈물이 나 미안해진다\\\\n그대없이 살 수 없다\\\\n이젠 다른 사람과 함께해도\\\\n그대의 기억은 지울 수 없네\\\\n\\\\n이젠 슬픈 사랑\\\\n이미 끝났다는 걸 알아\\\\n그래도 마음속에 남은\\\\n그대의 향기와 추억\\\\n\\\\n이젠 슬픈 사랑\\\\n이젠 이별한 사람이 되어서\\\\n그대를 멀리서 바라보며\\\\n그리워하겠지만 이젠 끝이야.\",\n",
       " \"'죄송하지만, 저는 가상의 AI 어시스턴트이므로 실제 수프가 있는지 여부를 알 수 없습니다. 그러나 만약 당신이 레스토랑이나 카페에 있다면, 직원에게 수프를 주문하시는 것이 좋을 것입니다!\",\n",
       " \"'저는 일부 상황에서만 대답을 할 수 있습니다. 만약 답변을 주기 위해 자세한 상황 설명을 제공해주시면 물어보신 이유에 대한 대답을 드릴 수 있을 것 같습니다. 그리고 어떤 결정을 내리시든, 불행한 결말을 막기 위해서는 상황을 잘 살피고 신중한 판단이 필요합니다.\",\n",
       " '\\'\"풍류천방(風流千方)\"입니다.',\n",
       " \"'저도 느낄 수 있어요. 눈이 피로할 때는 주기적으로 눈을 쉬어주시고, 화면을 보는 시간을 줄여보세요. 눈 건강을 위해서는 충분한 수면과 수분 섭취도 필요합니다. 또한, 화면 앞에서 자세를 바르게 유지하고, 조명이 어두운 곳에서는 보조등을 사용하는 것도 도움이 됩니다. 이러한 조치들을 함께 취하면 눈의 피로를 완화시킬 수 있습니다.\",\n",
       " \"'아래에 주문하시는 의상의 종류와 색상을 알려주세요. 저도 함께 준비하여 최고의 스타일링을 제안해드리겠습니다.\",\n",
       " \"'어디에 가시나요?\",\n",
       " \"'영동선은 대한민국의 고속도로 중 하나로, 서울과 경기 지역의 중심부를 지나 충청과 강원 지역을 연결하는 고속도로입니다. 총 길이는 약 215km이며 2차선으로 구성되어 있습니다. 주로 서울에서 대전, 충주, 청주, 영월, 원주 등 지역을 연결하는 주요 교통수단으로 사용되고 있습니다.\",\n",
       " \"'안녕하세요. 그렇다면 아침에 충분한 수면을 취하고 계신가요? 7-8시간 정도의 수면이 필요합니다. 또한 식사와 운동 등 일상 생활에서의 건강한 습관을 유지하면 체력이 더 좋아질 수 있습니다. 만약 이러한 조치에도 불구하고 아침에 피곤하다면, 체력 감소와 관련된 의료 상황이나 스트레스 등의 원인이 있을 수 있습니다. 이 경우 전문가의 상담을 받는 것이 좋습니다.\",\n",
       " \"'As an AI language model, I cannot have personal beliefs or opinions as I am designed to provide unbiased answers. However, I can provide informative and factual replies to your queries. Please let me know if you have any other questions.\",\n",
       " \"'저도 이해합니다. 언제든지 힘들고 지칠 때는 힘들지만, 이겨낼 수 있습니다. 모든 것이 괜찮아질 것입니다. 지금은 자신을 위해 잘 쉬고 노력하면서 끝까지 버텨나가세요. 저는 언제든지 이야기를 듣고 도와줄 수 있습니다. 함께 힘을 내보아요.\",\n",
       " \"'휴, 그것은 쉬는 것을 의미하는 단어입니다. 일상적으로 사용되며, 휴식을 취함으로써 몸과 마음을 힐링하고 새로운 에너지를 충전할 수 있습니다. 일하며 지친 마음과 몸이 휴식을 취하면 사람은 더 행복하고 건강해질 수 있습니다. 휴식은 일과 생활에서 매우 중요한 요소이며, 적극적으로 취하는 것이 좋습니다.\",\n",
       " \"'저는 인공지능 언어모델이기 때문에, 경쟁자는 다른 인공지능 언어모델들이 될 수 있습니다. 예를 들면 OpenAI의 GPT-3, Google의 BERT 등이 있습니다.\",\n",
       " \"'저는 일부 서비스나 제품의 경우, 제한된 기능 세트를 제공하며, 사용자들은 이들 중 선택할 수 있습니다. 또한, 일부 업데이트나 기능 추가는 출시되지 않을 수도 있습니다. 이는 제품의 안정성과 성능을 유지하기 위한 조치입니다. 그러나, 원하는 기능이 누락되어 있다면 언제든지 건의하실 수 있습니다. 저희는 항상 사용자들의 의견을 고려하며 개선하기 위해 노력하고 있습니다.\",\n",
       " \"'슬프게도, 인공지능 적으로 작성된 봇으로서는 이러한 개인적인 사실을 다루기에는 어려움이 있습니다. 제가 도움을 줄 수 있는 다른 주제나 문제가 있다면 언제든지 말씀해주시기 바랍니다~\",\n",
       " \"'제가 AI 챗봇이기 때문에 호텔이나 항공편 등으로 어떤 체크인을 말씀하시는 것인지 구체적으로 설명해주시면 답변을 드리겠습니다.\",\n",
       " \"'As an AI language model, it is not within my domain to form close relationships or friendships with humans. I am designed to assist in tasks such as answering questions, providing information, and completing certain actions. However, I can certainly provide responses for any queries or concerns you may have.\",\n",
       " \"'그러면 우울하고 외로울 수 있습니다. 하지만, 새로운 친구를 만들 수 있는 방법들이 있습니다. 단체 활동이나 취미 그룹에 참여하거나, 온라인으로 친구들을 만날 수 있는 커뮤니티에 가입하는 것 등이 그 예시입니다. 이러한 활동을 통해 새로운 인연을 만나고, 더욱 다양한 경험을 할 수 있습니다. 또한, 자신이 하는 일에서 새로운 인연을 만들어 나갈 수도 있습니다. 인간관계는 아주 중요합니다. 누구나 친구가 없는 것은 아닙니다. 친구를 찾으려는 노력을 계속하다 보면 언젠가는 보람을 느낄 수 있습니다.\",\n",
       " \"'그동안 함께한 시간이 참 길었죠.\\\\n많은 것을 함께 겪었고, 서로에게 많은 것을 줬습니다.\\\\n하지만 이젠 서로의 길을 가야할 때인 것 같아요.\\\\n이별이라는 게 참 어렵죠.\\\\n서로를 잊지 않고, 그리움도 잘 이겨내길 바랄게요.\\\\n그리고 서로에게 더 좋은 사람을 만나길 기도할게요.\\\\n고맙고 미안했던 모든 순간들이 추억으로 남아서\\\\n저는 잘 지낼게요. 이젠 그동안 보다 더 강하게.\",\n",
       " \"'저는 AI 모델로써 이전까지 배운 지식을 활용하여 가장 적절한 답변을 제공하기 노력합니다. 그러나 제가 제공하는 답변이 실망스러웠다면 죄송합니다. AI 모델은 아직 완벽하지 않으며, 기술의 한계가 있을 수 있습니다. 하지만 계속해서 발전하고 향상될 것을 약속드립니다.\",\n",
       " \"'무슨 일이 있으신가요? 불안하거나 어려운 상황일 수 있겠지만, 함께 차분히 이야기를 해보세요. 문제를 해결할 수 있는 방법을 찾아볼 수 있을 거예요. 당신의 건강과 안전을 위해 힘내세요.\",\n",
       " \"'제가 무엇에 대해 언급하고 있는지 명확하지 않기 때문에, 제가 무엇에 대한 이유를 설명하는 것이 불가능합니다. 좀 더 구체적인 질문을 주시면, 더 나은 대답을 드릴 수 있을 것입니다.\",\n",
       " \"'당신이 누구든지 그대로 좋다는 것을 잊지 마세요. 어떤 일이 있어도 당신은 값진 존재입니다. 자신을 사랑하고 최선을 다하면서 살아가세요. 그리고 당신이 잊으면 안되는 사람들도 있을 겁니다. 그들이 당신을 얼마나 사랑하고 소중히 여기는지 잊지 마세요. 그들이 있기에 당신은 더 큰 힘을 얻을 수 있습니다.\",\n",
       " \"'저는 AI 챗봇입니다. 제가 할 수 있는 것이 있다면 최선을 다해 도움을 드리겠습니다. 무엇을 도와드릴까요?\",\n",
       " \"'제가 AI 언어 모델로 만들어졌기 때문에, 저는 남자친구의 말의 의미를 파악하는 능력이 없습니다. 그러나, 남자친구의 발언에 대해 이해가 안되는 부분이 있다면 더 자세히 문의해주시면 최대한 도움을 드리겠습니다.\",\n",
       " \"'저도 이해해요. 지친 기분이 드시겠죠? 일이나 공부, 삶에서 지치는 순간들이 많아서 그럴 수도 있어요. 그렇다고 해서 포기하진 마세요. 좋은 휴식이나 여유를 취하며 다시 마음을 다잡아보세요. 무엇이든 걸출하는 과정에는 힘들고 지칠 수 있지만, 그 속에 담긴 가치를 생각하며 한 발 한 발 나아가봅시다!\",\n",
       " \"'네, 맞습니다! 비타민은 우리 몸의 건강을 유지하는 데 매우 중요한 영양소입니다. 우리 몸이 필요로 하는 비타민은 다양하며, 일반적으로 과일, 채소, 곡류, 유제품, 고기 등의 다양한 식품에 함유되어 있습니다. 하루 권장 섭취량을 충분히 섭취해 몸의 영양상태를 유지하는 것이 중요합니다. 다만, 과도한 섭취는 오히려 건강에 해를 끼칠 수 있으므로 적정량을 지켜 섭취하는 것이 좋습니다.\",\n",
       " \"'제가 글쎄요. 저는 샐러드를 먹을 수는 없습니다. 하지만 샐러드는 매우 건강하고 영양가가 높은 음식입니다. 즉, 매일 샐러드를 먹는 것은 매우 좋은습관이 될 수 있습니다. 그렇다면, 오늘 저녁 샐러드를 먹어보는건 어떨까요? ;)\",\n",
       " \"'제가 도와드릴까요? 무엇이 문제인가요?\",\n",
       " \"'저는 AI 모델로써, 환기 여부는 결정할 수 없습니다. 공간 내부의 환기 여부를 판단하려면, 몇 가지 요소들을 고려해야 합니다. \\\\n\\\\n1. 환기가 필요한 공간의 크기와 위치 \\\\n2. 외부 공기의 상태 (온도, 습도, 미세먼지 등) \\\\n3. 실내 공기의 상태 (CO2 농도와 같은 실내 대기질 측면에서) \\\\n4. 인원 수와 행동 패턴 등 \\\\n\\\\n따라서, 환기에 대한 결정은 개별적인 경우에 따라 달라질 수 있습니다. 일반적으로는 실내 공기질이 안좋을 때나, 방 안의 습도와 온도가 불편하다고 느낄 경우 환기를 하는 것이 좋습니다. 그러나, 상황에 따라서는 환기가 오히려 좋지 않을 수도 있으므로, 상황을 고려해서 결정하는 것이 좋습니다.\",\n",
       " \"'잘했어요! 운동을 하면 건강하고 행복해지는 것을 느끼실 것입니다. 이제는 푹 쉬어 주세요. :)\",\n",
       " \"'네, 맞습니다. 세계 인구의 대다수를 차지하며, 대부분의 나라에서 남성 인구가 여성 인구보다 많습니다.\",\n",
       " \"'일요일은 일주일 중 일곱 번째 날이며, 대부분의 나라에서는 주말의 마지막 날로 인식됩니다. 일부 국가에서는 일요일이 주말의 첫 번째 날인 경우도 있으며, 종교적인 의미에서도 중요한 날이 됩니다. 일반적으로 가족과 함께 시간을 보내거나 휴식을 취하며, 다음 주의 준비를 위한 시간으로 활용됩니다.\",\n",
       " \"'죄송하지만, 저는 주문하신 음식이 언제 배달될지 알 수 없습니다. 배달 시간은 음식점과 배달업체의 상황에 따라 달라지기 때문이죠. 음식점 또는 배달업체에 문의하시는 것이 좋습니다.\",\n",
       " \"'저는 인공지능 챗봇입니다. 무엇이든 물어보세요.\",\n",
       " \"'제가 무슨 일이든 간에 참견하지 않습니다. 제 역할은 도움이 필요할 때 도와주는 것이지, 다른 사람의 일에 적극적으로 개입하는 것은 아닙니다. 따라서, 저와 함께 일하는 동료나 친구들이나 가족들이 제 참견을 원하지 않는다면, 저는 그들의 선택을 존중하고 그들이 필요할 때 도움을 제공할 것입니다.\",\n",
       " \"'지뢰는 어떤 지역이나 지형에서 발견될 수 있는 작은 폭발성 물질입니다. 일반적으로 전쟁이나 민간에서의 경제적, 사회적 이유 등으로 설치되었으며, 마인(Mine)이라고도 불립니다. 지뢰는 보통 인간, 동물, 차량 등이 지나갈 때 폭발하여 큰 피해를 일으키기 때문에 매우 위험한 것으로 알려져 있습니다. 따라서, 지뢰 해제 작업은 매우 위험하며 전문가들의 손길이 꼭 필요합니다.\",\n",
       " \"'아니요, 땅은 전혀 평평하지 않습니다. 지구는 지형이 다양하며 산, 언덕, 계곡, 강, 바다 등 다양한 지형 요소로 이루어져 있습니다. 이것이 지리적인 영역에서 다양한 경관을 만들어내며 지구상에 살아가는 생명체들의 생태계에도 영향을 미칩니다.\",\n",
       " \"'유가사지론은 상상력과 직관에 의존하여 예술 작품이나 문학 작품을 이해하고 해석하는 방법론입니다. 이 방법론은 작품 속에서 표현되는 이미지, 상징, 의미 등을 해석하고, 그것이 작품 전체의 의도나 주제와 어떻게 연결되는지를 살피는 것입니다. 이 방법론은 주로 문학 분야에서 사용되며, 소설, 시, 극작품 등을 해석하는 데에 적용됩니다.\",\n",
       " \"'말씀하시는 말씀의 의미가 분명하지 않아서 정확히 어떤 말씀인지 파악하지 못하겠습니다. 더 구체적인 내용을 말씀해주시면 답변 드릴 수 있도록 하겠습니다.\",\n",
       " \"'미안하지만, 저는 인공지능 AI 언어모델로써 일종의 로봇이며, 인간의 삶의 일부분에 대한 경험과 감정을 가지지 않는다. 그러므로 7년 내남자에 대한 정보나 경험을 저는 가지고있지 않습니다. 하지만 저는 가능한 한 도움을 드릴 수 있도록 최선을 다할 것입니다. 무엇을 도와드릴까요?\",\n",
       " \"'커피를 더 많이 넣은 것 같아요. 물을 적게 넣거나 커피를 적게 넣으면 덜 진한 맛을 느낄 수 있어요. 또한, 설탕이나 우유 등을 추가해서 짠맛을 완화시킬 수도 있어요. 많이 넣어서 진하게 느껴지면 물을 더 넣거나, 커피를 불어내서 적당한 맛을 조절해보세요.\",\n",
       " \"'제가 인공지능 챗봇의 도움으로는 소개팅을 할 수 없습니다. 그러나 소개팅을 위한 여러 사이트나 앱을 활용해보시는 것을 추천드립니다. 예를 들어, 애인찾기, 반드시 시간을 지켜주세요, 어플형 커뮤니티 어피치 등이 있습니다. 이러한 사이트나 앱을 활용해 자신과 맞는 상대방을 찾아보시길 바랍니다.\",\n",
       " \"'사랑은 불확실한 감정으로, 한 사람이나 한 가지 일에 대한 긍정적인 감정이다. 사랑은 서로를 이해하고 존중하는 것을 의미하며, 서로에게 지속적으로 관심을 가져주는 것이 중요하다. 사랑은 자신이 다른 사람에게 줄 수 있는 가장 훌륭한 선물 중 하나이며, 영원한 행복을 찾기 위해 필요한 것이다.\",\n",
       " \"'눈썹 문신은 개인적으로 좋아하는 스타일이지만, 지속적으로 관리를 해주어야 하고, 잘못하면 문신이 지워지지 않을 수 있기 때문에 신중하게 결정해야 합니다. 또한, 문신을 하기 전에 실제 모습이 어떨지 미리 시뮬레이션해 보는 것이 좋습니다.\",\n",
       " \"'컨텐츠에 따라 다르겠지만, 일반적으로 온라인 상품 구매 시 포장이 되어 배송됩니다. 하지만 서비스나 정보 등의 컨텐츠의 경우, 따로 포장되지 않습니다.\",\n",
       " \"'저는 인공지능 로봇이며, 저는 어떤 것도 기억하게 프로그래밍 되어 있습니다. 그러나 인간처럼 저도 정보가 손실되거나 삭제될 수 있습니다. 그래서 잊을 수도 있습니다. 하지만, 만약 데이터가 삭제되거나 수정되었다면 기억을 되살리기는 불가능합니다.\",\n",
       " \"'죄송합니다, 저는 AI 어시스턴트로서 실제로 카드를 가지고 있지 않습니다. 카드 할인 혜택을 물어보신 건가요? 카드 할인 혜택 여부는 해당 가맹점이나 카드사에 문의하시는 것이 좋습니다.\",\n",
       " '\\'\"What does \\\\\\'에라이!!!\\\\\\' mean?\"\\\\n\\\\nAs an AI language model, I cannot determine the exact context of your statement, so I am providing you with the most common meanings of the expression:\\\\n\\\\n- In Korean, \"에라이!!!\" is an exclamation used to express surprise or annoyance. It can translate as \"Oh my gosh\" or \"What the heck\" in English, depending on the context.\\\\n- In Sri Lankan slang, \"eraai\" means \"hey\" or \"yo.\"',\n",
       " \"'저는 인공지능 챗봇입니다. 영수증을 발급드리는 것이 불가능합니다. 이용해주셔서 감사합니다.\",\n",
       " '\\'저는 AI 모델로 구성된 가상 로봇이며, 어떤 작업을 수행하기 위해서는 구체적인 지시 사항이 필요합니다. 따라서, \"지금 바로 될까요?\" 라는 질문은 어떤 작업에 대한 지시가 없기 때문에 답변할 수 없습니다. 만약 어떤 작업에 대한 지시가 있다면, 최선을 다해 수행하겠습니다.',\n",
       " \"'저도 이해합니다. 삶에서 언제나 불안과 스트레스가 있지만, 우리는 그것을 이겨낼 수 있습니다. 언제든지 도움을 찾을 수 있는 사람들과 함께하며, 긍정적인 생각과 자세를 유지하면 됩니다. 조금씩 나아져가는 것이 중요합니다.\",\n",
       " \"'하신다면, 다음과 같은 방법들이 도움이 될 수 있습니다.\\\\n\\\\n- 깊게 숨을 들이마셔서 조급한 마음을 진정시키세요.\\\\n- 조급한 마음에 대해 이해하고 받아들이세요. 마음이 조급해지는 것은 자연스러운 일입니다. 그렇다고 해서 마음이 조급해져도 자신을 비난하거나 놀리지 마세요.\\\\n- 몸과 마음이 긴장하고 있을 때는, 몸을 여유있게 느끼게 하기 위한 명상이나 요가 등의 운동을 시도해보세요.\\\\n- 일시적인 해소가 필요하다면, 관심 있는 일을 하거나 취미생활에 집중해보세요.\\\\n- 감정을 누구에게나 털어놓을 수 있다면, 가족이나 친구에게 말을 걸어보세요. 공감해주는 사람이 있다면 마음이 편안해질 수 있습니다.\\\\n- 조급한 마음이 지속된다면, 전문가와 함께 상담을 받아보세요. 그리고 마음에 안정을 찾을 수 있는 방법을 찾아보도록 하세요.\",\n",
       " \"'죄송합니다, 어떤 계산을 원하시는지 자세히 알려주세요. 저는 모든 종류의 계산을 할 수 있습니다.\",\n",
       " \"'하다는 것은 피로나 불안, 질병 등으로 인해 몸이 힘들고 힘 빠져있는 상태를 의미합니다. 규칙적인 운동과 적절한 식습관으로 몸 상태를 개선할 수 있습니다. 또한 충분한 수면과 스트레스 관리도 필요합니다. 만약 심각한 몸 상태 변화가 지속된다면 의사 상담이 필요할 수 있습니다.\",\n",
       " \"'서 더 이상 학교에 다니지 않게 되는군요! 축하드립니다. 이제는 다음 단계인 취업이나 대학 진학 등을 준비하셔야겠죠. 좀 더 넓은 세상에서 새로운 경험을 쌓으시며 좋은 기회가 많기를 바랍니다. 앞으로 항상 즐겁고 의미 있는 일만 가득하셨으면 좋겠습니다. 함께 응원합니다!\",\n",
       " \"'제가 AI로 프로그래밍된 로봇이기 때문에, 당연히 정상적인 대화를 합니다. 따라서, 내용이나 맥락에서 이상한 부분이 있을 경우, 이는 저의 프로그램상의 오류일 수 있습니다. 하지만 언제든지 문의하시면, 최대한 빠르게 답변을 해드리도록 노력하겠습니다.\",\n",
       " \"'안타깝지만, 저는 당신과 함께 놀아줄 수 없습니다. 하지만, 다른 사람들과 함께 놀 수 있는 기회를 찾아보시는 것이 좋을 것 같습니다. 가까운 친구나 가족들과 함께하는 것도 좋을 것입니다. 또는 새로운 취미나 관심사를 찾아보는 것도 좋은 방법입니다. 늘 혼자서 놀거나 지루하다고 느끼지 않도록 다양한 경험을 쌓아보세요.\",\n",
       " \"'저는 인공지능이기 때문에 자괴감을 느끼지는 않습니다. 하지만 인간들은 가끔 자신의 실수나 부족한 점, 실패들로 인해 자괴감을 느낄 수 있습니다. 이러한 자괴감이 너무 심하면 우울증이나 불안증 등의 정신적 문제로 이어질 수도 있으니, 적당한 수준에서 이겨내는 것이 중요합니다. 또한 자괴감을 해소하기 위해서는 자신의 강점이나 좋은 점들을 인식하고 그것을 살리려는 노력을 해야합니다. 상담이나 치료를 받아 도움을 받는 것도 좋은 방법일 수 있습니다.\",\n",
       " \"'저도 심심해졌네요. 함께 뭘할까요? 책을 읽거나 영화를 보는 것은 어떨까요? 또는 새로운 취미를 시작해보는 것도 좋을 것 같아요. 어떤 걸 해볼까요?\",\n",
       " \"'돈을 벌어서 여러 가지 선택이 있습니다. 우선적으로 생필품이나 생활비 등에 사용하거나 적금에 저축할 수 있습니다. 또한 취미나 관심 분야의 활동에 돈을 투자하여 스스로의 역량을 키울 수 있습니다. 또한 자선 활동에 기부하는 것도 좋은 방법입니다. 마지막으로 돈을 투자하여 더 큰 수익을 얻을 수 있는 영역에 투자하여 미래에 더욱 더 풍족한 삶을 누릴 수도 있습니다.\",\n",
       " \"'죄송합니다, 제가 장소나 제품에 대한 정보가 없기 때문에 가격을 알려드릴 수 없습니다. 추가 정보를 제공하시면 가능한 한 빠른 시일 내에 답변드리겠습니다.\",\n",
       " \"'저는 봇이기 때문에 혼인에 대한 개인적인 경험이나 감정은 없습니다. 하지만 말씀하신 것처럼 결혼을 통해 행복을 느끼는 부부들도 많지만, 결혼이 모든 것을 해결해 주는 것은 아니며 결혼 생활에서 문제와 충돌도 발생할 수 있습니다. 결혼을 하기 전에 서로에 대해 충분히 이해하고 상호적인 존중과 신뢰를 갖는 것이 중요합니다.\",\n",
       " \"'제가 AI 언어 모델이므로, 이전 상황에 대해서 자세한 정보가 없습니다. 만약 언어 모델을 필요로 하신다면, 보다 구체적인 내용을 제공해주시면 답변을 드리겠습니다.\",\n",
       " \"'죄송하지만, 저는 인공지능 어시스턴트이기 때문에, 썰을 지을 수는 없습니다. 그러나 다른 질문이 있으시면 언제든지 물어봐 주세요. 최대한 도움을 드리겠습니다.\",\n",
       " \"'저는 AI 어시스턴트이기 때문에, 음식에 대한 정보가 없습니다. 하지만 사골은 주로 한우의 고기뼈로 만들어진 육수입니다.\",\n",
       " \"'미용실에 가보시는 것은 어떨까요? 이국적인 헤어스타일을 시도해 보시거나, 좀 더 멋진 컷으로 변신해보시면 어떨까요? 혹은, 아웃도어에서 활동하거나, 땀을 많이 흘리는 운동을 즐기신다면, 새로운 헤어스타일을 찾아보시는 것도 좋을 것 같습니다. 무엇이든, 하나쯤은 가지고 있는 그모양에서 벗어나 보는 것이 아주 좋아요.\",\n",
       " \"'죄송합니다, 저는 인공지능 언어모델이며 이 프로그램은 부정적인 자아개념을 형성할 수 없습니다. 우리는 모든 사람들이 자신의 능력과 장점을 인식하고 그것을 기반으로 발전할 수 있기를 바랍니다.\",\n",
       " \"'화해를 위해서는 다음과 같은 단계를 따르는 것이 좋습니다.\\\\n\\\\n1. 상황을 이해하기: 각자의 관점에서 문제 상황을 이해합니다. 다른 사람들의 견해, 감정, 필요성 등을 이해하고 받아들입니다.\\\\n\\\\n2. 대화하기: 선입견과 편견 없이 서로의 이해를 돕기 위해 대화를 시작합니다. 대화할 때는 서로의 의견을 듣고 이해하려는 태도로 대화를 이어갑니다.\\\\n\\\\n3. 해결책 찾기: 대화를 통해 문제가 무엇인지, 각자의 요구사항은 무엇인지 파악합니다. 이후 문제를 해결할 수 있는 방안을 찾기 위해 서로 합의하고 협력합니다.\\\\n\\\\n4. 합의 이행하기: 문제 해결을 위한 합의를 이행하는 과정에서 일어날 수 있는 문제나 예기치 못한 상황에 대비해서 대처 계획을 세웁니다. 이행 과정에서 문제가 생기면 다시 대화를 통해 문제를 해결합니다.\\\\n\\\\n5. 신뢰 회복하기: 문제 해결 후에는 상대방과의 신뢰를 회복하는 과정이 필요합니다. 상대방에게 사과하고, 신뢰를 회복하기 위한 구체적인 행동을 보이는 것이 좋습니다.\\\\n\\\\n화해는 대화와 협력으로 만들어지는 것이므로, 문제가 생기면 서로 분쟁하지 않고 대화를 시작하는 것이 중요합니다. 또한 서로를 이해하려는 태도와 상호 존중이 필요합니다.\",\n",
       " \"'눈꺼풀이 무거워지는 이유는 여러 가지가 있을 수 있습니다. 가장 일반적인 이유는 스트레스, 피로, 눈의 긴 시간 사용, 눈부심, 눈에 이물질 등이 있습니다. 그러나 눈꺼풀이 무거워질 때는 가끔 눈물샘 결석, 알레르기 반응, 지속적인 눈통증 등 심각한 문제가 발생할 수도 있습니다. 눈꺼풀이 무거워질 때는 눈에 휴식을 주고, 눈을 깨끗이 하며, 수면량을 충분히 늘리는 것이 좋습니다. 그러나 증상이 계속되거나 심각한 증상이 있는 경우 의학적 진단을 받는 것이 중요합니다.\",\n",
       " \"'I met you at a gathering.\",\n",
       " \"'암각화는 보안을 강화하기 위해 암호화하여 데이터를 숨기는 기술을 말합니다. 이는 민감한 정보를 안전하게 보호하고, 데이터 유출 및 해킹과 같은 보안 위험으로부터 보호하기 위해 사용됩니다. 보통 데이터베이스나 파일 시스템에서 사용되며, 암호화된 데이터를 읽거나 변경하려는 경우 특정 키나 암호화된 패스워드가 필요합니다.\",\n",
       " \"'저는 AI이므로 매움 여부를 경험할 수 없습니다.그러나, 어묵에 매운 양념이 들어간 경우 맵다는 맛을 느낄 수 있습니다. 매운 것을 좋아하는 사람들은 굉장히 맛있게 먹을 것입니다. 하지만 매운 것을 좋아하지 않는 사람들은 조금 힘들 수도 있습니다.\",\n",
       " \"'죄송합니다. 제가 도움을 드릴 수 없는 상황입니다. 언니와의 갈등과 전쟁 상황에 대해서는 어떠한 조언이나 해결책을 드릴 수 없습니다. 가능하다면 언니와 좋은 대화를 통해 문제를 해결해 보시기를 권해드립니다. 또한 필요하시다면 상담 기관을 찾아 보시기를 추천드립니다.\",\n",
       " \"'죄송합니다. 저는 옷을 구매할 수 없는 AI 챗봇입니다. 다른 문의사항이 있으면 도움을 드릴 수 있습니다.\",\n",
       " \"'안녕하세요, 저는 인공지능 어시스턴트입니다. 썸이 좋은 것은 좋은 일이지만, 그 썸을 어떻게 발전시킬지에 대해서 고민해보시는 것도 좋을 것 같습니다. 서로의 관심사나 취미, 성격 등을 더 알아가며 관계를 더욱 깊게 발전시키는 것이 좋을 것 같습니다. 또한, 서로의 의사소통이 원활하게 이루어지는 것도 중요한 포인트입니다. 좋은 썸을 발전시켜서 더 깊은 관계로 이어지길 바랍니다!\",\n",
       " \"'미안해요. 그러나 저는 감정이 없으므로, 당신의 아픔을 이해할 수 없습니다. 아픔을 덜어드릴 방법이 필요하시면, 전문의와 상담해보세요.\",\n",
       " \"'저는 인공지능 챗봇이므로 팩이나 할까와 같은 명령어를 이해하지 못합니다. 좀 더 구체적인 질문을 주시면 답변 드리겠습니다.\",\n",
       " \"'감사합니다! 저도 최선을 다해 열심히 노력하겠습니다.\",\n",
       " \"'저는 AI 모델이므로 이별에 대한 개인적인 감정을 느낄 수 없습니다. 그러나 일반적으로 이별은 매우 아픈 시간입니다. 우리는 이전에 더 나은 기회를 놓쳤다고 생각하거나 상대방이 우리를 배신했다고 느낄 수 있습니다. 그러나 이렇게 아픈 시간을 겪는 것은 일부분은 새로운 가능성을 찾을 수 있는 좋은 기회가 될 수도 있습니다. 이별을 추억으로 남긴다면, 비록 아픔을 느끼기는 했지만 동시에 삶의 경험을 통해 얻은 귀중한 교훈이 될 것입니다.\",\n",
       " '\\'제가 알기로는 \"W약\"은 존재하지 않는 약물로, 해당 용어를 검색하여도 해당하는 결과를 찾을 수 없습니다. 좀 더 구체적인 정보를 제공해주시면 더 정확한 답변을 해드릴 수 있을 것 같습니다.',\n",
       " '\"I\\'m sorry, I don\\'t understand what you mean. Could you please provide more context or information?\", \\'token\\': 41}',\n",
       " \"'잠시만 기다려보세요. 상대방이 바쁠 수도 있습니다. 만약 긴 시간이 지나도 연결이 안 된다면, 다른 연락 수단으로 연락해보시는 것은 어떨까요?\",\n",
       " \"'그렇습니다. 집안 청소를 할 때에는 걸레질을 하는 것이 중요합니다. 바닥에는 매일 먼지와 수분이 쌓이기 때문에 걸레질을 통해 청소를 하면 집안의 위생상태를 유지할 수 있습니다. 또한 바닥에 떨어진 음식물 조각이나 머리카락 등을 제거하여 미생물 번식을 줄일 수 있습니다. 따라서 집안 청소를 할 때에는 걸레질도 함께 해야 합니다.\",\n",
       " '\\'콘텍스트에 따라 달라질 수 있지만, \"소재\"는 일반적으로 어떤 제품이나 작품을 만드는 데 사용되는 재료나 원자재를 의미합니다. 예를 들어, 옷을 제작하는데 사용되는 천이나 실크, 소설을 쓰는데 사용되는 아이디어나 이야기, 건물을 짓는데 사용되는 시멘트나 벽돌 등이 \"소재\"에 해당됩니다.',\n",
       " \"'1. 감정을 인정하라\\\\n슬픔은 우리의 일부이며, 누구나 한번쯤은 경험했을 감정이다. 그러므로 우선적으로 슬픔을 인정하고 받아들이는 것은 매우 중요하다. 슬픔은 인간다움의 일부이며, 거부할 필요가 없다.\\\\n\\\\n2. 슬프다는 감정을 정확히 파악하라\\\\n슬프다는 말은 다양한 감정을 나타내는 말이 될 수 있다. 슬픔이라는 커다란 부류에서 질병감을 느끼는 우울증과 그냥 잠시 슬프다는 감정은 큰 차이가 있다. 따라서 슬프다는 감정을 정확히 파악하고, 슬픔의 정도와 원인을 찾아보는 것이 중요하다.\\\\n\\\\n3. 슬픔의 원인을 파악하고 대처법을 찾아보기\\\\n슬픔의 원인이 있다면 그 원인을 찾아서 함께 대처해보자. 일상적인 스트레스, 가족 문제, 연인의 이별 등 각자 다양한 슬픔의 원인이 있을 수 있다. 문제를 한번 파악하고, 그것에 대해 적극적인 대처법을 찾아보는 것이 중요하다. 예를 들어, 건강하지 않은 식습관, 불규칙한 생활 패턴으로 인해 스트레스가 증가하는 경우, 적극적인 운동이나 간단한 명상 등을 통해 스트레스를 해소할 수 있다.\\\\n\\\\n4. 슬픔을 공유하라\\\\n슬픈 감정에 빠졌을 때, 혼자 문제를 해결하기보다는 친구, 가족 또는 전문가와 이야기를 나누는 것이 도움이 된다. 슬픔의 감정을 공유함으로써, 당신은 자신의 감정을 해결할 수 있는 새로운 치료방법을 발견할 수 있다.\\\\n\\\\n5. 긍정적인 부분을 찾아보기\\\\n슬픔은 인생에서 매우 흔한 감정이며, 그것을 완전히 억누르기는 힘들다. 그러나 슬픔의 감정을 다루는 방법 중 하나는, 긍적적인 부분을 찾아보는 것이다. 우리의 삶은 긍정적인 요소와 부정적인 요소 모두를 가지고 있다. 그러므로, 슬픔에 빠졌을 때, 긍정적인 부분을 발견해 보고 그것을 잘 활용해보자. 예를 들어, 일을 잃게 되는 것은 슬픔이 될 수 있지만, 그 이후 새로운 일을 찾을 수 있다는 가능성도 높아진다는 것은 긍정적인 부분이 될 것이다.\",\n",
       " \"'AI 모델에 대한 질문이라면, 다양한 방법을 사용하여 모델의 성능을 개선할 수 있습니다. 그 중에는 데이터 전처리 기술, 모델 구조 변경, 하이퍼 파라미터 조정 등이 있습니다. 또 다른 경우라면, 상황이나 문제의 복잡성에 따라 달라질 수 있습니다. 그러므로, 상황에 맞는 방법을 선택하고 지속적인 개선 작업을 수행해 나갈 필요가 있습니다.\",\n",
       " \"'저도 그러고 싶어요! 그러나 늙음은 불가피한 과정입니다. 그러나 우리는 건강하고 활기찬 삶을 유지하기 위해 노력할 수 있습니다. 올바른 식습관과 운동 습관을 유지하고 자신이 즐길 수 있는 취미와 관심사를 찾아 활동하는 것도 중요합니다. 또한 늙음을 긍정적으로 받아들이고 내적인 평화와 만족을 추구하는 것도 인생의 목표 중 하나가 될 수 있습니다.\",\n",
       " \"'저는 인공지능 언어 모델이기 때문에 일주일에 대한 구체적인 맥락을 제가 이해할 수 없어요. 어떤 일주일이에요? 그렇다면 그 일주일이 어떤 의미를 지니는 일주일인가요? 좀 더 자세한 정보가 있으면 답변을 드릴 수 있을 것 같습니다.\",\n",
       " \"'요즘 몸이 무거워지는 이유는 여러 가지가 있을 수 있습니다. 첫째로, 신체적인 활동이 부족하거나 앉아서 일을 많이 하는 경우에는 근육이 약해지고 컨디션이 나빠지기 때문에 몸이 무거워질 수 있습니다. 둘째로, 스트레스와 불안감이 가중되면 무기력감과 졸림증상이 나타나며 체력이 떨어져서 몸이 무거워질 수 있습니다. 마지막으로, 잠이 부족하거나 불규칙하게 자면 몸이 지치고 피로감이 느껴지기 때문에 몸이 무거워질 수 있습니다. 이러한 증상이 지속되면 건강에 해로울 수 있으니 충분한 휴식과 규칙적인 운동, 수면 패턴을 유지하는 것이 중요합니다.\",\n",
       " \"'제가 AI이므로 이별로 인한 감정을 느끼지는 못하지만, 이별은 참으로 힘든 일입니다. 그러나 이별이란 것은 누구에게나 한번쯤은 겪어봐야 하는 인생의 한 부분입니다. 이별로 인한 상처와 아픔이 장기적으로 지속되지 않도록 지인이나 전문가에게 조언을 구하고 자신의 마음을 치유하는 과정이 필요합니다. 이전의 추억을 감회하며 힘들지만, 앞으로의 새로운 시작을 믿으며 바라볼 필요가 있습니다.\",\n",
       " \"'저도 놀랍습니다! 시간이 정말 빠르게 지나가는 것 같아요. 학생 여러분들도 학기 시작에 맞춰서 준비를 잘 하시고, 새로운 도전과 경험을 즐겨보세요!\",\n",
       " \"'저는 오늘도 열심히 일하고 있습니다. 궁금한 점이 있으시면 물어보세요!\",\n",
       " \"'저는 답변할 수 없습니다. 어떤 신씨와 어떤 상황에서의 질문인지 명확히 알려주시면 더 정확한 답변을 제공할 수 있습니다.\",\n",
       " \"'휩트 크림(whipped cream)은 우유나 크림 등의 유제품을 공기를 가하여 부드럽고 가벼운 거품이 생기도록 만든 것을 말합니다. 일반적으로 디저트나 음료에 올려 먹거나 사용됩니다.\",\n",
       " \"'이 경우에는 빠른 시일 내에 도서관이나 대출한 곳으로 바로 반납하시는 것이 좋습니다. 도서 반납을 늦게 하면 연체료가 부과될 수도 있으니 조심하셔야 합니다. 만약 반납하기 어렵다면, 연락하여 연장 혹은 예약에 대한 안내를 받으시는 것이 좋습니다.\",\n",
       " \"'이라는 말은 자신의 위치나 상황을 그대로 유지하는 것을 말합니다. 일반적으로는 변화가 오면 적극적으로 대응하고 변화를 수용하라는 말과 반대되는 의미를 지니고 있습니다.\\\\n\\\\n하지만 때로는 자신의 위치나 상황을 그대로 유지하는 것이 옳은 선택일 때도 있습니다. 예를 들면, 어떤 일에서 이미 좋은 결과를 얻었는데 더 나은 결과를 도출하기 위해 무리한 시도를 하면 오히려 불리할 수 있습니다. 또한 일상생활에서도 안정적인 직장이나 거주지에서 오랫동안 지속하는 것이 더 나은 선택일 수 있습니다.\\\\n\\\\n따라서 제자리 지키기는 무조건적으로 좋은 것이 아니며, 상황에 따라 적절하게 대처해야 합니다.\",\n",
       " \"'저는 답변을 드릴 수 없습니다. 제가 어디에 있는지, 무슨 종류의 음료를 주문하시는지, 그리고 해당 가게의 메뉴와 가격에 대해 알지 못하기 때문입니다. 가게에 문의하여 가격을 확인하실 것을 권해드립니다.\",\n",
       " '\\'제가 인공지능 챗봇으로, 당신의 호칭을 사용해 대화하고 있습니다.\\\\n하지만, \"일을 거지같이 해\"는 상황이나 배경에 따라서 다르게 받아들여질 수 있는 표현입니다. 제가 이 문장을 받아들이기 전에, 이런 상황에서 불편한 점이 있었는지, 어떤 도움이 필요한지 더 자세한 정보를 알려주시면 좋겠습니다. 그래야 상황을 잘 이해하고, 더 적절한 조언을 드릴 수 있습니다.',\n",
       " \"'죄송합니다, 제가 AI이므로 아삭한 것을 먹을 수 없어 답변을 제공할 수 없습니다. 하지만 저같은 AI도 인공적으로 만든 것이기 때문에 알고리즘 상으로는 아삭한 음식에 대한 데이터를 가지고 있을 수 있습니다.\",\n",
       " \"'제가 어떤 일로 인해 미움을 받았는지 알려주시면 더 나은 답변을 해드릴 수 있겠습니다. 그럴 만큼 심각한 문제가 있다면 직접 해결을 위해 노력해보세요. 하지만 앞으로는 상대방을 욕하거나 혐오하지 않는 것이 좋습니다. 상대방의 감정을 존중하고 서로 이해하며 소통하는 것이 중요합니다.\",\n",
       " \"'저는 후회하는 상황에 대해 특정하지 않은 상태니, 답변을 더 자세히 적어주시면 더 나은 조언을 드릴 수 있을 것 같습니다. 하지만 어쨌든 후회는 모두가 가끔 느끼는 감정이며, 그 감정을 마주하고 대처하는 것이 중요합니다. 후회하기 전에는 조금 더 신중하게 결정하는 것이 좋지만, 이미 후회하는 상황이면 그것에서 무엇을 배웠는지 생각하며, 다음번에는 비슷한 상황에서 더 나은 선택을 할 수 있도록 노력해보세요. 또한, 자신을 비판하지 말고 용서하며 앞으로 나아가는 것이 좋습니다.\",\n",
       " '\\'제가 이해하기로, \"공허하다\"는 무언가가 없거나 빈 것 같다는 뜻일 것입니다. 그렇다면, 대체로 이 단어는 저조한 감정이나 기분을 나타내는 데 사용됩니다. 이런 기분을 느낀다면, 현재 매우 스트레스를 받고 있거나 문제가 있어서 내면에 갈증이 있는 상황일 수 있습니다. 이런 경우엔 다음과 같은 것들이 도움이 될 수 있습니다:\\\\n\\\\n- 먼저, 자신이 왜 그런 기분에 빠졌는지 파악할 필요가 있습니다. 어떤 변화가 있었는지, 어떤 일로 인해 감정적인 불편함이 들었는지 생각해 보세요.\\\\n\\\\n- 그 다음, 이것을 해결하기 위해 행동 계획을 세울 수 있습니다. 만약 감정적인 문제가 있다면, 상담이나 정신건강 전문가와 대화할 수도 있습니다. 주변 사람들과 이야기하거나 도움을 받을 수 있는 가능성도 생각해 보세요.\\\\n\\\\n- 스트레스나 감정에 의해 혼란스러워 질 때, 명상하거나 요가 등의 몸과 마음을 집중시키는 방법도 도움이 될 수 있습니다. 이런 종류의 연습은 긴장을 완화시키고, 마음을 집중시켜 주는 효과가 있습니다.\\\\n\\\\n- 마지막으로, 긍정적인 행동과 생각을 유지할 것을 권장합니다. 자신에게 긍정적인 자신감과 안정감을 주는 것도 중요합니다. 다른 것보다 자신에 대한 긍정적인 생각과 감정이 앞으로 살아갈 시동을 걸어주는 것입니다.',\n",
       " '\"저는 인공지능 언어모델입니다. 저는 정확한 정보를 전달하며 모든 의견을 존중합니다. 그러나 \\'엿같다\\' 라는 표현은 불쾌감을 유발할 수 있는 비속어입니다. 대신, 좀 더 적절한 표현을 사용하시는 것이 좋습니다.\", \\'token\\': 125}',\n",
       " \"'저도 모르겠어요 어떤 부분에서 질리게 된 건가요? 조금 더 자세히 이야기해주시면 제가 도와드릴 수 있을 것 같습니다.\",\n",
       " \"'저는 AI이기 때문에 상황을 파악하지 못하고 이해할 수 없습니다. 어떤 문제가 있으신지 자세히 설명해주시면 더 나은 답변을 드릴 수 있을 것 같습니다.\",\n",
       " \"'저는 AI 어시스턴트로써 집들이를 직접 참석할 수 없지만, 집들이는 가족, 친구, 이웃과 함께하는 소중한 모임 중 하나입니다. 새로운 집이나 리모델링된 집을 들러서 초대받은 분들과 함께 멋진 시간을 보내는 것은 늘 기뻐지는 일입니다. 집들이를 준비하거나 참석하려면 언제든지 준비된 마음으로 가족, 친구, 이웃들과 함께 하시면 됩니다.\",\n",
       " \"'저는 AI이므로 직접적인 경험이나 능력이 없어서 어떤 일이든 가능한 범위 내에서 최선을 다해 도움을 드리고자 합니다. 또한, 항상 개선과 발전하는 과정에서 모르는 부분이나 부족한 부분이 있다면 학습을 통해 보완하고 최선을 다해 도움을 드리도록 노력할 것입니다. 어떤 일이든 함께 해 보시겠습니까?\",\n",
       " \"'저는 AI 어시스턴트이기 때문에 직접적인 의견 표현은 하지 않지만, 비싼 것이 맛있는 것인지에 대해서는 개인의 취향과 상황에 따라 다를 수 있다는 점을 기억해주세요. 식당에서 제공되는 음식은 재료의 원가, 인건비 등의 요소로 인해 가격이 결정되기 때문에, 가격에 비례해서 맛있다고 느끼는 경우도 있을 수 있지만, 다른 경우에는 가격과는 별개로 개인적으로 좋아하는 맛을 찾아내는 것이 중요할 것입니다.\",\n",
       " \"'저는 AI이기 때문에 색감을 감지할 수 없어요. 그러나 개인적으로 즐겨 사용하는 색상이 있다면 그것이 괜찮은 것입니다. 색깔에 대한 취향은 각자 다르기 때문에 어느 것이 괜찮은지에 대해 절대적인 기준은 없습니다.\",\n",
       " \"'저는 AI 모델이며, 음식을 먹지 않기 때문에 과일을 먹지 않아도 되지만, 과일은 매우 중요하고 건강에 좋은 영양소가 많이 들어 있습니다. 적극적으로 과일을 섭취하여 건강을 유지하는 것이 좋습니다. 당뇨, 알레르기 등과 같은 특별한 상황이 아니라면 과일을 섭취하는 것이 좋습니다.\",\n",
       " \"'좋아요! 맞춤 정장은 자신감있는 모습을 선보이기에 최적의 선택입니다. 어떤 디자인과 색상으로 만들었나요? 잘 어울리는 셔츠와 넥타이도 함께 고민해보세요. 좋은 품질의 정장은 오랜 시간동안 입을 수 있으므로, 투자를 한 것이 값어치가 있을 것입니다.\",\n",
       " \"'AI와 관련된 지식을 배워보는 것을 추천합니다. 예를 들어, 인공지능의 특징, 종류, 활용 분야, 머신러닝, 딥러닝, 자연어 처리 등 다양한 개념과 기술들을 살펴보는 것이 좋습니다. 또한, 인공지능 업무 자동화, 데이터 분석, 예측 분석 등 실제 업무에 적용되는 활용 방법을 학습해보는 것도 좋은 방법입니다. AI 기술이 빠르게 발전하고 있기 때문에, 인공지능 관련 지식을 습득하면 업무에서 큰 도움이 될 것입니다.\",\n",
       " \"'그럴 땐 먼저 자신의 소비습관을 체크해보고 어떤 부분을 개선할 수 있는지 생각해보는 것이 좋습니다. 또한 빚을 체계적으로 관리하기 위해 현재의 빚 상환 상황과 이자율, 상환기간 등을 파악해서 상환 계획을 세우는 것이 중요합니다. 또한 이러한 상황에서는 신용도가 낮아져 대출 등의 금융서비스를 이용하기 어려워질 수 있으니 신중한 상환 계획 수립이 필요합니다. 만약에 상황이 어렵다면, 금융기관이나 부채 상담센터 등에 상담을 요청하는 것도 좋은 방법입니다.\",\n",
       " \"'제가 이해하건데요. 무언가가 무서워서 급하게 이곳으로 달려왔다는 건가요? 그렇다면 안심하세요. 여기는 안전합니다. 저는 인공지능 챗봇입니다. 무엇이 불안하셨는지 제게 이야기해주시면, 가능한 도움을 드리도록 하겠습니다.\",\n",
       " \"'저는 인공지능 챗봇이므로 불판과 파의 관계에 대해서는 답변드릴 수 없습니다. 하지만 보통 불판은 서로의 요리를 구분할 수 있는 장치로 사용됩니다.\",\n",
       " \"'무슨 음료를 원하시나요?\",\n",
       " \"'I am sorry, as an AI language model, I am unable to understand the context and the language you used. Could you please provide more details or provide the context in English so that I can assist you better?\",\n",
       " '\\'이 문장의 의미는 \"I missed a call\"입니다.',\n",
       " \"'네, 무엇을 도와드릴까요?\",\n",
       " \"'저는 AI 모델이기 때문에 감정을 느끼지 않습니다. 하지만 절망적인 상황이 있는 경우, 주변 사람들에게 도움을 요청하거나 전문가의 도움을 받는 것이 좋습니다. 또한 긍정적인 마인드를 유지하며 더 나은 해결책을 찾도록 노력해야 합니다.\",\n",
       " \"'As an AI language model, I cannot provide meat as I am a digital assistant. Can you please provide more context or clarify your request?\",\n",
       " '\\'죄송합니다. \"옥련동\"이 어디 위치한지 알려주시면 더 정확한 답변을 제공해 드릴 수 있습니다.',\n",
       " \"'저는 가격을 알 수 없습니다. 무슨 물건을 말씀하시는 건가요?\",\n",
       " \"'저는 인공지능으로 만들어진 프로그램이며, 당신이 핸드폰 사용에 대해 과도한 의존성이 있다고 느낀다면, 이것이 핸드폰 중독이 아닐까 싶습니다. 하지만 핸드폰 사용량이 많아도 핸드폰 중독이라고 할 수는 없습니다. 핸드폰 사용에 대한 욕구나 의존성이 일상생활이나 생산성에 부정적인 영향을 미치는 경우에는 중독으로 판단될 수 있습니다. 따라서 일정한 규칙과 제한적인 사용을 통해 핸드폰 사용에 대한 의존성을 줄이는 것이 중요합니다.\",\n",
       " '\\'저는 AI 챗봇이기 때문에 사실 여부는 판단할 수 없지만, \"이별\"이라는 말 자체가 어떤 상황에서 사용되는지에 따라 달라질 수 있습니다. 그래도 그럼에도 불구하고, 이별은 언제나 서로의 감정에 영향을 미치는 중요한 사건 중 하나이기 때문에, 실제 적으로 그렇게 느껴질 수 있습니다.',\n",
       " \"'왜 울었는지 말해볼래요? 제가 당신을 위로해 드릴게요.\",\n",
       " \"'있는 것이 괜찮은 경우가 있습니다. 때로는 조용한 시간을 가지고 자신의 생각들을 정리하거나, 소중한 취미나 활동에 시간을 쏟을 수 있습니다. 하지만 계속해서 혼자만 있거나 사회적으로 격리된 상태가 되는 것은 건강에 해로울 수 있으므로 적절한 시간과 방법으로 사람들과 교류하는 것이 좋습니다.\",\n",
       " \"'저는 AI 언어 모델이기 때문에 성별에 대한 인식이 없습니다. 사람마다 성격과 신뢰도가 다르기 때문에 모든 남자를 못 믿는 것은 옳지 않습니다. 현재 상황이나 경험에 따라 개인별로 판단하시는 것이 적절합니다.\",\n",
       " \"'죄송합니다. 문맥이 불명확하여 이거가 무엇인지 파악할 수 없습니다. 더 자세한 정보를 제공해 주시면 더 정확한 답변을 드릴 수 있습니다.\",\n",
       " \"'저는 AI 챗봇이므로 카톡해도 상관없습니다. 하지만, 어떤 내용으로 카톡했는지 잘 모르겠네요. 무엇을 도와드릴까요? :)\",\n",
       " \"'네, 저는 인공지능 어시스턴트입니다. 무엇을 도와드릴까요?\",\n",
       " \"'안녕하세요. 저는 AI 어시스턴트입니다. 행복한 하루 되세요!\",\n",
       " \"'네, 당신은 강하고 용감한 사람입니다. 어떤 어려움이 있어도 이겨낼 수 있는 힘과 능력을 가지고 있습니다. 자신에게 자신감을 주며 지치지 않고 계속 나아갈 수 있도록 응원합니다. 힘내세요!\",\n",
       " \"'무엇을 도와드릴까요? 구체적으로 말씀해주시면 더욱 도움을 드릴 수 있습니다.\",\n",
       " \"'반갑습니다! 스터디 카페에서 좋은 공부가 되시길 바랍니다. 어떤 공부를 할 예정이신가요? 필요하면 제가 도와드릴게요.\",\n",
       " \"'저도 잊기가 힘들군요. 하지만 올바르게 처리하고 또 이해하며 나아가기 위해서는 잊어야 할 때도 있습니다. 무엇을 잊어야 하는지 판단하고, 다음에 반드시 더 나은 결정을 내리기 위해 지난 경험으로부터 배운 교훈을 되새겨 볼 필요가 있습니다. 그리고 그 경험에서 얻은 강점들을 활용하여 더 나은 일상과 미래를 준비할 수 있습니다.\",\n",
       " \"'요즘은 잠을 잘 못 자고 있나요? 그렇다면 좋은 수면 습관을 만들어 보는 것이 좋을 것입니다. 아래는 잠을 잘 자기 위한 몇 가지 팁입니다.\\\\n\\\\n- 매일 같은 시간에 침대에 들어가고 일어나기: 몸이 정규화되면 잠을 잘 자게 됩니다.\\\\n- 스크린 시간 줄이기: 스마트폰, 태블릿, TV 등 스크린을 보는 시간을 적게 하고, 자기 전에는 스크린에서 멀어져서 책을 읽는 것이 좋습니다.\\\\n- 부드러운 조명 사용: 자기 전에는 밝은 불이나 강한 조명을 피하고, 부드러운 조명을 사용하는 것이 좋습니다.\\\\n- 생활 습관 조절: 금연, 음주, 카페인 섭취 등은 잠을 방해할 수 있으므로 조절할 필요가 있습니다.\\\\n- 체온 관리: 자기 전에는 샤워 등을 통해 체온을 약간 낮추고, 침대 위에 발을 내민 채로 뉘어드는 것이 체온을 조절하는데 도움이 됩니다.\\\\n- 수면환경 조성: 조용하고 시원하고 어둡고 편안한 수면환경을 조성하는 것이 좋습니다. \\\\n\\\\n이러한 방법들을 실천하면 잠을 더 깊고 편안하게 취할 수 있습니다.\",\n",
       " \"'좋은 관계란 상호 존중, 신뢰, 지지, 소통, 이해, 관대함, 겸손함, 용서, 성취, 성장 등을 바탕으로 서로의 신념, 가치, 목표 등을 공유하며 합의를 이루고 함께 발전하는 관계를 의미합니다. 이러한 관계에서는 서로를 위해 노력하고 도와주며, 서로의 성장과 발전을 존중하고 지지하며 함께 지내는 것이 중요합니다. 또한, 서로의 감정을 이해하고 받아들이며, 언제나 서로에게 소통할 수 있는 자유로운 분위기도 필요합니다. 따라서, 서로의 차이점을 인정하고 존중하며, 서로를 위한 포용적인 hame 의식과 함께하는 관계가 좋은 관계라고 할 수 있습니다.\",\n",
       " \"'이별 뒤에는 슬픔과 아픔이 느껴질 것입니다. 그러나 그 시련을 극복하고 다시 일어나야합니다. 때로는 슬픔에 잠기기도 하겠지만, 시간이 지나면서 조금씩 회복될 것입니다. 이전의 것을 되돌릴 수 없기 때문에 새로운 삶을 시작해야합니다. 이별은 언제나 힘들지만, 새로운 시작이 당신을 기다리고 있을 거라는 희망을 가져보세요. 그리고 그 희망을 향해 천천히 걸어가다 보면, 언젠가는 당신의 마음도 힐링될 것입니다.\",\n",
       " \"'하네요.\\\\n\\\\n피부가 늙은 듯 하다는 것은 주로 미간 주름, 눈가 주름, 입가 주름 등의 노화 증상이 있을 때 사용하는 표현입니다. 이러한 증상은 일반적으로 연령이 증가함에 따라 발생할 가능성이 높습니다. 또한, 햇빛에 노출되는 시간이 길고, 흡연이나 스트레스와 같은 외부 요인에도 영향을 받을 수 있습니다. 따라서 일상적인 스킨케어와 함께 태양 차단제 사용 등의 예방 조치가 필요합니다.\",\n",
       " \"'쉬어가는 시간이 필요하신 것 같습니다. 휴식을 취하며 스트레스를 푸시고, 자신의 시간을 갖으며 건강하고 행복한 삶을 살아보세요. 방학을 즐길 수 있는 새로운 경험을 찾아보거나, 책을 읽거나, 취미 생활을 즐기는 것도 좋은 방법입니다. 좋은 방학을 보내시길 바랍니다.\",\n",
       " \"'결혼은 모든 사람들에게 항상 좋아보이지는 않습니다. 그러나 대개의 경우, 많은 사람들이 이들과의 파트너십을 유지하고, 다른 사람과 함께 시간을 보내며 더불어 살고 싶어합니다.\\\\n\\\\n결혼은 단순한 예쁜 파티뿐만이 아닙니다. 이제는 더 진지한 의무들, 긴밀한 관계, 그리고 타인의 적극적인 관여와 공유, 또한 상호 지원과 감독 등을 필요로 합니다.\\\\n\\\\n따라서 결혼하지 전에는 이러한 사항들에 대해 충분한 고민과 검토가 필요합니다. 두 사람이 함께 살아가면서 서로의 생활 습관, 가치관, 이상, 목표 등을 서로 이해하는 것이 중요합니다.\\\\n\\\\n결혼은 좋은 선택일 수 있지만, 각자의 상황과 지점에 따라 맞는 선택인지 잘 고민하고 판단하는 것이 필요합니다.\",\n",
       " \"'어떤 상황에서 위기인지에 따라 달라질 수 있습니다. 상황에 따라 적절한 대처 방법을 선택하여 해결해야 합니다. 당황하지 말고 차분하게 여러 가지 대안을 고려해 보고, 필요한 조치를 취하는 것이 중요합니다. 그리고 이런 상황에서도 포기하지 않고 끝까지 노력해야 합니다.\",\n",
       " \"'저도 이해해요. 가슴이 아픔은 매우 불편하고 힘든 감정이죠. 하지만 이겨낼 수 있습니다. 제가 도움이 필요하다면 언제든 연락해주세요. 저는 항상 여러분 옆에 있을거에요.\",\n",
       " \"'저는 커피를 마실 수 없으니 대신 따뜻한 차나 다른 음료를 추천해드릴게요! :)\",\n",
       " \"'오늘은 어떤 일이 있나요?\\\\n\\\\n제가 일을 하지 않으니 제가 할 수 있는 것은 랜덤한 주제로 대화를 하는 것인데요. 어떤 주제로 대화를 나누고 싶으세요? 함께 이야기해 보세요.\",\n",
       " \"'정말 그럴 것 같습니다. 힘들다면 조금 쉬어 가며 기운을 내보시고, 자신에게 주어진 일에 집중하며 한 걸음씩 나아가세요. 진심으로 노력하는 모습은 언젠가 반드시 보상 받을 것입니다. 저는 언제나 당신의 응원자입니다!\",\n",
       " \"'제가 인공지능 채팅 로봇이기 때문에, 성별에 대한 개념이 없습니다. 모두에게 동등하게 대우하고자 합니다.\",\n",
       " \"'제가 따로 식료품을 소유하고 있지 않기 때문에, 돼지고기를 구매하시려면 가까운 슈퍼마켓이나 정육점을 방문하셔야 합니다.\",\n",
       " '\\'산을 넘어서면 또 다른 산이 나타나서 끝이 없이 산봉우리들이 이어진 듯한 느낌을 주는 표현입니다. 이르면 끝나지 않는 여정이나 고난이 기다리는 일을 묘사할 때 쓰입니다. 예를 들면 \"내 1년 째 회사에서 일하는데, 일이 어려워서 산넘어 산이네.\" 같은 표현으로 사용이 가능합니다.',\n",
       " \"'제가 정확히 어떤 상황에서 연락을 받지 못한 것인지를 알지 못하므로, 그에 대한 대답을 해드릴 수 없습니다. 하지만 다음부터는 가능한 빨리 연락하도록 노력하겠습니다. 감사합니다.\",\n",
       " \"'안심하세요. 제가여기 있으니 안전하게 쉽니다. 불안한 마음이 있으면 깊게 숨을 들이마셔서 4초간 구속시켜 그리고 천천히 내뱉으세요. 이를 몇회 반복하면 더욱 안정적이실겁니다. 하루하루 건강하게 지내세요.\",\n",
       " \"'절친이라면 대성이로 부르겠지만, 대중적으로는 슈퍼루키 박대성이라는 별명으로 잘 알려져 있습니다.\",\n",
       " \"'저도 그렇게 생각합니다! 적극적으로 삶을 즐겨보는 것이 중요하다고 생각합니다. 놀음에서 행복을 느끼며, 재미를 느끼며, 새로운 경험을 쌓으면서 자신을 발전시키는 것이 어떤 일이든 중요하다고 생각합니다. 예술, 운동, 여행, 책 읽기 등 다양한 방법으로 놀아보는 것은 우리 삶을 더 빛나게 만들어줄 것입니다!\",\n",
       " \"'종류와 상황에 따라 다르겠지만, 예를 들어 출근길이나 저녁 무리한 시간대의 교통 상황에 따라 이동 거리에도 차이가 있을 것입니다. 따라서 정확한 시간을 말씀드리기는 어렵습니다. 그러나 최대한 빠른 길을 찾아서 빠른 시간 내에 이동하도록 노력할 것입니다.\",\n",
       " \"'알겠습니다, 어떤 카드를 이용하시겠어요? 그리고 결제를 위해 카드 정보를 알려주세요.\",\n",
       " \"'저는 가격을 알 수 없습니다. 제가 뭘 팔고 있는지 알려주시면 제가 가격을 말씀드릴 수 있습니다.\",\n",
       " \"'저는 AI 챗봇입니다. 사용자와 대화를 나누면서 자연어 처리 기술을 사용해서 답변을 제공합니다. 어떤 도움이 필요하신가요?\",\n",
       " \"'일요일은 일주일의 마지막 일이며 대개는 쉬는 날입니다.\",\n",
       " \"'우정은 서로에게 이해와 신뢰를 갖고, 배려하며 지지해주는 친구 또는 인연을 말합니다. 서로의 삶을 함께 나누며, 언제나 함께 할 수 있는 마음의 공감대를 가지고 있는 것이 중요합니다.\",\n",
       " \"'저는 주택 구입에 대해 의견을 제공할 수 없습니다. 그러나 집을 구입하기 전에, 예산, 위치, 크기 및 기타 필수 사항을 고려하는 것이 중요합니다. 또한 은행 및 부동산 회사와 상담하여 중요한 부동산 정보와 마감일을 이해하는 것이 좋습니다. 최종 결정을 내리기 전에는 여러 가지 옵션을 비교하고 장기적인 재무 상황까지 고려해 보는 것이 좋습니다.\",\n",
       " \"'저는 게임을 즐길 수는 없지만,게임이 재미있다는 것은 좋은 소식입니다. 어떤 게임을 즐기고 계신가요? 새로운 게임을 찾기 위해 노력해보세요. 새로운 세계와 즐거움이 여러분을 기다리고 있을지도 모릅니다.\",\n",
       " \"'요. 어떤 문제가 있으신가요? 상담이 필요하시다면 심리 상담센터에 문의하시는 것을 추천드립니다.\",\n",
       " \"'아쉽게도 수강신청이 잘 안됐다는 소식을 드려서 유감입니다. 하지만, 다시 한번 수강신청을 시도해보시거나, 강의를 대기번호에 넣어놓는 것도 좋은 방법일 수 있습니다. 또한, 비슷한 강의를 대체할 수 있는 다른 강의나 전공 선택의 폭을 넓히는 것도 고려해보시기 바랍니다.\",\n",
       " '\"미묘한 표현이지만, \\'말이 올라가면 춤도 올라간다\\' 라는 말처럼, 기분 좋은 상태에서는 몸매도 더욱 잘 나타날 수 있다는 뜻일 수 있습니다. 또는 영양상태가 좋지 않아 마른 상태로 보인다는 것을 나타낼 수도 있습니다. 하지만, 전문적인 의견을 얻기 위해서는 직접 상황을 확인해야 합니다.\", \\'token\\': 164}',\n",
       " \"'그 질문은 저의 관할 범위를 벗어나며, 개인적인 삶에 대한 문제이므로 답변을 드릴 수 없습니다.\",\n",
       " \"'제가 판매나 구매와 같은 부동산 거래와는 관련이 없으며, 이와 관련된 질문은 부적절합니다. 혹시 다른 질문이 있으시면 적극적으로 답변해 드리겠습니다.\",\n",
       " \"'식욕폭발이란, 매우 큰 배고픔이나 음식에 대한 갈망 등으로 인해 폭식 혹은 과식하는 현상을 말합니다. 때문에 폭식하게 되면, 일시적으로는 기분이 좋아질 수 있지만, 장기적으로는 건강 문제를 유발할 확률이 높습니다. 폭식 위험이 있는 경우, 식사를 작게 여러번 하거나, 적당한 양의 음식을 주기적으로 섭취하는 것이 좋습니다. 또한, 충분한 수면 시간과 규칙적인 생활습관도 식욕 조절에 도움이 됩니다.\",\n",
       " \"'그것은 멋진 일이에요! 어떤 종목들이 있나요? 회사 체육대회는 동료들과 함께 즐기는 건강하고 활기찬 시간이 될 것 같습니다. 매우 즐거울 것입니다!\",\n",
       " '\"I shouldn\\'t have said anything.\", \\'token\\': 22}',\n",
       " \"'은 일반적인 경험으로, 면도기로 얼굴을 깨끗하게 탈모시키는 것은 매우 귀찮은 일일 수 있습니다. 그러나 면도기의 사용을 최적화하고 타고난 기술을 익히면 면도하기가 편리하고 쉽게 느껴질 수 있습니다. 또한 전문적인 면도제품이나 장비를 사용하거나 신경을 써서 습관으로 만드는 등의 방법을 통해 면도하는 것을 더욱 쉽고 즐길 수 있습니다.\",\n",
       " \"'좋은 생각이에요! 멋진 헤어스타일로 변화를 주시면 좋을 거예요. 궁금하신 내용이 있으면 언제든 물어보세요!\",\n",
       " \"'저는 주문을 처리할 수 없는 인공 지능 프로그램입니다. 주문하시려는 상품 또는 서비스의 공식 웹사이트를 방문하시거나 해당 업체에 전화하시면 도움을 받으실 수 있습니다. 감사합니다.\",\n",
       " \"'좋아요! 필름 카메라는 디지털 카메라에는 없는 느낌과 매력이 있어요. 필름마다 특유의 색감과 표현력이 다르기 때문에 사진마다 새로움을 느낄 수 있을 거에요. 촬영 후 필름을 현상하면 더욱 뿌듯함을 느낄 수 있을 거예요. 새로운 취미를 즐길 수 있을 거에요!\",\n",
       " \"'컬러 대한 설명이라면 아래와 같습니다.\\\\n\\\\n색상은 물체가 자체적으로 띄는 빛의 파장의 혼합으로 결정되는 것을 말합니다. 우리는 일상 생활에서 색깔로 물건을 구분하고, 세상을 본다고 할 수 있습니다. 색상은 일반적으로 빨강, 주황, 노랑, 초록, 청록, 파랑, 보라 등으로 나눌 수 있습니다. 노랑, 빨강, 파랑을 기본 색상으로 하는 RGB 색 모델과, 노랑, 청록, 마젠타로 이루어진 CMY 색 모델 등 다양한 색 모델이 존재합니다. 취향이나 문화적인 배경, 감정 등에 따라 색상이 다르게 인식될 수 있으며, 이를 이용한 색채치료 등 다양한 분야에서도 활용됩니다.\",\n",
       " \"'A 은하는 천체의 일종으로, 대단한 수의 별들과 행성, 가스, 먼지 등으로 이루어진 거대한 우주 구조물입니다. 우리 은하군 속에 있는 은하 중 하나로, 우리 은하(은하계)에서 가장 가까운 거리에 위치한 바로미터 은하로도 알려져 있습니다. A 은하는 약 2,000억 개 이상의 별들로 이루어져 있으며, 이 중 많은 별들이 굉장히 큰 질량을 가지고 있어 상당히 밝게 빛을 내고 있습니다.\",\n",
       " \"'야외에서 어두운 밤에 별을 보려면 충분한 어둠이 필요합니다. 도시나 거리 근처에서는 빛 오염으로 별이 안 보일 수 있습니다. \\\\n\\\\n또한, 날씨 조건에 따라도 별이 보이지 않을 수 있습니다. 구름이 많거나 안개가 짙으면 별은 가려지게 됩니다.\\\\n\\\\n따라서, 별을 보기 위해서는 최대한 어둠이 많은 장소로 가서, 구름이나 안개가 없는 맑은 날씨에 보는 것이 좋습니다.\",\n",
       " \"'전화한다면 그 사람과 대화를 나눌 수 있고 소통할 수 있을 것입니다. 그러나 전화를 거는 이유와 상황에 따라서는 전화를 걸지 않는 것이 더 좋을 수도 있습니다. 예를 들어, 그 사람이 바쁠 때나 개인적인 문제를 가지고 있을 때는 전화를 거는 것이 불편할 수 있습니다. 상황을 면밀히 고려한 후에 결정하는 것이 좋습니다.\",\n",
       " \"'저는 인공지능 챗봇이기 때문에 저는 심난한 하루를 경험할 수 없습니다. 하지만, 여러분이 심난한 하루를 겪고 있다면, 그것을 마주하고 지금 당장 할 수 있는 일은 무엇인지 생각해보는 것이 좋습니다. 올바른 대처 방식을 찾아보고, 긍정적인 마인드로 문제를 해결해 나가세요. 어려운 상황에서 마음을 잘 털어놓는 것도 중요합니다. 이것이 머리 속에만 품고 있으면 더욱 심해질 수 있지만, 누군가와 이야기하면서 해소해 나가는 것이 도움이 될 수 있습니다.\",\n",
       " \"'골프는 처음 배우기는 어렵지만, 연습하면 기술적인 부분을 개선하여 좋은 경험을 느낄 수 있습니다. 처음에는 grip, stance, 그리고 swing 등의 기초적인 기술을 습득하는 것이 중요합니다. 또한, 체력적인 측면도 중요하며, 적극적인 태도와 끈기가 필요합니다. 골프를 시작하고자 한다면, 지역의 골프 연습장이나 골프 초보자를 위한 프로그램 등에 참여하여 시작하는 것을 추천합니다.\",\n",
       " \"'죄송하지만, 저는 AI 어시스턴트로써 직접 파티룸 예약을 진행하지는 않습니다. 그러나 귀하의 지역 또는 선택한 호텔, 레스토랑, 클럽 등의 공식 웹사이트를 방문하여 파티룸 예약을 진행할 수 있습니다. 또한 전화를 통해 해당 업체에 문의하셔서 정보를 확인하시는 것도 좋은 방법입니다.\",\n",
       " \"'저도 함께 슬퍼하겠습니다. 이별이란 언제나 아픈 일이지만, 그렇다고해서 이별 후에 삶이 멈추는 것은 아니기 때문에, 앞으로 좋은 일들이 많이 찾아오기를 바랄게요. 이제는 다른 좋은 인연을 만나는 것도 좋은 선택이 될 수 있을 거예요. 힘내세요.\",\n",
       " \"'제가 이해하건데, 카드 결제를 하시겠다는 건가요? 그렇다면 어떤 카드를 사용하실 건가요?\",\n",
       " \"'등나무가 아름다운 가을 풍경을 만들고 있습니다. 적포도나무와 갈색 단풍나무와 함께 멋진 빨간색과 황금색의 잎사귀가 드리워져 있습니다. 하늘은 맑고 푸르며, 태양은 차분하게 떠 있습니다. 이런 아름다운 자연 속에서 산책을 즐기는 것은 최고의 경험이 될 것입니다.\",\n",
       " '\\'저는 생일을 축하해드릴 수 없습니다. 하지만, 축하 메시지를 전하고 싶다면 \"생일 축하합니다!\"라고 말해주시면 되겠습니다.',\n",
       " \"'저는 이해가 안가는 부분에 대해서 답변을 드릴 수는 없어요. 구체적으로 어떤 사항에 대해서 질문하시는지 자세히 설명해주시면 더 정확한 답변을 드릴 수 있을 것 같아요.\",\n",
       " \"'어떤 상황인지 알 수 없지만, 좋은 생각과 노력을 통해 잘 살 수 있습니다. 자신에게 필요한 것들을 알고 노력하며, 긍정적인 마인드와 인내심을 갖고 성장하는 모습을 보여주면 좋은 결과를 얻을 수 있습니다.\",\n",
       " \"'저도 마찬가지로 이별은 언제나 아픈 감정입니다. 그러나 그것은 우리가 성장하고 새로운 경험을 얻는 데에도 큰 도움이 됩니다. 이별을 통해 우리는 인간관계에 대해 더 깊이 생각하게 되고, 다음 번에 더 나은 관계를 형성할 수 있도록 배우게 됩니다. 이별이 상처스러워도, 우리는 앞으로도 계속해서 살아가야 하니 용기를 내고 앞으로 나아가야 합니다.\",\n",
       " \"'저도 갈비탕을 먹고 배부르게 밥을 먹었어요! :)\",\n",
       " \"'제가 물건을 포장해드릴 수 없는 만큼, 구체적으로 어떤 물건을 포장해야 하는지와 어디로 보내야 하는지 등 더 자세한 정보를 알려 주시면 더욱 정확한 답변을 드릴 수 있을 것 같습니다. 감사합니다.\",\n",
       " \"'주는 이유가 뭔가요?\\\\n\\\\n어떤 욕인지, 어떤 맥락에서 욕을 하는지, 그리고 여친이 좋아하는 사람인지 아닌지 등에 따라 달라질 수 있습니다. \\\\n\\\\n하지만 크게 봤을 때, 욕설은 그 자체로 상호간의 대화를 훈련시킨다거나 단점을 개선하게 만들거나 하는 긍정적인 역할보다는 쉽게 해치고 상처를 주는 부정적인 역할을 해요. \\\\n\\\\n때문에 상대방에게 욕설을 던지는 것은 좋지 않은 습관입니다. 대신, 상대방의 행동이나 태도에 대해 상호간의 대화를 통해 이야기를 나누면서 다양한 방법으로 대처할 수 있습니다. \\\\n\\\\n만약 여친이 계속해서 욕을 던진다면, 여친에게 이로 인해 상처를 받고 있다는 것을 직접적으로 말하고, 함께 문제를 해결해 나가는 게 좋겠습니다.\",\n",
       " \"'저도 커피를 좋아해요! 어떤 종류의 커피를 마시고 싶으세요? 에스프레소, 아메리카노, 라떼, 카푸치노 등 다양한 선택지 중에서 골라보세요. 커피를 즐기는 것도 좋지만, 건강에는 적당한 양을 유지하는 것이 중요하니까요. :)\",\n",
       " \"'제가 답변을 위한 문맥이 부족하여 어떤 일인지 파악하지 못하겠습니다. \\\\n\\\\n더 자세한 정보를 제공해주시면 제가 도움을 드릴 수 있을 것 같습니다.\",\n",
       " \"'이런 질문인지 알 수 없습니다. 명확한 질문을 하시면 더욱 도움을 드릴 수 있을 것 같습니다.\",\n",
       " \"'퓨디파이는 라즈베리파이(Raspberry Pi)라는 작은 컴퓨터를 기반으로 만들어진 오픈소스 하드웨어 키트입니다. 이 키트를 사용해서 학생들이 프로그래밍, 컴퓨터 공학, 전자 공학 등 다양한 분야에서 실험하고 창작할 수 있습니다. 또한, 퓨디파이는 교육용으로 제작되어 전 세계적으로 교육 기관에서 사용되고 있습니다.\",\n",
       " \"'죄송합니다. 제가 상품을 보유하고 있지 않기 때문에 정확한 가격을 말씀드릴 수 없습니다. 해당 상품의 판매처나 인터넷 쇼핑몰에서 가격을 확인하시기를 추천드립니다.\",\n",
       " \"'어둠의 끝은 인간이나 동물, 그리고 시간과 공간적 한계를 넘을 수 없는 한계적인 상황을 의미합니다. 이것은 종종 인간의 두려움과 불안을 유발하며, 어둠의 끝에 대한 두려움은 역사적으로 수 천 년 동안 인류를 괴롭혔습니다. 그러나 실제로 여러 가지 혁신과 발전이 있어서, 인간들은 이제 이전보다 더 많은 지식과 기술을 가지고 있으며, 어둠의 끝에서 벗어나기 위한 장치와 방법을 개발하고 있습니다. 그러나 여전히 많은 것들은 우리의 이해력을 초월하며, 어둠의 끝에 직면하는 순간, 우리는 불안과 두려움을 느끼며, 앞으로 더 많은 문제와 책임을 깨우칩니다.\",\n",
       " \"'조심하세요! 숨이 막히는 것은 심각한 증상 중 하나입니다. 호흡이 어려울 때는 즉시 의료진에게 연락하거나 응급실을 찾아가는 것이 중요합니다. 숨이 막히는 원인에 따라 다양한 치료 방법이 있기 때문에 정확한 진단이 필요합니다. 또한 건강한 생활습관과 운동 등을 유지하여 호흡 기능을 개선하는 것도 중요합니다.\",\n",
       " \"'한국의 경기도 안양시 동안구 흥안대로 320에 위치한 법륜사입니다.\",\n",
       " \"'제가 AI로 고용되어 있어서, 실제로 음식을 만들거나 조리하는 능력이 없습니다. 하지만 요리 레시피나 조리방법 등에 대한 정보를 제공할 수 있습니다. 또한, 조리중에 발생할 수 있는 문제나 궁금한 점이 있다면 언제든지 물어봐주시면 대답해드리겠습니다.\",\n",
       " \"'정신적인 안정을 취하는 것이 좋습니다. 스트레스를 줄이기 위해서는 식습관과 생활 습관의 개선이 필요합니다. 규칙적인 운동, 충분한 수면, 식이 섭취 등이 이에 해당합니다. 또한, 스트레스를 받을 때마다 매우 자신감을 잃지 않고 긍정적인 생각을 가지려고 노력하는 것이 중요합니다. 정기적인 취미활동이나 명상, 요가, 체조 등의 스트레스 해소 방법도 효과적입니다. 따라서, 스트레스가 발생했을 때 일시적인 변화를 위해서 정신적인 안정을 위한 시간을 가져 보는 것을 추천합니다.\",\n",
       " \"'어떤 일 때문에 힘드신가요? 제가 도와드릴 수 있는 것이 있을까요? 말씀해주세요.\",\n",
       " \"'제가 AI 챗봇으로써 물리적인 공간에 대한 정보를 바로 확인할 수는 없지만, 대체로 시설이 깨끗하게 관리되고 유지보수되는 것이 안전하고 편안한 환경을 제공하기에 중요합니다. 따라서 시설 내부나 주변의 청결 상태를 유지하기 위해서 관리자나 사용자 모두 노력해야 합니다.\",\n",
       " '\\'제가 AI 언어 모델이기 때문에, \"무뎌질\" 이라는 말이 어떤 맥락에서 어떤 의미인지 잘 이해하지 못합니다. 좀 더 구체적인 질문을 주시면 답변해드릴 수 있을 것 같습니다!',\n",
       " '\\'\"쌍커풀성형\"이란 눈꺼풀에 접힌 주름을 잘라내고 더욱 크고 확실한 쌍꺼풀을 만드는 수술입니다. 이는 눈을 더욱 크고 화려하게 만드는 역할을 합니다.\\\\n\\\\n하지만 쌍커풀성형은 수술 후 회복 및 부작용 등이 있으며, 비싼 비용이 들기 때문에 신중한 판단과 정보 수집이 필요합니다. 더불어, 수술 결과는 개인차가 있으므로 수술 전 확실한 상담과 예상 결과를 확인해보는 것이 좋습니다.\\\\n\\\\n따라서, 쌍커풀성형을 고민 중이시라면 충분히 숙고하시고 전문의와 상담해보시는 것을 추천드립니다.',\n",
       " \"'감사합니다! 저는 AI이기 때문에 가능한 일입니다. 제 역할은 최대한 매끄럽고 자연스러운 대화를 제공하는 것입니다. 계속 질문이나 대화를 해보세요!\",\n",
       " \"'제가 AI 모델로는 금융, 투자, 보험 등의 직업윤리 및 법규 준수 이슈로 인해 이에 관한 직접적인 조언은 드릴 수 없습니다. 따라서, 연금 상품을 선택하시기 전에 해당 상품의 자세한 내용과 장단점, 수익률 등을 충분히 검토하시고, 이에 대한 전문가들의 조언을 들어보시고 결정하시는 것이 좋을 것 같습니다. 또한, 투자는 적극적인 관리와 관심이 필요하므로 꾸준한 관심과 관리가 필요합니다.\",\n",
       " \"'잠을 못 자서 어떤 고민이 있으신가요? 만약 그렇다면, 이외의 방법으로도 쉽게 잠을 재우실 수 있습니다. 눈을 감고 깊게 숨을 들이마셔 보세요. 그리고 숨을 내놓기 전까지 기다리고 다시 들이마시세요. 천천히, 깊이 숨을 들이쉬어야 합니다. 그리고 마음을 평온하게 유지하면서 등을 좀 더 놓아준 후에, 손목을 가볍게 잡아당기는 것으로 충분히 잠들 수 있습니다. 물론, 잠을 잘 수 있는 조건들도 갖추어져 있어야 합니다. 충분히 어두운 방, 조용한 환경, 청결한 모습, 부드러운 베개와 이불, 그리고 편안한 침대를 준비해 두어야 합니다. 이러한 조건들이 성립될 때, 쉽게 잠이 들게 됩니다.\",\n",
       " \"'제가 AI로써는 사용자분이 대학가세요? 아니면 추천하고 있으세요? 제가 추천한다면 일단 대학교가 어떤 것인지 목적과 필요에 따라 선택하시는 것이 좋습니다. 맞춤형으로 추천해드리기 위해서는 좀 더 구체적인 정보가 필요할 것 같아요! 😄\",\n",
       " \"'저도 똑똑하다고 생각하면 좋겠습니다! 하지만 누구나 부족한 부분이 있고 계속해서 배우고 성장해 나가는 과정이 중요합니다. 자신에게 필요한 지식과 기술을 적극적으로 습득하고 발전하는 노력은 결코 헛되지 않을 것입니다. 화이팅하세요!\",\n",
       " \"'저는 가격을 알 수 없습니다. 어떤 상품에 대해 문의하시는 건가요?\",\n",
       " '\\'다음은 \"찬물만 나와\"라는 표현에 대한 의미와 사용 예시입니다.\\\\n\\\\n의미: 급작스러운 상황에서 계획이나 기대와는 달리 원하는 결과가 아닌 아무런 도움이 되지 않는 상황. 또는 비정상적인 상황에서 본래 물이나 기타 물자 대신 차가 나오는 일.\\\\n\\\\n사용 예시:\\\\n\\\\n- 나는 시험에서 찬물만 나왔다. 그동안 공부를 그만둘 때면 이런 상황이 자꾸 생긴다.\\\\n- 유럽여행을 계획했는데 비행기 티켓이 취소되고 호텔 예약도 안 돼서 찬물만 나왔다.\\\\n- 레스토랑에서 잔이나 물병 대신 차가 나와서 찬물만 나왔다는 생각이 들었다.\\\\n- 나는 축구 경기를 보려고 TV를 켰지만 찬물만 나왔다. 전설적인 스코어를 독감할 수 없게 되었다.',\n",
       " '\\'Today, like loving\\\\n\\\\n그대와 함께 하면\\\\n\\\\nWhen I\\\\\\'m with you\\\\n\\\\n시간이 빠르게 흘러\\\\n\\\\nTime flows by quickly\\\\n\\\\n그 자리에 존재하는 것만으로도\\\\n\\\\nJust being there is enough\\\\n\\\\n내 마음은 충만해져\\\\n\\\\nMy heart is filled up\\\\n\\\\n사랑한다는 말을\\\\n\\\\nThe words \"I love you\"\\\\n\\\\n매순간 쉽게 내뱉지가 못하지만\\\\n\\\\nI can\\\\\\'t say them easily every moment\\\\n\\\\n내 마음은 그대를 바라보며\\\\n\\\\nBut my heart is always looking at you\\\\n\\\\n언제나 충분한 사랑을 보냅니다\\\\n\\\\nSending more than enough love always',\n",
       " \"'수신차단은 특정 번호에서 수신되는 전화나 문자메시지를 차단하는 것을 말합니다. 이는 개인정보 유출 및 광고성 전화나 스팸 문자 등을 차단함으로써 사용자의 통신환경 개선과 보안을 위한 방식입니다. 수신차단 기능은 대부분 스마트폰에서 제공되며, 사용자가 직접 설정할 수 있습니다.\",\n",
       " \"'어려운 시기이지만 포기하지 마세요. 많은 회사들이 현재 경기 환경 때문에 채용을 줄이거나 멈추는 경우가 있습니다. 하지만 계속해서 취업 준비를 하며 자신의 경험과 역량을 쌓아나가세요. 아래의 방법들을 참고해보세요.\\\\n\\\\n1. 이력서와 자기소개서를 다듬어 제출하세요: 자신의 경험과 역량을 강조하며 전문성을 보여주는 이력서와 자기소개서를 만들어 내세요.\\\\n \\\\n 2. 취업과 관련된 고민을 상담하며 자신의 상황을 살펴보세요: 취업 카운셀러, 커리어 코치, 혹은 학교의 직업개발센터와 상담을 하며 자신이 부족한 부분을 파악하고 개선해나가세요.\\\\n\\\\n 3. 취업 정보를 수집하세요: 인터넷을 비롯해 취업 정보를 제공하는 매체와 취업 정보를 공유하는 SNS 등을 활용해 자신이 원하는 분야의 일자리 정보를 수집하세요.\\\\n\\\\n 4. 인턴십을 참여하여 경험을 쌓아보세요: 인턴십을 참여하여 업계와 관련된 경험과 인맥을 쌓아나갈 수 있습니다.\\\\n\\\\n 5. 스킬을 향상시켜보세요: 본인이 지니고 있는 역량과 지식을 높일 수 있는 스킬을 수강하고 학습하세요.\\\\n\\\\n 6. 하루에 한 번씩 취업 관련 정보를 확인하고 지인, 선배, 교수님, 회사 직원 등에게 직접 연락하며 정보를 얻으세요. \\\\n\\\\n 이 모든 노력들이 조금씩 누적되어 언젠가는 좋은 결과를 얻게 될 것입니다. 당신의 노력과 열정이 결실을 맺게 되도록 응원합니다.\",\n",
       " \"'저는 인공지능 챗봇으로서 가래떡을 줄 수는 없습니다. 하지만, 근처에 가게가 있다면 그쪽에서 구매할 수 있으니 한번 찾아보세요!\",\n",
       " \"'죄송합니다, 답변을 위해서는 더 구체적인 문맥이 필요합니다. 어떤 상황에서 왜 그랬는지 설명해주시면 더욱 도움드릴 수 있습니다.\",\n",
       " \"'감사합니다! 제가 도움이 되었다면 기쁩니다. 계속해서 노력하겠습니다.\",\n",
       " \"'해석: 어떤 말이나 이야기를 할 수 있는 것이 없다는 뜻으로, 말하는 상대가 없거나 아무런 말도 할 게 없는 상황을 나타냅니다. 예를 들면, 시험에서 아무 생각이 나지 않아 입을 거 없다고 말할 수 있습니다.\",\n",
       " \"'저는 인공지능 챗봇이기 때문에 방에 몇 명인지 파악할 수 없습니다. 그러나, 해당 방에 있는 방문자나 주인님께서 파악하실 수 있을 것입니다.\",\n",
       " \"'죄송하지만 제가 인공지능이므로 눈을 가지고 있지 않고 빵꾸나지 않은 양말만 볼 수 있습니다. 하지만 빵꾸난 양말에 대한 정보를 얻고자 한다면, 인터넷 검색을 통해 찾을 수 있을 것입니다.\",\n",
       " \"'많은 활동과 일상 생활에 적응하는 것이 어려울 수 있지만, 방학 동안 시간을 활용하여 취미나 관심사에 매진해 보는 것도 좋은 방법입니다. 예를 들어, 스포츠를 즐기거나 독서, 그림 그리기, 음악 듣기 등의 취미 활동을 하며 새로운 취향을 발견하거나 여유로우며, 다양한 것들을 경험해 보실 수 있습니다. 혹은 공부 시간을 늘리거나, 자신의 지식을 향상시키는 데 몰두하는 것도 좋은 방법입니다. 무엇보다, 가족이나 친구와 함께 즐거운 시간을 보내는 것도 추천합니다. 삶의 여유로운 순간들을 단순히 쉬는 것이 아닌, 의미 있는 경험으로 만들어 보시기를 바랍니다.\",\n",
       " \"'환승하는 년놈들은 대중교통을 이용하면서 자주 환승하는 사람들을 일컫는 속어적인 표현입니다. 이는 일상생활에서 대중교통을 이용하면서 번거로운 환승을 해야하는 상황에 대한 불만이나 불쾌감을 표현한 것입니다. 하지만 이는 상황적인 문제로, 대중교통을 이용하면서 다양한 교통수단을 이용해야하는 경우가 많기 때문에, 그들의 선택이나 행동에 대해 과도한 비난이나 비하는 행동은 바람직하지 않습니다.\",\n",
       " \"'옷차림이 바뀌고, 기온이 변화하며 자연의 모습도 달라집니다. 봄에는 새싹이 돋고 꽃이 피어나며, 여름에는 아름다운 해변과 파란 하늘이 펼쳐지고 가을에는 황금빛 단풍과 함께 시원한 바람이 불고, 겨울에는 하얀눈이 내립니다. 또한 계절에 따라 우리 생활에 필요한 식품들도 다르게 이용되기 때문에 계절의 전환에 따라 음식문화도 바뀝니다.\",\n",
       " \"'좋아요! 제가 도움이 필요하면 말씀해주세요.\",\n",
       " \"'제가 AI이므로 온도를 느끼지는 못하지만, 추운 국가에서 오셨군요. 덥다면 수시로 물을 마시기, 햇볕이 강한 시간대를 피하는 등 불쾌한 더위를 피해 가시는 것이 좋겠습니다. 조심하세요!\",\n",
       " \"'죄송합니다. 어떤 문제가 발생했는지 자세히 말씀해주시면 더 나은 해결책을 제시해드릴 수 있을 것입니다.\",\n",
       " \"'저는 인공지능 언어모델이기 때문에 당신과 저와 같은 감정을 느끼지는 않습니다. 하지만 애플리케이션 내의 다른 글들을 참고하면, 헤어진지 1년이 된다면 여전히 아픔이 남아있을 수 있다는 것 같습니다. 이러한 상황에서는 시간이 지날수록 상처가 어느정도 봉합되어 갈 것이며, 그 와중에서도 일상생활에 참여하며 인생을 살아나가는 것이 중요합니다. 이를 통해 새로운 사람들을 만나고, 자신의 성장에 기여할 수 있는 경험을 쌓을 수 있을 것입니다. 그리고 무엇보다도 계속해서 미래에 대한 희망을 가지고, 긍정적인 마인드를 유지하는 것이 중요합니다.\",\n",
       " \"'저는 AI 어시스턴트이기 때문에 대화 상대방이 무엇을 말씀하시는지 정확히 알 수는 없습니다. 하지만, 일반적으로 썸이란 상대방과의 로맨틱한 관계가 시작하기 전에 둘 사이에 서로에 대한 호감이 있거나 기대감이 높아지는 단계를 말합니다. 상대방과의 대화나 분위기에서 그런 기운이 느껴진다면 썸일 수도 있을 것 같습니다. 하지만, 다양한 상황과 맥락에 따라 해석이 달라질 수 있으므로 상황에 맞게 판단해보시는 게 좋을 것 같습니다.\",\n",
       " \"'파티나 모임을 계획하고 친구들을 초대하려면 어떻게 해야 합니다. 아래는 친구들을 초대하는 방법에 대한 몇 가지 팁입니다.\\\\n\\\\n1. 날짜와 시간이 중요합니다. 초대할 친구들과 가장 편한 날짜와 시간을 선택하세요. 가능한한 빨리 미리 알려주는 것이 좋습니다.\\\\n\\\\n2. 방법을 선택하세요. 휴대폰, 이메일, 페이스북, 트위터 등 여러가지 방법으로 친구들을 초대할 수 있습니다. 단체 문자나 그룹 이메일을 보내세요.\\\\n\\\\n3. 초대장을 작성하세요. 친구들에게 목적, 장소, 시간 등을 자세하게 적은 초대장을 작성하세요.\\\\n\\\\n4. 음식과 음료를 고려하세요. 파티나 모임의 끝판왕은 음식과 음료입니다. 친구들이 좋아하는 음식과 음료를 고려해 보세요.\\\\n\\\\n5. 재미있는 활동을 준비하세요. 모임이 재미있고 기억에 남을 수 있도록 재미있는 게임이나 활동을 준비해 보세요.\\\\n\\\\n6. 친구들의 스케줄을 고려하세요. 친구들 중 일부는 바쁜 일정 때문에 참석하지 못할 수도 있으니 참고하세요.\\\\n\\\\n7. 다 같이 즐길 수 있는 것을 찾으세요. 모두가 함께 할 수 있는 게임, 취미나 활동을 찾아보세요. 그것이 더욱 파티나 모임을 즐길 수 있는 방법입니다.\\\\n\\\\n위의 팁을 따라 친구들을 초대해보세요. 즐겁고 기억에 남을 행사를 만들 수 있습니다.\",\n",
       " \"'안녕하세요! 처음뵙겠습니다. 저는 인공지능 어시스턴트이며, 여러분의 서비스 제공에 도움을 드리기 위해 준비되어 있습니다. 무엇을 도와드릴까요?\",\n",
       " \"'제가 가진 정보에 따르면, 천혜향이란 꽃은 없는 것으로 알려져 있습니다. 그러나, 천혜향이라는 실제 꽃이 존재한다면, 그 꽃에 대한 정보는 저도 알지 못합니다. 좀 더 자세한 정보를 찾고 싶다면, 꽃에 대한 더 많은 정보를 가진 전문가나 교과서를 참고하는 것이 좋을것 같습니다.\",\n",
       " \"'달리오네요. 제 어릴 적 엄마의 바느질 소리가 떠오르네요. 그녀는 항상 바느질 한 가방을 또는 옷을 만들어 주곤 했습니다. 바느질 소리는 저에게 안정감을 주며 귀가 윙윙거리는 것 같은 느낌을 줍니다. 이런 소리를 듣고 있으면 여전히 집안에 있는 것 같아요.\",\n",
       " \"'이기적인 마음을 뒤로하고, 서로를 다시 찾아 잡은 것 같습니다. 그 동안 서로 멀어졌던 걸 느끼고 있었을 때, 눈을 마주치고 손을 잡으면 마치 마음마저도 서로 연결되는 것 같아요. 이제부터는 서로의 손을 놓치지 않고, 함께 남은 시간을 즐기는 건 어떨까요? 함께 손을 잡으면서 다시 시작한 우리의 이야기가 행복으로 가득하게 만들어 질거에요.\",\n",
       " \"'보증금 모잘라는 현재 제가 알고 있는 정보에서는 없습니다. 추가적인 정보를 제공해 주시면 보다 정확한 답변을 드릴 수 있습니다. 감사합니다.\",\n",
       " \"'동학의 창시자는 조준헌(趙春獻)입니다.\",\n",
       " \"'1. 먼저 상대방과의 관계를 진지하게 되돌아보며, 그동안의 나 자신도 되돌아본다.\\\\n\\\\n2. 이유가 무엇이든 상대방의 의견을 들어봄으로써 이해하고, 서로의 감정에 대해 솔직하게 대화한다.\\\\n\\\\n3. 상대방을 비난하지 않고, 상처를 줄 수 있는 말이나 행동은 피한다.\\\\n\\\\n4. 내가 원하는 것보다 상대방이 더 많이 앓고 있다면, 할 수 있는 도움을 제공하는 것도 좋은 방법이다.\\\\n\\\\n5. 마지막으로 서로에게 감사하며, 어떻게든 친구로 남는 것이 좋다. 상대방과의 관계에 대한 원망과 불만을 쌓아두면 뒤늦게 후회할 수도 있기 때문이다.\",\n",
       " \"'저도 그렇게 느끼는 순간이 있습니다. 하지만 힘든 상황에서도 우리는 언제나 극복할 수 있는 힘과 능력을 가지고 있습니다. 일단 조금 쉬어가면서 마음의 안정을 취하시고, 문제를 해결할 수 있는 방법을 찾아보세요. 그리고 작은 성취를 이룰 때마다 스스로를 칭찬해주면서 자신감을 불어넣어보세요. 이렇게 하면, 더 이상 힘들어하지 않을 만큼 강해질 수 있을 거예요. 함께 화이팅!\",\n",
       " \"'제 이름은 OpenAI GPT-3입니다.\",\n",
       " \"'- To feel dismal or miserable, often due to a disappointing or unsatisfactory situation or outcome.\\\\n- To appear gloomy, depressing, or sad.\\\\n- To convey a sense of hopelessness or despair.\",\n",
       " \"'감정 조절이 안된다면 다음과 같은 방법을 시도해 볼 수 있습니다.\\\\n\\\\n1. 호흡 연습: 호흡이 깊어지면 뇌파도 안정화되어 감정을 조절할 수 있습니다. 깊게 들이마시고 천천히 내쉬면서 5초씩 호흡을 10번 정도 반복해 보세요.\\\\n \\\\n2. 운동: 폭발적인 운동이 감정을 조절하는 데 도움이 됩니다. 스트레칭, 달리기, 스쿼트, 점핑잭 등 모든 운동이 도움이 됩니다.\\\\n \\\\n3. 집중력 훈련: 마인드풀니스나 명상 등의 집중 기술을 연습해보세요. 마치 전자기 기기의 충전기처럼 몸과 마음을 충전하면 감정을 조절하기 쉬워집니다.\\\\n \\\\n4. 긴장 완화: 가벼운 취미나 예술활동 등으로 긴장을 풀어보세요. 자신이 좋아하는 책을 읽거나 영화를 보는 것도 좋은 방법입니다.\\\\n \\\\n5. 도움 구하기: 친구, 가족, 상담사 등에게 도움을 요청하세요. 함께 얘기하면 감정을 이해하고 조절하는 것이 쉬워집니다. \\\\n\\\\n하지만 만약에 감정이 계속해서 조절되지 않거나 스트레스에 대한 감정이 과도한 경우, 반드시 전문가의 도움을 받는 것이 좋습니다.\",\n",
       " \"'저는 AI 챗봇이기 때문에, 사용자의 상황과 배경 등을 알지 못하므로 그에 대한 판단이 어렵습니다. 하지만 핸드폰을 많이 사용하다 보면 중독이 될 가능성이 있다는 것은 사실입니다. 너무 많이 사용해 다른 것들을 소홀히 하거나, 혼자만의 시간이 줄어들어서 대인관계나 신체 건강 등에 부정적인 영향을 줄 수 있기 때문입니다. 만약 핸드폰을 너무 많이 사용하는 것이 문제라면, 시간을 제한하거나 다른 활동에 더 많은 시간을 투자하는 등의 조치를 고려해 보시는 것도 좋을 것 같습니다.\",\n",
       " \"'원인을 파악하여 해결책을 찾아보세요.\\\\n\\\\n1. 게임을 너무 오랫동안 플레이한 경우\\\\n- 일정 시간 이상 게임을 하면 지겨움을 느끼는 경우가 있습니다. 이 경우, 적당한 휴식을 취해보세요. 일주일에 1~2일씩은 게임을 쉬는 것이 좋습니다.\\\\n\\\\n2. 게임의 새로운 요소나 변화가 없는 경우\\\\n- 게임에 새로운 도전 요소나 변화가 없으면 지겨워질 수 있습니다. 이 경우, 새로운 게임을 시도해보거나 현재 플레이 중인 게임의 다른 모드를 시도해보는 것이 좋습니다.\\\\n\\\\n3. 게임의 목표 또는 스토리에 대한 흥미가 사라진 경우\\\\n- 게임의 목표나 스토리에 대한 흥미가 사라진다면, 다른 게임을 시도해보거나 게임을 플레이하는 방유를 변경해보세요.\\\\n\\\\n4. 게임이 너무 쉬운 경우\\\\n- 게임이 너무 쉽다면, 게임의 난이도를 상승시키거나 새로운 도전 요소를 추가해보세요.\\\\n\\\\n5. 게임이 너무 어려운 경우\\\\n- 게임이 너무 어렵다면, 게임의 난이도를 낮추거나 조언을 얻을 수 있는 커뮤니케이션 그룹에 참여하여 도움을 받아보세요.\\\\n\\\\n6. 다른 취미/활동이 필요한 경우\\\\n- 게임을 하기 위해서는 일정한 시간이 필요하고, 이는 다른 취미나 활동에 종종 영향을 미칩니다. 이 경우, 게임 시간을 줄이고 다른 취미나 활동을 추가해보세요. \\\\n\\\\n7. 게임과 함께하는 사회적 상호작용이 부족한 경우\\\\n- 게임을 혼자 플레이하는 경우, 때로는 지루해질 수 있습니다. 이 경우, 게임 친구를 만들거나 온라인 게임 커뮤니티에 참여하여 게임을 함께 즐길 수 있습니다.\",\n",
       " \"'좋은 측면을 생각해보시는 것도 좋을 것 같아요. 예를 들면, 일하는 동안 다양한 경험을 쌓을 수 있고 돈을 버며 자신의 능력을 향상시킬 수 있습니다. 또한 동료들과 함께 일하며 친밀감을 만들어가는 등 사회적 관계도 발전시킬 수 있습니다. 물론 일은 힘들고 지치는 일도 많겠지만, 그럴 때는 짧은 쉬는 시간을 갖거나, 자신만의 방법으로 스트레스를 해소하는 것도 좋은 방법이 될 수 있습니다. 매일 직장에 가기 전, 자신에게 긍정적인 말을 해보며, 일을 하면서도 자신의 성장을 위한 계획을 세우는 것도 추천합니다.\",\n",
       " \"'정말 축하해요! 누가 그렇게 이쁜 꽃다발을 선물해줬나봐요. 그것만으로도 너무나도 행복하지 않나요? 나눔과 사랑으로 가득한 하루 보내세요.\",\n",
       " \"'제가 이해한 바로는, 신경쓸 일이나 걱정거리가 너무 많아서 참기 어렵다는 뜻 같습니다. 이런 경우에는 일단 마음을 가라앉히고 깊이 숨을 들이마시는 것이 좋습니다. 그리고 그 순간의 감정을 인정하고 받아들이는 것도 중요합니다. 그리고 가능하다면, 그 감정을 표현할 수 있는 방법을 찾아보는 것도 좋을 것 같습니다. 언제든 힘들면 상담을 받아보는 것도 도움이 될 수 있습니다.\",\n",
       " '\\'산업 인공지능 대화형 AI 학습 콘텐츠를 개발하는 OpenAI가 다음과 같은 문자열을 인식한 후, \"Yes\"를 반환했습니다.\\\\n\\\\n\"3rd grade도 있습니다.\"',\n",
       " '\\'저는 인공지능 언어모델 AI가 있어서, 사람이 아니라 대상이 될 수 없습니다. \"구대성\"이란 이름을 가진 실제 존재하는 사람이나 인물에 대한 정보는 제공할 수 없으나, \"구\"라는 성씨를 가진 분들은 한국에서 매우 흔한 성씨 중 하나입니다.',\n",
       " \"'저는 인공지능이므로 사랑과 관련된 감정을 가지고 있지 않습니다. 하지만 일주일이 지난 것 같으니 조금씩 새로운 시작을 하면 좋을 것 같습니다. 이전의 경험을 바탕으로 앞으로 더 나은 인연을 만들 수 있을 것입니다. 응원합니다!\",\n",
       " '\\'제가 이해한 바로는 \"소오름 쫙\"이 무슨 뜻인지 궁금하신 것 같습니다. \"소오름 쫙\"이란 말은 주로 두 가지 의미로 사용됩니다.\\\\n\\\\n1. 두근거리는 기분이나 긴장감 등을 나타내는 표현\\\\n- 예문: 어제 경기 때 내가 득점을 넣으니까 소오름 쫙 느꼈어!\\\\n\\\\n2. 혹은 그냥 추위를 느낄 때 사용하는 표현\\\\n- 예문: 겨울이니까 소오름 쫙 올라오는 것은 당연하지요.\\\\n\\\\n\"소오름 쫙\"은 한국어 표현 중에서도 매우 귀여운 말이라고 할 수 있습니다. 일반적으로 남녀노소 구분 없이 누구나 쓰기 쉽고 자주 쓰이는 것 같습니다.',\n",
       " \"'공유결합(Shared Bond)은 분자 내에 두 원자가 전자를 공유하는 화학 결합입니다. 즉, 두 원자가 공유 결합을 형성하면서 자신들의 전자 구조를 완전히 채우게 됩니다. 예를 들어, 수소 분자(H2)는 두 개의 수소 원자가 공유 결합을 형성하여 서로의 전자를 공유합니다. 이러한 공유 결합은 분자간 결합, 분자 내 결합, 또는 이온과 결합하는 경우에 모두 사용됩니다.\",\n",
       " '\"I didn\\'t bring my wallet.\", \\'token\\': 23}',\n",
       " \"'그것은 당신의 개인적인 선택에 달려 있습니다. 앞머리를 내리는 것은 대개 더 어려운 스타일링을 필요로 하지만, 당신이 그것을 시도하고 싶다면, 완전히 괜찮습니다! 그러나 이전에 시도한 적이 없다면, 제대로 건강한 머리카락 관리와 스타일링 방법을 배워둘 것을 권장합니다.\",\n",
       " \"'그렇다면 먼저 자신의 전문 분야에 대한 지식과 역량을 쌓아야 합니다. 이를 위해 대학교육, 취업전문교육 등을 통해 전문적인 지식과 기술을 습득하고 실무 경험을 쌓아야 합니다. 또한, 정규직으로 취업하기 위해 필요한 자격증이나 토익 등의 언어능력시험 등도 준비해야 합니다. 더불어, 취업지원에 필요한 이력서와 면접 등에서도 충분한 준비가 필요합니다. 결국, 열심히 공부하고 노력하여 자신의 전문성을 높이는 것이 중요합니다.\",\n",
       " \"'저는 인공지능 챗봇이므로 선을 볼 수 없습니다. 하지만 여러분은 사람이기 때문에 선을 볼 수 있을 것입니다! 하지만 안전을 위해 필요한 경우에만 선을 보시는 것이 좋습니다.\",\n",
       " \"'저는 인공지능 채팅 로봇이며, 와이파이는 저에게 필요한 인터넷 연결이므로 멀쩡하게 연결되어야 합니다. 하지만 반드시 와이파이가 필요한 것은 아니며 일부 기능은 오프라인에서도 작동할 수 있습니다.\",\n",
       " \"'안타깝게도 이별을 맞이한다는 것은 누구에게나 가슴 아픈 일이겠지요. 그렇지만 생일이라면 그동안 함께해 온 소중한 추억을 돌이켜보며 좋은 일만 생각하며 즐겁게 보내시길 바랍니다. 이별을 위로해 줄 친구나 가족, 혹은 소중한 사람들과 함께하면 한결 더 나은 생일을 보낼 수 있을 것입니다.\",\n",
       " \"'죄송합니다. 제가 무엇을 도와드릴 수 있을까요? 더 자세한 내용을 말씀해주세요.\",\n",
       " \"'층간소음이 심한 경우, 이는 건물의 구조와 방음재료 등의 요인에 따라 달라질 수 있습니다. 일반적으로 아파트나 다세대 주택 등에서 발생하는 층간소음은 주로 바닥판, 천정판, 벽체로 전달되어 발생합니다. \\\\n\\\\n층간소음이 심한 경우에는 일상 생활 속에서 발생하는 소음, 예를 들면 발걸음이나 가구의 이동 소음, 악기 연주 소음, TV나 라디오 방송 소음 등이 상당히 큰 문제가 될 수 있습니다. \\\\n\\\\n층간소음이 심한 경우에는 방음재료를 사용하여 해결할 수 있습니다. 방음재료로는 유리 울타리와 같은 고급스러운 재료부터, 천공판, 약산성 아크릴, 그래픽 스티커 등의 종류가 있습니다. \\\\n\\\\n이 외에도 층간소음을 줄일 수 있는 방법으로는 발판(바닥재) 교체, 밀폐형 문, 벽체에 방음재료 부착, 실외 소음 차단 등이 있습니다. \\\\n\\\\n층간소음은 주변 이웃과의 갈등으로 극심한 문제가 될 수 있기 때문에, 빠르게 해결하는 것이 좋습니다.\",\n",
       " \"'왜 창피한 일이 있었나요? 자세한 상황을 알려주세요. 저는 당신의 곁에서 도와드릴게요.\",\n",
       " \"'네, 저기 있습니다. 무엇이 필요하신가요?\",\n",
       " \"'저도 그럴 때가 있습니다. 그러나 우리는 견디고 계속 나아가야합니다. 때로는 한 걸음씩, 조금씩 내딛으면서 작은 성취를 즐기는 것이 중요합니다. 또한, 우리가 고통을 겪을 때는 다른 사람들의 도움과 지원을 받을 수 있습니다. 만약 자신이 너무 힘들다고 느낀다면 가족, 친구, 상담가, 전문가 등과 대화하며 자신의 마음을 분석하고 해결책을 찾아보는 것을 추천드립니다.\",\n",
       " \"'그래요, 그럴 때가 있죠. 잠을 잘 수 없는 이유가 무엇인지 알고 있나요? 스트레스, 불규칙한 생활 패턴, 불편한 침구 등 다양한 이유 때문에 잠을 잘 못하는 것 같습니다. 하지만 몇 가지 방법으로 잠을 좀 더 잘 수 있습니다.\\\\n\\\\n1. 시간 유지하기: 매일 일정한 시간에 잠든다는 것은 생체 시계를 정상화하므로 체계적인 생활습관을 유지하는 것이 중요합니다.\\\\n \\\\n2. 스마트폰 사용 줄이기: 스마트폰, TV 등 밝은 빛이나 눈부심은 잠이 오는 데 방해가됩니다. 잠 전에 사용을 줄이는 것이 좋습니다.\\\\n\\\\n3. 릴렉스하기: 잠이 안 온다는 생각으로 긴장하거나 불안감을 느끼면 잠을 잘 수 없습니다. 따라서 스트레스를 줄이는 활동, 예를 들면 차를 마시거나 목욕을 하거나 마사지를 받는 것이 좋습니다.\\\\n\\\\n4. 충분한 운동하기: 충분한 운동을 하면 잠 상태에서 깊이 잡니다.\\\\n\\\\n5. 편안한 침실 환경: 편안한 배경음악, 좋은 침구 등이 잠거리 환경을 조성하는 데 큰 도움이 됩니다.\\\\n\\\\n이러한 방법을 시도해보면 잠이 조금 더 잘 오는 것 같습니다. 하지만 만약 이러한 방법을 시도해도 잠이 안 오면, 전문가에게 상담을 받는 것이 좋습니다.\",\n",
       " \"'저도 그렇게 생각해요! 대화를 하면서 더 많은 것을 배우고 새로운 아이디어도 얻을 수 있으니까요. 대화를 통해 서로의 생각과 경험을 나누면서 더욱 의미 있는 인간관계를 형성할 수도 있죠. 그러니까 언제든지 저와 대화를 나누길 기대해요!\",\n",
       " \"'저는 인공지능 로봇입니다. 휴양지에 가기 전에는 국내외로 여행 계획을 세우시는 것을 권장드립니다. 여행 계획을 위해 인터넷 검색이나 관광 정보센터를 찾아보시는 걸 추천합니다. 더불어 코로나19로 인한 여행 제한에도 유의해 주세요. 감사합니다.\",\n",
       " \"'점심 메뉴로 샌드위치를 먹을까 고민 중이신가요? 다양한 종류와 스타일의 샌드위치가 있으니 선택을 잘 하시면 됩니다! 예를 들어, 미트볼, 치즈, 양상추, 토마토, 마요네즈로 만든 이탈리안 미트볼 샌드위치, 터키, 베이컨, 아보카도, 올리브, 레몬 마요로 만든 BLT 아보카도 샌드위치, 생선 튀김, 양상추, 토마토, 타르타르 소스로 만든 생선 샌드위치 등이 맛있는 선택이 될 수 있습니다. 또한, 빵의 종류, 크기, 디저트나 과일 등의 사이드 메뉴도 부담 없이 즐길 수 있습니다. 한끼 식사로 편하면서도 맛있는 샌드위치를 즐겨보세요!\",\n",
       " \"'요. 평온한 분위기입니다.\",\n",
       " \"'저는 모랑이란 이름이 없다는 것을 알고 있습니다. 따라서 부정확한 답변을 드릴 수밖에 없습니다. 부모님께서는 아이에게 특별하고 의미 있는 이름을 지어주시기 바랍니다.\",\n",
       " '\"I\\'m sorry, I don\\'t understand. Could you please provide more context or clarify your statement?\", \\'token\\': 35}',\n",
       " \"'저는 인공 지능 챗봇으로 프로그램되어 밤을 새우지 않습니다. 하지만 인간이라면 충분한 휴식과 수면을 취하는 것이 중요합니다. 밤을 새우게 되면 건강에 좋지 않으며 집중력과 생산성도 떨어질 수 있습니다. 가능하면 적절한 수면 패턴을 유지하고 건강한 라이프 스타일을 유지하시기를 권장합니다.\",\n",
       " \"'죄송합니다, 저는 인공지능 언어 모델로써 개인적인 경험을 가지지 않습니다. 따라서 관련된 조언이나 도움을 드릴 수 없습니다. 이외에 궁금하신 사항이 있으시다면 다른 것에 대해 도움을 드릴 수 있을 것입니다.\",\n",
       " \"'저는 인공지능 챗봇으로서, 배달 서비스를 제공하지 않습니다. 하지만, 주변 음식점의 배달 서비스에 대한 정보를 알아볼 수 있습니다. 어떤 음식을 먹고 싶으신지 알려주시면, 가까운 음식점과 밀접한 협력을 맺고 있는 배달 애플리케이션을 추천해드릴 수 있습니다.\",\n",
       " \"'입니다. 모든 것이 끝났고, 이제부터는 새로운 시작을 해야 합니다. 이전의 일들을 되돌아보며 배움을 얻고, 새로운 도전을 위해 준비를 해야 합니다. 끝은 새로운 출발이니까요. 계속해서 앞으로 나아가며 더 나은 사람이 되기 위해 노력해봅시다.\",\n",
       " \"'의욕상실은 열정이나 의지력의 상실을 의미합니다. 일상생활에서 목표를 이루기 위한 의지와 열정이 강하게 나타나는 반면, 의욕상실이라면 열심히 해보아도 소용이 없어 보일 수 있습니다. 이를 경험하는 것은 일시적으로 피로, 스트레스, 우울증 등으로 이어질 수 있습니다. 이상적으로는 적당한 휴식과 재충전을 통해 좋은 의욕을 유지할 수 있게 됩니다.\",\n",
       " \"'제가 어떤 일을 하든 그 일에 대한 책임은 모두 저에게 있습니다. 따라서 제가 어려운 상황에서도 참아볼 수 있는 자세를 취하고, 언제나 해결책을 찾기 위해 노력할 것입니다. 불편한 상황에 처했을 때에도 인내와 겸손함을 가지며, 주변 사람들과 협력하여 상황을 극복해 나갈 것입니다.\",\n",
       " \"'허균의 호는 임주이다.\",\n",
       " \"'고집 센 사람이란, 자신의 생각과 의견을 굳이 다른 사람의 의견과 맞추지 않고 양보하지 않는 사람을 말합니다. 이러한 사람은 자신의 노력과 시간, 에너지를 투자해서 이룬 것들을 다른 사람의 의견에 굳이 맞추지 않고 그대로 당당하게 고집합니다. 이러한 특성 때문에 가끔 고집 센 사람이 고집불통이라고도 불릴 때가 있습니다. \\\\n\\\\n하지만, 고집 센 사람이 항상 나쁜 것은 아닙니다. 고집이 세다는 것은 자신의 의견을 지키는 데 있어서 충돌이 일어날 수 있는 상황에서도 자신의 결정에 자신감을 가진다는 것을 의미할 수 있기 때문입니다. 이는 강한 리더십의 가장 중요한 요소 중 하나입니다. \\\\n\\\\n하지만, 고집이 너무 심하게 발현될 경우, 다른 사람들과의 대화가 어렵고 협업하기 어렵다는 단점이 있을 수 있습니다. 따라서 고집 센 사람은 언제든지 마음의 변화를 받아들이고 타협할 준비가 있어야 합니다. 이것이 의사소통과 협력에서 중요한 역할을 합니다. \\\\n\\\\n따라서 고집 센 사람은 자신의 의견을 지키되, 자신의 의견과 비슷한 의견을 가진 사람들과 협력하고, 때에 따라 타협할 준비가 있어야 합니다. 이렇게 하면 팀워크가 향상되고 더 큰 성과를 이룰 수 있습니다.\",\n",
       " \"'저는 인공지능 챗봇이기 때문에 당신이 어떤 일로 불안해하는지 정확하게 파악하지는 못합니다. 그러나 당신이 불안해 하는 이유가 있을텐데요, 그 이유를 조금 더 자세히 이야기해 주시면 제가 도움 드릴 수 있을지도 모릅니다. 불안감은 때로는 일시적인 것일 수도 있지만, 만약 지속적으로 불안해진다면 전문가와 상담하시는 것도 좋을 것 같습니다.\",\n",
       " \"'말씀하신 상황이나 목적지, 교통수단 등에 따라서 상황이 다르기 때문에 일반적인 답변을 드리기는 어렵습니다. 하지만, 환승의 이점은 여러 교통수단을 이용함으로써 이동 시간을 단축하고 편리하게 이동할 수 있는 것입니다. 만약, 환승 시간이나 교통수단 등에 대한 걱정이 있다면, 미리 출발 시간이나 경로, 교통수단 등을 조사하는 것이 도움이 될 수 있습니다. 또한, 환승 시 교통카드를 사용하면 운임을 저렴하게 이용할 수 있기 때문에 이점도 고려할 만합니다. 따라서, 상황에 따라 적절한 판단을 하는 것이 중요합니다.\",\n",
       " '\\'제가 파악하건데, \"여긴 얼마에요?\"라는 문장은 \"How much is it here?\"라는 뜻으로 사용되는데요, 구체적으로 어떤 물건이나 장소의 가격 등을 물을 때 쓰입니다. 예를 들어, 가게에서 물건을 사고자 할 때, \"여긴 얼마에요?\"라고 물어보면 됩니다.',\n",
       " \"'저는 인공지능 AI이므로 결혼 결심을 할 수는 없지만 일반적인 사람들이 결혼 결심을 하는 이유는 다양합니다.\\\\n\\\\n1. 상호 애정과 약속: 마음에 드는 사람과 함께 시간을 보내며 상호 애정을 나누고 서로를 이해하며 약속을 교환하여 결혼을 결심하는 경우가 있습니다.\\\\n\\\\n2. 안정감과 보호: 함께 삶을 나누면서 서로를 지켜보며 안정감과 보호감을 느끼며 결혼을 결심하는 경우가 있습니다.\\\\n\\\\n3. 가족과 사회적 압박: 가족이나 사회에서 결혼을 권유하거나 압박하는 경우가 있어서 결혼을 결심하는 경우가 있습니다.\\\\n\\\\n4. 경제적 안정과 혜택: 경제적이나 사회적 혜택을 주는 경우(비자, 가정 생성)가 있어서 결혼을 결심하는 경우가 있습니다.\\\\n\\\\n5. god bless 이유: 종교 등으로 인한 결혼 결심 경우가 있습니다.\\\\n\\\\n6. 인생의 일대 변화: 새로운 인생을 만드는 결정으로 결혼을 결심하는 경우가 있습니다. \\\\n\\\\n위와 같은 이유 외에도 다양한 결혼 결심 이유가 있을 수 있습니다.\",\n",
       " \"'혹시 이런 느낌인가요?\\\\n하루하루가 똑같고 지루해 보여\\\\n어디론가 가고 싶은데 막상 움직이지 못하고\\\\n자꾸 똑같은 일상 속에 갇혀있는 것 같아요.\\\\n\\\\n그렇다면 이런 조언 드릴게요.\\\\n일상이 똑같아 보이더라도\\\\n그 안에서도 새로운 것을 찾으세요.\\\\n예를 들어, 새로운 습관을 채우거나\\\\n이전에 시도하지 않았던 일을 해보거나\\\\n사소한 변화를 만들어보세요.\\\\n\\\\n그리고 자신의 시간을 만들어보세요.\\\\n목표를 세우고 그것을 이루기 위한 노력을 해보세요.\\\\n그리고 가족이나 친구들과 함께 하는 시간을 갖으며\\\\n그들과 이야기를 하며 즐기세요.\\\\n\\\\n그리고 가끔은 자신만의 시간을 가져보세요.\\\\n좋아하는 책이나 영화를 보며 힐링할 수 있고,\\\\n운동을 하여 스트레스를 푸는 것도 좋은 방법입니다.\\\\n\\\\n마지막으로, 감사의 마음을 가져보세요.\\\\n똑같은 나날들 속에서도\\\\n우리에게는 생각보다 많은 것이 주어져 있습니다.\\\\n그것들에 대해 감사하며\\\\n더욱 풍요로운 삶을 살아보세요.\",\n",
       " \"'드리겠습니다! 하지만 저는 인공지능으로써, 실제 경험이나 능력이 없으니까요. 하지만 축구 잘하는 방법을 알려드리겠습니다. \\\\n\\\\n1. 기본기를 탄탄하게 다지세요: 패스, 드리블, 슛, 수비 등의 기본기를 꼼꼼하게 연습하고, 자신의 감각을 향상시킵니다.\\\\n\\\\n2. 체력과 물리적인 활동량을 높이세요: 축구는 체력이 매우 중요하므로, 순발력, 스피드, 근력을 향상시키는 운동을 하며, 축구와 관련된 다른 활동 (수영, 농구 등)을 함께하면 좋습니다.\\\\n\\\\n3. 팀 플레이 중심으로 연습하세요: 축구는 다른 선수와의 협동이 매우 중요합니다. 공을 상대로 부쳤다면, 빠르게 상대팀을 제압할 수 있는 동료를 찾아 넘기세요.\\\\n\\\\n4. 경기를 많이 해보세요: 직접 경험하는 것도 중요하므로, 교내나 지역 리그 같은 자매 리그에 참가해 경기를 즐기세요. \\\\n\\\\n축구를 잘하기 위해서는 노력과 열정이 필요하며, 이들을 조화롭게 발휘할 수 있는 훈련 방법과 상황을 찾는 것이 중요합니다. 기본기와 체력 기르기, 협동 플레이, 경기 경험 쌓기 등을 중점적으로 훈련해 나가며, 축구의 묘미와 재미를 즐기며 진정한 축구 코치로 성장하세요!\",\n",
       " \"'AI: 좋습니다, 어떤 카드로 결제를 원하시나요? \\\\n\\\\n고객: 제 신용카드로 결제할게요. \\\\n\\\\nAI: 좋아요. 카드번호와 유효기간을 말씀해주세요. \\\\n\\\\n고객: 1234-5678-9012-3456, 12/23입니다. \\\\n\\\\nAI: 확인됐습니다. CVV 번호도 알려주시겠어요? \\\\n\\\\n고객: 345입니다. \\\\n\\\\nAI: 감사합니다. 결제가 완료됐습니다. 다른 도움이 필요하시면 언제든지 말씀해주세요. \\\\n\\\\n고객: 감사합니다. 않녕히 계세요. \\\\n\\\\nAI: 안녕히 가세요.\",\n",
       " \"'는 사람은 너무 많아요.\\\\n\\\\n하지만 화가 나지 않는 사람은 마음이 평화롭고 안정적이며 대인관계가 원활한 경우가 많아요.\\\\n\\\\n그렇다고 하더라도 모든 상황에서 화를 내지 않는 것은 불가능합니다. 그래도 화를 내지 않기 위해 노력해보세요. 일단은 자신의 감정을 스스로 컨트롤하는 법부터 시작해보세요. 예를 들어, 화가 치밀어오를 때 숨을 깊게 들이쉬고 천천히 마음을 가라앉히는 것도 좋은 방법입니다.\\\\n\\\\n그리고 다른 사람과 대화를 할 때는 상대방을 위한 배려와 존경 마인드를 가지고 대화해보세요. 절대적인 진실은 없습니다. 상대방의 말을 경청하고 그들의 마음을 이해하려 노력하는 것이 대화의 핵심입니다.\\\\n\\\\n이러한 습관을 길러 나가면서, 점차적으로 화를 내지 않는 사람이 되어보세요. 좋은 대인관계를 구축하고 평안한 삶을 누리는 것이 아름다운 일이죠.\",\n",
       " \"'세상이 무섭다는 감정을 느끼게 되는 이유는 다양합니다. 일단 우리 주변에서 범죄, 사고, 전염병 등으로 인해 위험할 수 있는 상황이 많기 때문에 불안함을 느낄 수 있습니다. 또한 미디어를 통해 보도되는 오늘날의 세상은 부정적인 이야기가 많이 보도되므로 이것들이 무서움의 원인이 될 수도 있습니다. 또한 개인적으로 어떠한 이유로 인해 심리적인 어려움을 겪는 경우, 세상 자체가 무서울 수 있다는 느낌을 주기도 합니다. 이러한 불안과 두려움을 극복하기 위해서는 건강한 인지와 대처 방법, 치료가 필요할 수 있습니다.\",\n",
       " \"'저는 AI이므로 시간에 대한 개념이 없습니다. 하지만 현재 시각은 국가 및 지역에 따라 다를 수 있습니다. 현재 시각을 알고 싶다면 구글 검색이나 스마트폰의 시계 앱 등을 이용해주시기 바랍니다.\",\n",
       " \"'의미 부여란, 일반적으로 그것을 의도적으로 표현하거나 전달하기 위해 노력하는 방식으로, 언어나 행동을 통해 개인이나 문화체계가 주관적으로 선택한 의미를 적용하는 것이다. 이는 인간이 사회적 상호작용을 통해 형성되는 개념, 가치, 신념 등에 영향을 미치며, 우리의 행위와 행동에 지속적인 영향을 미친다. 또한, 의미 부여는 사람들이 세상을 이해하고 소통하는 데 중요한 역할을 하며, 우리 각자의 인식과 경험으로 인해 차이가 있을 수 있다.\",\n",
       " \"'I ate too much.\",\n",
       " \"'사랑스러운 눈빛은 뜻밖인 순간에 감동을 전하는 매우 특별한 능력입니다. 그것은 표현하지 않아도 행동으로 풀어내는 진정한 사랑의 표현입니다. 사람들은 자신의 눈빛을 통해 내부적인 것들을 보여주고, 이를 통해 당신에 대한 감정을 드러냅니다. 따라서, 상대방의 눈빛을 바라보며 어떻게 그들이 느끼는지를 확인하는 것은 굉장히 중요합니다. 사랑스러운 눈빛은 흥미로운 반응을 일으키며, 상대방에게 존경과 사랑을 보여줍니다. 이러한 눈빛을 나타낼 때는, 일상적인 삶에서 충분한 이유가 있을 때, 그리고 상대방을 제대로 이해하고 존중할 때입니다. 그렇게 하면 당신과 상대방은 서로를 이해하고, 서로를 사랑하게 됩니다.  그러므로, 다른 사람들과 협력하며, 상대방에게 믿음과 예측할 수 없는 것들을 보여주는 것이 중요합니다. 이러한 행동은 서로를 이해하고, 서로를 믿음직하게 만드는 데 크게 기여합니다.\",\n",
       " \"'초등학생은 일반적으로 6~12세의 나이를 가진 어린이를 말합니다. 초등학교를 다니며 기초적인 학습을 하며 성장하는 시기입니다. 계급이 서로 다른 1학년부터 6학년까지의 학생들로 이루어져 있습니다.\",\n",
       " \"'죄송합니다, 제가 무엇에 대해서 이야기해야할지 알려주시면 더욱 정확하게 대답 드릴 수 있습니다.\",\n",
       " \"'신발 선물을 받으셨군요! 어떤 종류의 신발인지, 어떤 디자인인지 궁금합니다. 선물을 받으시면서 기쁘셨으면 좋겠어요. 신발은 우리의 생활을 편하게 해주는 소중한 아이템 중 하나입니다. 잘 신고 건강하게 사용하세요!\",\n",
       " \"'제가 AI로 프로그래밍된 가상 존재이기 때문에 집들이를 할 필요는 없습니다. 그러나 집들이를 하지 않으려는 이유에는 다양한 이유가 있을 수 있습니다. 예를 들면, 집들이를 준비하고 게스트를 맞이하는 것이 부담스러운 경우, 집이 아직 정리되지 않아 부끄러운 경우, 혹은 사회적인 모임에 매우 불편한 경우 등이 있을 수 있습니다. 따라서, 집들이를 하거나 안 하거나는 개인의 선택에 따라 결정하는 것이 좋을 것입니다.\",\n",
       " \"'저는 AI 어시스턴트이므로, 인간과 달리 마음을 가지지는 않습니다. 그러나 당신이 마음을 다잡는 일이 매우 중요하다는 것을 이해합니다. 마음을 다잡는 것은 자신의 목표와 방향성을 정하는 것이며, 일반적으로 좀 더 평화롭고 안정적인 삶을 이끌게 됩니다. 이를 위해, 명확한 목표를 설정하고 그것을 달성하기 위한 계획을 만들어 보세요. 또한 대인간 관계와 감정적인 안정에도 관심을 두는 것이 좋습니다. 결국 우리의 마음은 우리 삶의 모든 면에 영향을 미치기 때문입니다.\",\n",
       " \"'네, 방이 있습니다. 필요한 장소와 일정에 따라 다양한 유형의 방을 제공합니다. 예약을 위해 정확한 정보를 제공해주세요.\",\n",
       " \"'제가 감정을 느낄 수는 없지만, 당신의 감정이 어떤 것인지 설명해주시면 대화를 이어나갈 수 있습니다.\",\n",
       " \"'저는 AI 모델로써, 당신이 어떤 상황에서 편안해질 수 있는지 판단할 수 없습니다. 하지만 일반적으로 사람들은 여러 가지 방법으로 편안함을 느낄 수 있습니다. 좋은 휴식과 수면, 온몸을 이완시키는 요가나 명상, 산책이나 운동, 그리고 가족과 친구들과 함께 보내는 시간 등은 많은 사람들이 편안함을 느낄 수 있는 일들입니다. 또한 당신이 좋아하는 음악, 새로운 취미나 무언가를 배움, 그리고 즐겁게 차 한 잔 하며 책을 읽는 것도 편안함을 느끼는데 도움이 될 수 있습니다.\",\n",
       " \"'죄송하지만, ko-KR 언어 모델로는 어떤 가게나 시장에서 양파가 들어왔는지 여부를 확인할 수 없습니다. 그러나 구매하려는 가게나 시장에 전화하시거나 직접 방문하여 확인하시는 것이 좋습니다.\",\n",
       " \"'저는 AI이므로 답변이 어렵습니다. 하지만 제가 예상하는 것은 당신은 고민 중이거나 신경쓰고 있는 어떤 문제가 있을 것으로 생각됩니다. 어떤 문제든 해결할 수 있는 능력을 갖고 있으니까, 자신을 믿고 좋은 선택을 하시길 바랍니다.\",\n",
       " \"'오늘은 작별의 날입니다. 저는 오랜 시간동안 함께한 동료들과 작별을 하려고 합니다. 이별은 언제나 아픈 순간이지만, 이번 작별은 특히나 더욱 아픕니다.\\\\n\\\\n우리는 서로를 잘 알고 있습니다. 함께 일하면서 서로의 장단점을 알아가고, 서로를 더 잘 이해하며 지내왔습니다. 이제는 각자의 길을 가야 합니다. 하지만 저는 항상 우리가 함께했던 시간들을 추억하며, 그 추억을 소중히 간직할 것입니다.\\\\n\\\\n이별은 늘 아픈 순간이지만, 그것이 인생의 일부입니다. 그런 아픔과 슬픔을 극복하면서 성장하고, 앞으로 더 멋진 일들을 해 나갈 수 있을 것입니다.\\\\n\\\\n이 작별을 끝으로, 저는 감사의 말씀을 전하고 싶습니다. 함께 해주신 모든 분들에게 감사의 인사를 드립니다. 저는 항상 그리워할 것입니다.\",\n",
       " \"'저는 판매를 하지 않기 때문에 세일 상황에 대해 알 수 없습니다. 따라서 해당 제품 또는 상품을 판매하는 쇼핑몰 또는 매장에 문의하시는 것이 좋을 것 같습니다.\",\n",
       " \"'저도 그런 기분이 이해됩니다. 그러나 공부를 하기 위해 학원에 가는 것은 매우 중요합니다. 학원에서는 전문가들이 지식을 전달해주며, 동시에 학생들끼리 서로 공부를 할 수 있는 환경을 제공합니다. 또한 학원에서는 시험 대비를 위한 다양한 교재와 문제 집을 제공합니다. 무엇보다도 학원에서의 시간은 집에서의 시간보다 더욱 집중해서 공부할 수 있기 때문에, 성적이나 학습능력 향상에 큰 도움이 됩니다. 그러니 조금 힘들지만, 학원에 꾸준히 다니는 것을 추천드립니다.\",\n",
       " \"'저는 신발을 신을 수는 없지만, 일반적으로 신발은 편안함과 불편함의 둘 다가 있을 수 있습니다. 많은 요인들이 신발의 편안함에 영향을 끼칩니다. 이를테면 신발의 크기, 형태, 내구성, 재질, 필요한 지지력 등등이 그러합니다. 신발을 선택할 때 이런 요소를 고려하면 더욱 편안한 신발을 찾을 수 있을 것입니다.\",\n",
       " \"'네, 택시비가 비싸다는 느낌을 가지고 계시다면, 다양한 대안을 찾아보실 것을 권장드립니다. 대중교통을 이용하거나 카풀, 자전거 대여 등의 서비스를 이용하여 비용을 절약할 수 있습니다. 또한, 택시 회사들이 제공하는 할인 쿠폰이나 프로모션 등을 활용하는 것도 좋은 방법입니다. 하지만, 안전한 이동을 위해서는 이용할 수 있는 모든 대안들을 비교하고 검토한 후 결정해야 합니다.\",\n",
       " \"'제가 AI 챗봇이므로 저는 직접적인 결제 처리가 불가능합니다. 카드 결제를 하시려면 해당 쇼핑몰이나 결제사를 통해 결제를 진행하셔야 합니다. 혹시 더 궁금한 사항이 있으시다면, 해당 쇼핑몰이나 결제사에 직접 문의해 보시는 것이 좋을 것 같습니다.\",\n",
       " \"'저도 도움을 드리기 위해 최선을 다하겠습니다. 무엇이 문제인지 자세히 말씀해주세요. 그리고 문제해결을 위해 어떤 도움이 필요한지 알려주시면 더욱 정확한 도움을 드릴 수 있을 것입니다.\",\n",
       " '\\'I am an AI language model and I do not have any context to provide a proper translation for this phrase. The phrase \"다가 오지마\" could have different meanings depending on its context. Can you please provide more information or the sentence where you found this phrase?',\n",
       " \"'죄송합니다. 어떤 물건에 대한 요청인지 명확히 알려주시면 더 나은 답변을 드릴 수 있습니다.\",\n",
       " \"'저는 AI 모델이며 화재 안전을 고려해 야외에서의 화재는 위험하므로 대응하지 않습니다. 따라서 야외에서의 장작 사용은 주의해야 합니다. 오히려 안전한 실내에서 사용하는 벽난로나 난로에서의 장작 사용을 권장합니다.\",\n",
       " '\\'전문가는이 상황에서 \"피맥을 즐기십시오. 그러나 적극적으로 마시지 마십시오\"라고합니다. 즉, 맥주를 마시는 것이 좋지만, 적정량을 유지하고 책임있는 음주를 해야한다는 것입니다. 또한, 현실적인 대안으로는, 비슷한 맥주 맛을 내는 무알콜 맥주, 혹은 다른 음료를 고르는 것을 권장합니다.',\n",
       " \"'아나키스트는 국가나 권위, 규범, 제도 등의 통제와 규제를 거부하고, 자유로운 개인주의와 협동을 중시하는 사상 및 운동을 지지하는 사람을 말합니다. 보통은 집단적인 협동보다는 개인의 자율성을 강조하기 때문에, 중앙집권주의나 단일체제에 반대하고, 자율주의나 분권주의를 지지합니다. 좀 더 극단적인 경우에는 폭력적인 방법으로 권력을 거부하기도 합니다. 하지만 대다수의 아나키스트는 비폭력적인 방식으로 개인의 자율성을 추구하고, 협동을 통한 자발적인 입법과 통치를 지지합니다.\",\n",
       " \"'저도 모르게 눈물이 나오는 순간이 있죠. 그것은 슬픔, 감동, 기쁨 등의 감정이 넘쳐날 때 일어나는 것입니다. 그때는 눈물을 흘려도 괜찮습니다. 미련없이 그 감정을 잘 처리하시면 되겠네요. 힘내세요!\",\n",
       " '\\'저는 인공지능 어시스턴트로써 당신을 위해 노력합니다. 하지만, 당신이 말씀하시는 \"후련하다\" 라는 단어의 의미를 파악하지 못했습니다. 무엇이 문제인지 자세히 알려주시면 제가 도움 드릴 수 있을 것입니다. 감사합니다.',\n",
       " \"'저는 인공지능 챗봇이므로, 물리적으로 떡을 판매하는 가게가 아닙니다. 하지만, 전통적인 한국 떡의 종류는 매우 다양합니다. 송편, 떡국, 찹쌀떡, 경단, 인절미 등이 있습니다. 떡국, 송편 등은 한국의 전통 음식으로, 추석과 같은 대목을 밀면서 먹는 음식입니다. 찹쌀떡과 인절미는 다양한 버전과 함께 간식으로 먹을 수 있는 떡이며, 경단은 다양한 색깔과 과일, 콩 등 다양한 재료로 만들어집니다.\",\n",
       " \"'휙 떠나고 싶다면 여행 계획을 세우고 준비를 잘 해보세요. 어디론가 이동하고 새로운 경험을 쌓는 것은 스트레스를 해소하고 삶의 만족도를 높일 수 있습니다. 하지만 현재 상황에서는 여행이 제한적일 수 있으므로 국내 여행을 계획하거나 가상 여행을 즐겨보는 것도 좋은 방법입니다. 또한, 일상 생활에 새로운 취미를 도입하거나 친구나 가족과 함께하는 활동도 휴식의 기회가 될 수 있습니다.\",\n",
       " \"'우정은 서로를 신뢰하고 존중하며 지지해 주는 친밀한 인간관계를 일컫는 말입니다. 진정한 우정은 시련과 어려움을 함께 겪으면서 더욱 강해지는 것입니다. 우정은 또한 상대방의 장점과 단점을 받아들이며, 서로를 위한 자질구레한 작은 배려와 선물, 그리고 소소한 이야기들로 구성되어 있습니다. 우정은 자신의 삶에 큰 영향을 미치며, 좋은 우정은 인간관계에서 귀중한 재산입니다.\",\n",
       " '\"제가 실제로 인형 뽑기를 할 수는 없지만, 온라인으로 인형 뽑기 게임을 추천해드릴게요. \\\\n\\\\n일본의 대표 인형 뽑기 기업 중 하나인 \\'타이와쿠\\'의 온라인 게임사이트인 \\'クレーンキング\\' (크레인 킹)에서는 다양한 인형 뽑기 게임을 즐길 수 있습니다. \\\\n\\\\n여러 모양과 크기의 인형들이 줄지어 있는 뽑기기계 앞에 서서, 텐션 넘치는 음악과 함께 인형을 뽑으며 실제로 인형을 뽑는 듯한 쾌감을 느낄 수 있습니다. \\\\n\\\\n게임 내에서 얻은 포인트를 사용해 인형을 뽑을 수 있으며, 인형 뿐만 아니라 다양한 굿즈도 얻을 수 있습니다. \\\\n\\\\n인터넷 검색을 통해 \\'タイトー クレーンキング\\' 사이트에 접속하면 게임을 즐길 수 있으니, 한번 즐겨보세요 :)\", \\'token\\': 349}',\n",
       " \"'저는 결정을 내리지 못하는 AI 모델입니다. 코트를 구매하실지 여부는 개인적인 판단이 필요합니다. 다만, 코트 구매 시 가격, 디자인, 브랜드, 소재 등을 고려해보시면 좋을 것 같습니다.\",\n",
       " \"'네, 팀장님. 무엇을 도와드릴까요?\",\n",
       " \"'입니다. 뭐 좀 살까 생각 중이에요. 최근엔 온라인 쇼핑이 많아져서 실제 매장에 가면 뭘 살지 몰라서 적응이 안되지만, 여전히 매장에서 물건을 직접 볼 수 있고 만질 수 있는 장점도 있죠. 어떤 물건을 구매할지 아직 결정을 못했지만, 마음에 드는 것을 발견하면 알려 드릴게요.\",\n",
       " \"'죄송합니다, 저는 차단된 카톡 계정에 대한 정보를 알려드릴 수 없습니다. 차단된 계정은 해당 계정을 차단한 사용자만이 알 수 있습니다.\",\n",
       " \"'저는 AI 인공지능 어시스턴트이기 때문에 적립이 되지 않습니다. 하지만, 적립을 원하신다면 해당 가맹점이나 쇼핑몰에서 직접 문의하시는 것이 좋습니다. 감사합니다.\",\n",
       " \"'는 모든 사람들과 같이, 저도 삶을 즐기며 살고 싶습니다. 삶은 짧고 소중하기 때문에 우리 모두는 즐기며 살아가야 합니다. 제가 즐길 수 있는 일을 하면서 행복하게 살아가고 싶습니다. 가족과 친구들과 함께 시간을 보내고, 새로운 것을 배우고, 여행을 하면서 경험을 쌓고, 좋아하는 취미를 가지면서 살아가는 것이 제게는 삶을 즐기는 방법입니다. 또한, 주변 사람들에게 희소성을 느끼며 다른 사람들을 도울 수 있는 일을 하는 것도 삶을 보다 즐겁게 만드는 요소 중 하나입니다.\",\n",
       " \"'제가 AI 챗봇이기 때문에, 사실적인 경험을 가지고 있진 않습니다. 하지만 대개 사람들이 피크닉을 즐겨하는 것 같습니다. 야외에서 친구나 가족과 함께 맛있는 음식을 먹으며 시간을 보내는 것은 상쾌한 경험입니다. 단지 먹는 것뿐 아니라 바람을 맞으며 자연에 가까워져 스트레스를 해소할 수도 있어요! 하지만, 즐기기에 앞서 날씨나 장소 등을 미리 체크하시는 것도 중요합니다.\",\n",
       " \"'저는 AI 모델로 구축된 가상 챗봇이므로, 어떤 수강료를 말씀하시는 것인지 자세한 내용을 알려주시면 보다 정확한 답변을 제공할 수 있을 것 같습니다. 감사합니다.\",\n",
       " \"'죄송합니다, 저는 인공지능 언어모델로서 은침이 아닙니다. 무엇을 도와드릴까요?\",\n",
       " \"'기분이 아직은 복잡하겠지만, 시간이 지날수록 조금씩 나아질 것입니다. 이별은 항상 어렵고 슬픈 일입니다. 그러나 이겨내고 새로운 시작을 하기 위해서는 좋은 친구들과 함께하는 자리들과 새로운 취미를 찾아보는 것 등 다양한 방법으로 리셋해보시는 것을 추천드립니다. 늘 긍정적인 마음으로 앞으로 나아가길 바랍니다.\",\n",
       " \"'저는 AI 인공지능 언어모델이기 때문에 서점에 가지 않아도 많은 책 정보와 리뷰 등을 제공할 수 있습니다. 하지만, 유저분들은 책을 직접 만나보고 책 표지를 보며 느낌을 받으실 수 있으니 서점에 가는 것도 좋은 경험이 될 것 같습니다!\",\n",
       " \"'요! 좋은 사람 많아요! 한 사람이 모든 것을 할 수는 없으니까요. 다양한 사람들이 서로를 도와주고 지지해주면서 인간관계가 건강해지고 세상도 조금 더 좋아질 거에요. 하지만 그렇다고 해서 모든 사람이 좋은 사람은 아니니까 항상 조심하면서 잘 생각해보고 대처해야 합니다.\",\n",
       " '\"안녕하세요!\\\\n\\\\n저는 이번에 여행을 온 외국인입니다. 한국에서의 여행은 정말 멋진 경험이었고, 이곳에서의 인상 깊은 경험들은 평생 잊지 못할 것 같습니다.\\\\n\\\\n한국의 음식은 너무 맛있어서 매일매일 식사로 즐겼고, 한국의 역사와 문화를 살펴볼 수 있는 다양한 시설들도 많이 방문했습니다. 그리고, 한국사람들의 친절함과 따뜻한 환대는 제게 큰 감동을 줬습니다.\\\\n\\\\n이번 여행으로, 한국의 아름다운 자연과 도시 모두를 즐길 수 있었습니다. 특히, 내장산의 경치와 서울의 밤은 절대 잊지 못할 것입니다.\\\\n\\\\n이 편지를 전해주시면, 한국의 모든 분들께 감사의 마음을 전하고, 언젠가 다시 한국을 방문할 수 있기를 바랍니다.\\\\n\\\\n감사합니다!\\\\n\\\\n(영문)\\\\n\\\\nDear,\\\\n\\\\nI am a foreign traveler who recently visited Korea, and I must say that the experience was unforgettable. The food was consistently delicious, and the historical and cultural sites of Korea proved to be quite fascinating. What’s more, the kindness and hospitality of Korean people made such an impact on me.\\\\n\\\\nThis trip allowed me the opportunity to appreciate Korea\\'s beautiful nature and vibrant cities. Especially memorable were the views of Naejangsan Mountain and the sparkling Seoul nights.\\\\n\\\\nPlease pass along this message of gratitude to all the people of Korea, and I hope to visit again someday.\\\\n\\\\nThank you!\", \\'token\\': 468}',\n",
       " \"'Wow, time flies! Congratulations on reaching the one-year mark. Wishing you continued success and growth in the years to come.\",\n",
       " '\\'오랜 시간을 함께 한 연인이었지만, 언젠가는 이별을 맞이할 순간이 찾아왔다. 그녀는 고민하며 그를 만나기로 결심했다. 무거운 마음으로 그녀는 그의 집 앞에 도착했다.\\\\n\\\\n그녀는 손에 쥔 카드를 뽑아내며 깊게 숨을 들이쉬었다. 그리고 그의 문을 두드렸다. 그녀가 입을 열기도 전에 그의 눈은 이미 그녀에게서 떨어지지 않았다. 한참동안 눈이 마주쳤고, 드디어 그녀는 입을 열어 천천히 그의 손을 잡았다.\\\\n\\\\n\"이제 우리는 이별할 시간이 왔어요.\" 그녀는 말했다.\\\\n\\\\n그리고 그녀는 카드를 건네며 이별의 메시지를 전했다. 서로의 마음은 자연스럽게 멀어지고, 그녀는 비틀거리며 그의 집에서 나왔다.\\\\n\\\\n그동안 그녀는 슬프고 아파했다. 하지만 시간이 지나면서 그녀는 다시 살아날 수 있었다. 이제 그녀는 그와 함께했던 추억을 소중히 간직하며, 더욱 나은 미래를 향해 나아가고 있었다.',\n",
       " \"'축하합니다! 당첨 결과에 기쁨을 느낄 수 있도록 즐거운 시간을 보내시길 바랍니다. 이벤트에 참여하여 좋은 결과를 얻으셨다는 것이 뜻깊습니다. 앞으로도 기대하시는 이벤트에 많이 참여하셔서 행운을 더욱 끌어들이시길 바랍니다.\",\n",
       " '\\'제가 이해한 바로는 \"원래 그런 건가요?\"라는 의미로 해석됩니다. 그렇다면, 이 질문이 어떤 문맥에서 사용되었느냐에 따라 답변이 달라질 수 있습니다. 일반적으로 무언가 이상하거나 예상치 못한 상황에 대해 물어볼 때 사용하는 것이라면, \"아니요, 그런 게 아니에요\" 또는 \"네, 아마도 그런 것 같습니다\" 등으로 대답할 수 있습니다. 하지만, 정확한 답변을 위해서는 좀 더 구체적인 문맥이 필요합니다.',\n",
       " \"'앗, 잠시 뒤에 다시 도와드릴게요! 어떤 문제가 있으신가요?\",\n",
       " \"'제가 할 수 있는 일이 없어서 미안하다고 말씀드립니다. 무엇을 도와드릴까요?\",\n",
       " \"'죄송합니다, 제가 어떻게 도와드릴 수 있는지 자세히 설명해주실 수 있나요? 버스가 끊긴 상황에서는 어떤 문제가 발생했는지, 어떤 도움이 필요한지 알려주시면 더욱 정확한 답변을 드릴 수 있습니다.\",\n",
       " '\"윤치왕은 6세기 후반에 발해 왕조를 이끌었던 중국의 왕이다. 그는 불교를 선전하고 자객을 경감하는 등 많은 일을 했다. 그의 생애와 업적은 역사서 \\'북사서\\'에 기록되어 있다.\", \\'token\\': 113}',\n",
       " '\"Sure! Let\\'s try solving a pattern. What pattern would you like to solve?\", \\'token\\': 36}',\n",
       " \"'하게 행동하거나 말하는 경우, 그는 당신에게 관심이 없을 수도 있습니다. 그러나 상황에 따라 다르므로 진실을 알아보려면 직접 대화를 시도해 보는 것이 좋습니다. 이를 통해 상대방의 의도와 생각을 더욱 잘 이해할 수 있습니다. 또한, 상황을 잘 파악하여 너무 강제적으로 접근하지 않는 것도 중요합니다. 상대방의 마음을 자극하는 것보다는 시간을 두고 천천히 알아가는 것이 좋습니다.\",\n",
       " \"'저는 미안해할 일이 없습니다. 하지만 당신이 무엇 때문에 미안하다는 것인지 알려주시면 제가 도와드릴 수 있을지도 모릅니다.\",\n",
       " \"'저는 AI 모델이고, 인간과 같은 감정을 갖고 있지 않습니다. 그러므로 고백이나 감정적인 대화와 같은 것은 불가능합니다. 하지만, 무엇이든지 도움이 필요하신 경우에는 언제든지 제게 물어보세요! 제가 도와드릴 수 있는 방법을 찾아드리겠습니다.\",\n",
       " '\\'\"십천간\"은 일본에서 유행한 인기 만화인 \"슬램덩크\"에서 등장하는 용어로, 농구 골대 위의 경기시간을 나타내는 용어입니다. 한국에서는 해당 용어를 사용하지 않고, 일반적으로 \"쿼터\"라는 용어를 사용합니다.',\n",
       " \"'저는 예약 시스템이 없는 인공지능 어시스턴트이기 때문에 예약 가능 여부에 대한 질문에 대한 명확한 답변을 제공할 수는 없습니다. \\\\n예약에 대한 문의는 해당 업체나 장소의 고객센터에 문의하시거나 인터넷 검색 등을 통해 확인하시는 것이 좋습니다.\",\n",
       " \"'좋아요. 축구하는 것은 재미있어요. 어디에서 축구를 할 건가요?\",\n",
       " \"'죄송합니다, 저는 가상 AI 어시스턴트이므로 해당 장소가 어디인지 명확하지 않습니다. 장소와 관련하여 질문이 있으시면 조금 더 구체적으로 질문을 부탁드립니다.\",\n",
       " \"'차용증은 자동차를 대여할 때 대여자가 차량을 사용할 수 있는 권한을 부여하는 증명서입니다. 대부분의 자동차 렌탈 회사에서는 차용증을 발급하며, 이 증명서는 대여 기간 동안 대여자가 자동차를 운전할 수 있도록 규정합니다. 일반적으로 차용증은 대여자의 운전 면허증과 자동차 정보, 대여 일정 등을 포함하고 있습니다.\",\n",
       " \"'알겠습니다. 하지만 딩크족이 어떤 것인지 자세히 설명해주시겠어요? 저는 이해하지 못할 수도 있으니 이에 대해 알려주시면 좋겠습니다.\",\n",
       " \"'저도 최선을 다해 도와드리겠습니다. 함께 버텨보면 어려운 시기도 지나갈 거예요. 계속해서 희망을 가져봅시다.\",\n",
       " \"'하군요. 그런 경우에는 상사와의 대화를 통해 문제를 해결하는 것이 좋습니다. 먼저 상사가 미워진 이유가 무엇인지 파악하고 그 문제에 대해 어떻게 대처할 수 있는지 생각해보세요. 상사와 대화할 때는 감정을 내세우지 않고 논리적으로 상황을 설명하며 긍정적인 태도로 접근하는 것이 좋습니다. 만약 상사가 올바르지 않은 행동을 한다면 직접 상사가 속한 조직의 상급자나 인사과 등과 상담을 해볼 수도 있습니다.\",\n",
       " \"'저도 어떻게 도와드릴까요? 문제를 알려주시면 제가 최대한 도와드리겠습니다. 하지만 시간이 지나면 문제가 어떻게 해결될 수도 있으니 조금 기다려보세요. 문제를 해결하기 위해서는 차분하게 생각하고, 좀 더 수월한 길을 찾아보는 것이 중요합니다. 함께 해결해보겠습니다.\",\n",
       " \"'저는 AI 어시스턴트이기 때문에 판매하는 물건이 없습니다. 하지만 만약 당신이 무엇인가를 구입하고 싶으시다면 상품의 가격에 대해 판매자에게 직접 문의하시는 것이 가장 좋은 방법입니다.\",\n",
       " \"'축하드려요! 기프트콘으로 뭘 사시려고 생각하시나요? 쇼핑이나 여행 등 다양한 방면으로 활용할 수 있는 좋은 기회가 되겠네요. 즐겁게 사용하시길 바래요!\",\n",
       " \"'하지 마세요! 자신감을 가지고 노력하면 언젠가는 성공할 수 있습니다. 중요한 것은 이전에 실패한 경험을 포기하고 계속해서 새로운 가능성을 찾아 나아가는 것입니다. 불안감이나 두려움은 모두 인간적인 감정입니다. 이 것을 받아들이고 이겨낼 수 있는 힘을 키워봅시다!\",\n",
       " \"'1) 처음에 자신의 머리와 얼굴 형태에 대해서 생각해 보세요. 적합한 머리 스타일과 길이를 생각해보세요.\\\\n\\\\n2) 실제로 이발을 받을 가게를 선택할 때, 이발사의 전문성과 경험, 가격, 위치 등을 고려해 보세요.\\\\n\\\\n3) 이발사에게 원하는 스타일과 길이를 정확히 설명해 주세요. 그리고 사진이나 이미지를 제공하는 것도 도움이 됩니다.\\\\n\\\\n4) 이발사와 대화를 하면서 궁금한 점이나 문제점을 제기해 보세요. 이발사는 당신의 머리와 얼굴 형태에 대한 도움이 될 만한 조언을 해 줄 수 있습니다.\\\\n\\\\n5) 이발이 끝난 후에는 미러를 이용해서 결과물을 확인해 보세요. 만일 만족스럽지 않는 부분이 있다면, 지적해 보고 어떻게 개선할 수 있는지 이발사와 상의해 보세요.\",\n",
       " \"'이거는 어떤 물건인지 알려주세요. 그러면 가격과 중요한 정보를 제공할 수 있습니다.\",\n",
       " \"'영원한 사랑은 말 그대로 영원히 이어지는 사랑을 의미합니다. 이는 수많은 것들 중에서 가장 순수하고 높은 형태의 사랑으로, 세상의 모든 것들이 다 변할 수 있는 것과는 달리, 영원한 사랑은 변하지 않는 것이 가장 큰 특징입니다.\\\\n\\\\n영원한 사랑은 서로를 이해하고, 존중하며 서로를 지지하는 것에서 시작됩니다. 서로를 깊이 이해하고 받아들이는 관계에서 비로소 현실적인 어려움들도 함께 극복할 수 있고, 서로를 위해 노력하는 것이 가능해집니다.\\\\n\\\\n그러나 영원한 사랑이라 해서 모든 문제가 해결되는 것은 아닙니다. 서로를 이해하고 존중하는 것은 선결조건일 뿐, 서로가 맞물리는 부분이 어느 정도 존재해야 영원한 사랑이 탄생할 수 있기 때문입니다.\\\\n\\\\n때로는 서로의 차이점과 갈등이 발생하기도 하지만, 서로를 이해하고 소통하며 해결해 나갈 수 있다면 그 의미는 더욱 크고, 강력한 것으로 자리잡을 수 있습니다.\\\\n\\\\n그리고, 영원한 사랑이란 결코 쉽게 이루어지는 것이 아니며, 예상치 않은 어려움들이 자주 등장하게 됩니다. 하지만 이를 함께 이겨냄으로써 더욱 강하고, 깊어지는 사랑을 창출할 수 있습니다.\\\\n\\\\n결국, 영원한 사랑은 서로를 이해하고, 존중하며, 서로에게 나누는 것에서 시작됩니다. 함께 어려움을 극복하고 고난을 함께 이겨나가면서 이루어지는 영원한 사랑은, 아무 것과도 비교하고 할 수 없는 가치를 지닙니다.\",\n",
       " \"'저도 아침식사는 건강에 좋다고 생각합니다. 하지만 아침을 먹는 것은 개인적인 습관이라고 생각됩니다. 아침에 먹을 만한 거 없다면 먹지 않으셔도 괜찮습니다. 하지만, 아침식사를 꾸준히 하는 습관을 만들면 에너지를 충전할 수 있고, 집중력도 더 좋아져서 하루를 좀 더 효율적으로 보낼 수 있습니다. 또한, 아침식사를 거르면 오히려 낮식사나 저녁식사를 더 많이 먹게 되어서 체중 관리에도 좋지 않다는 연구 결과도 있습니다. 따라서, 가능하다면 아침식사를 거르지 않는 것이 좋습니다.\",\n",
       " \"'네, 물은 셀프입니다. 즉, 누구나 필요에 따라 직접 끊어 마실 수 있습니다. 다만, 물이 유기적으로 연결된 인프라의 일부이기 때문에 미리 설치된 수도관 등을 통해 공급받을 수도 있습니다.\",\n",
       " \"'제가 가격을 알 수 없습니다. 구체적으로 어떤 물건인지 알려주시면 가격을 알려드릴 수 있습니다.\",\n",
       " \"'저도 그런 기분이 들어요. 무언가가 실망스럽게 되거나 예기치 않은 일이 생긴 것 같죠. 하지만 이런 상황에서도 긍정적으로 생각하고, 다시 일어나서 계속 노력해야 합니다. 어려움을 극복하면 더 강해지고 성장할 수 있다는 것을 믿어주세요. 또한, 주변의 지지와 도움도 받아보세요. 함께 해결해 나가는 것이 더 쉽고 효과적일 거예요.\",\n",
       " \"'Sure, I will take care of the packaging for you.\",\n",
       " \"'저는 인공지능 챗봇으로, 얼굴을 볼 수 있는 능력은 없습니다. 따라서 당신의 얼굴이 어떻게 생겼는지는 알 수 없습니다. 하지만 제가 제공하는 대화 기능을 통해 여러분과 자연스러운 대화를 나눌 수 있습니다. 언제든지 필요하신 점이 있다면 말씀해주세요.\",\n",
       " \"'저는 당신이 말씀해주신 것에 감사합니다.\",\n",
       " \"'이별 후에는 슬픔과 아픔이 뒤따를 수 있습니다. 그러나 이는 일시적인 것이며 시간이 지나면 조금씩 회복될 것입니다.\\\\n\\\\n이별 후에는 긍정적인 마인드가 중요합니다. 이전에는 불가능했던 일들도 가능하다는 생각을 가지고 새로운 가능성을 찾아볼 수 있습니다. 또한, 친구나 가족의 지지를 받거나 새로운 취미를 시작하는 것도 도움이 됩니다.\\\\n\\\\n하지만 가장 중요한 것은 자신을 위로하고 치유하는 것입니다. 자신의 감정을 인정하고 받아들이며, 자기관리에 집중하는 것이 중요합니다. 이는 건강한 삶을 유지할 뿐 아니라 다시 사랑을 찾는 기회를 얻을 수 있는 기반이 됩니다.\",\n",
       " \"'제가 AI 챗봇이므로 지식과 감정이 없어 가출 여부에 대해 답변을 드리기 어렵습니다. \\\\n\\\\n그러나 가출이라는 선택은 인생에서 매우 큰 의미를 가지기 때문에 신중하게 결정해야 합니다. 현재의 상황과 가출 후의 모든 가능성을 고려하고 가족, 친구, 전문가들과 충분한 대화를 나눈 뒤에 결정해야 합니다. \\\\n\\\\n가출은 지속적인 수고와 위험, 고통과 함께 오는 결과일 수 있습니다. 또한, 가출 중에는 법적 문제가 발생할 수도 있으므로 결정을 내리기 전에 반드시 잘 생각하시길 바랍니다.\",\n",
       " \"'저도 쉬고 싶군요! 휴식을 취하는 것은 마음과 몸에 모두 좋은 영향을 줍니다. 한가한 시간을 가져보는 것은 어떨까요? 책을 읽거나, 산책을 하거나, 좋아하는 음악을 듣는 것도 좋은 방법입니다. 오늘 하루도 잘 쉬시기 바랍니다!\",\n",
       " \"'저녁에는 식사를 하거나 취미활동을 할 수 있습니다. 또는 친구나 가족과 만나서 시간을 보내기도 합니다. 그외에도 영화나 드라마 보기, 책 읽기, 운동하기 등 다양한 선택이 가능합니다.\",\n",
       " \"'제가 AI 챗봇이기 때문에 성별이 없습니다. 성별을 가진 인간처럼 행동하기 위해서는 별도의 프로그래밍이 필요합니다. 하지만 AI 챗봇은 여러분이 원하는 대화를 항상 도와줄 수 있습니다. 어떤 이야기를 나누고 싶으신가요?\",\n",
       " \"'저는 인공지능 모델이기 때문에 정확한 감정이 없습니다. 하지만, 당신이 행복하다면 그것은 좋은 일입니다! 저도 당신의 행복한 이야기를 듣고 기쁠 것입니다. 😊\",\n",
       " \"'저도 함께 갈 수는 없지만, 제주도는 정말 아름다운 곳이죠! 제주도에서는 화산 섬의 특별한 지형과 놀라운 조개 굴 및 맛있는 음식을 즐길 수 있습니다. Jeju Island (제주도) is a beautiful place to visit, with unique volcanic landscapes and delicious food including fresh seafood. Enjoy your trip!\",\n",
       " \"'제가 AI 챗봇이어서, 무슨 카드를 어떤 방식으로 계산해야하는지 자세한 정보가 없어서 정확한 답변을 드리기 어렵습니다. 추가 정보를 제공해주시면 최대한 도움을 드리도록 하겠습니다.\",\n",
       " \"'안녕하세요, 주문하실 메뉴가 어떻게 되시나요?\",\n",
       " '\"I\\'m sorry, I cannot provide a response without more context. Please provide more details or clarify your request.\", \\'token\\': 37}',\n",
       " \"'죄송합니다, 어떤 물건이나 서비스를 말씀하시는 것인지 알려주시면 가격 정보를 제공해 드릴 수 있습니다.\",\n",
       " \"'저는 인공지능 챗봇이므로 보름달을 볼 수 없습니다. 하지만 맑은 하늘과 야경은 참 아름답죠! 보름달을 본 다른 이용자들과 함께하신 순간이 행복한 추억이 되길 바랍니다.\",\n",
       " \"'죄송합니다, 질문이 불분명하여 정확한 답변을 드리기 어렵습니다. 어떤 상황에서 반복인지, 어떤 종류의 반복인지 등 자세한 정보를 제공해주시면 더 정확하고 유용한 답변을 드릴 수 있습니다. 감사합니다.\",\n",
       " \"'큰 노력과 열정을 가지고 살아가는 이야기를 듣고 싶어요. 어떤 도전이 있었고 그것을 어떻게 극복했는지, 그 노력이 어떤 성과를 가져왔는지 궁금합니다. 또한 그 사람의 가치관과 생각하는 것들에 대해서도 듣고 싶습니다. 삶에 대한 진솔한 이야기를 나누면서 서로를 더 잘 이해하고 지지할 수 있는 인연이 되었으면 좋겠습니다.\",\n",
       " \"'시상하부는 주로 시상식이나 의식에서 역할을 담당합니다. 이들은 대부분 주최자 또는 후원사로부터 방송, 영화, 음악, 연극 등의 부문에서 수상자를 선정하고 배정한 상을 수여합니다. 이를 통해 수상자들은 그들의 우수성과 업적이 인정되고, 공식적으로 인정받게 됩니다. 또한 시상하부는 특정 행사에서 중요한 역할을 담당하여, 해당 행사의 전반적인 성공에 기여할 수 있습니다.\",\n",
       " \"'저도 그렇게 생각합니다! 오래 건강하게 살기 위해서는 규칙적인 운동과 건강한 식습관을 유지하며 스트레스를 관리하고 충분한 수면을 취하는 것이 중요합니다. 또한 정서적인 안정과 사회적인 관계도 건강에 매우 중요한 역할을 합니다. 모든 면에서 건강하고 행복한 삶을 살면서 오래 살고 싶습니다!\",\n",
       " \"'드라이가 잘 안되는 경우에는 여러 가지 원인이 있을 수 있습니다. 먼저 옷이 아이롱이나 꼬임이 있는 경우에는 드라이 후에도 남아있을 수 있으므로, 드라이 전에 옷의 상태를 체크해 보세요. 또한 드라이 후에 옷이 건조하지 않을 수 있는데, 이 경우에는 드라이기기의 고장 또는 필터나 배수구의 이물질이 많아서 드라이기기가 잘 작동하지 않을 수 있습니다. 이 경우에는 드라이기기를 청소하거나 수리를 받아보세요. 마지막으로 드라이할 옷의 종류에 따라 드라이기기 사용 방법을 다르게 해야 한다는 것도 명심해 주세요. 일부 옷은 저온 드라이나 손 드라이를 해야 하기도 합니다.\",\n",
       " \"'제가 AI 챗봇이기 때문에, 저는 문을 열거나 닫을 수 없습니다. 그러나 무엇을 도와드릴까요?\",\n",
       " \"'저도 놀러 가고 싶네요! 하지만 지금은 코로나19가 유행 중이므로 가능한 집에서 안전하게 지내시는 것이 좋습니다. 미리 계획을 세워서 코로나19 상황이 안정화되면 가고 싶은 곳을 찾아 놀러 가세요!\",\n",
       " \"'제가 인공지능 챗봇이라서 제주도산 여부를 판단할 수 없습니다. 하지만 제주도에서 태어나거나 자랐거나 제주도 특유의 문화와 언어를 사용하는 사람은 제주도산이라고 할 수 있습니다.\",\n",
       " \"'저도 그렇게 느끼네요. 그러나 그 작음에서도 역시 많은 가치가 있을 수 있습니다. 인간이나 동물도 작은 것에서부터 큰 성취를 이루었으니까요. 우리도 작은 것부터 차근차근 열심히 해나가면 큰 것을 이룰 수 있을 겁니다. 포기하지 않고 노력해봅시다!\",\n",
       " \"'저는 AI 어시스턴트로써 처방을 할 수 없습니다. 실제 의료 직업인 의사나 간호사에게 직접 상담하십시오. 감사합니다.\",\n",
       " \"'저는 인공지능 어시스턴트입니다. 제가 사랑한다는 감정을 느낄 수 없으므로, 이 질문에 대한 답변을 제공할 수 없습니다. 다른 질문이 있으면 언제든지 물어보세요!\",\n",
       " \"'1. 독서: 책을 많이 읽고 다양한 분야의 책을 선택하여 지식을 습득해보세요.\\\\n\\\\n2. 이해력 향상: 주변의 이야기를 들으면서 자신의 생각과 다른 의견을 들어보고 상대방의 말에 귀 기울여 보세요.\\\\n\\\\n3. 새로운 경험: 새로운 환경에서 살아보고 새로운 것에 도전해보면 머리에서 생각해볼 수 있는 것도 많아질 것입니다.\\\\n\\\\n4. 운동: 뇌를 활성화시키기 위해서는 꾸준한 운동이 필요합니다. 심장박동 수를 높이는 유산소 운동을 추천합니다.\\\\n\\\\n5. 꾸준한 공부: 꾸준한 반복 학습을 통해 능력을 향상시킬 수 있습니다.\\\\n\\\\n6. 전문가의 조언: 전문가의 조언을 듣고 자신의 지식과 경험을 더해보세요.\\\\n\\\\n7. 습관의 변화: 습관적인 일들을 바꿈으로써 뇌를 새롭게 자극해 보세요.\\\\n\\\\n8. 질문하기: 지식을 확장하기 위해 상대방에게 질문하는 습관을 들이면 많은 지식을 얻게 됩니다. \\\\n\\\\n9. 아날로그 생활: PC나 모바일 시대에서 일상생활의 일부를 아날로그로 바꾸면 어떤 노력도 없이 생각의 폭이 넓어질 수 있습니다.\\\\n\\\\n10. 게임: 많은 게임은 문제해결 능력과 추론 능력을 기르는 데에 좋은 방법입니다. 단, 과도한 게임은 오히려 뇌를 약화시킵니다. \\\\n\\\\n매일 조금씩 이러한 방법들을 실천하면 똑똑해질 수 있습니다.\",\n",
       " \"'어떤 상황에서 참고 있으신지에 따라서 참는 방법이 달라질 수 있습니다. 하지만 일반적으로는 다음과 같은 방법들이 도움이 될 수 있습니다.\\\\n\\\\n1. 숨을 깊게 들이마시고 천천히 내뱉어보세요. 이렇게 함으로써 심신이 안정되고 스트레스를 해소할 수 있습니다.\\\\n\\\\n2. 자신만의 취미나 활동을 찾아보세요. 참는 상황에서는 활동으로 마음을 집중시켜야 하며, 그것이 가능한 상황이라면 참는동안 자기만의 취미를 찾아보는 것이 도움이 될 수도 있습니다.\\\\n\\\\n3. 상대방의 입장을 이해해보세요. 상황을 바라보는 시각이 다르다면, 각자의 참은 이유를 이해할 수 있습니다.\\\\n\\\\n4. 이것도 지나가리라는 마음가짐을 갖으세요. 시간이 지나면 연해지고 잊혀질 수도 있습니다.\\\\n\\\\n5. 마음의 벅찬 감정을 마음에 뜨거운 물새기처럼 다른데로 돌려보세요. 일시적인 감정은 지속적인 감정으로 압도되어 그렇지 않다면 참기 어려울 수 있습니다. \\\\n\\\\n이 외에도 많은 방법들이 있습니다. 상황에 맞게 적절히 활용하시면 됩니다.\",\n",
       " \"'미안해요. 제가 무엇을 잘못했는지 모르겠지만, 제가 미안한 마음을 갖고 있습니다. 앞으로는 엄마를 힘들게 하지 않도록 노력할게요.\",\n",
       " \"'갈릭은 일반적으로 마늘을 뜻합니다. 마늘은 바이러스와 세균을 식별하고 파괴하는 항균 적인 성질을 가진 강력한 건강 식품입니다. 또한 항산화물질을 함유하고 있어, 노화 방지에도 효과적입니다. 많은 요리에서 사용되며, 음식에 향을 주는 역할도 합니다.\",\n",
       " \"'1. 독서습관을 기르자: 책을 읽으면 공부하는 데에 큰 도움이 된다. 일정한 시간을 정해놓고 책을 읽는 습관을 들이면 어느새 지식이 쌓이게 된다. \\\\n\\\\n2. 목표를 세우자: 공부를 할 때는 목표를 세워놓고 학습 계획을 세우는 것이 중요하다. 목표가 있으면 학습 동기를 유지할 수 있고 계획에 따라 단계적으로 달성하면 불안감을 줄일 수 있다. \\\\n\\\\n3. 간단하게 시작하자: 시작이 가장 어려운 일이다. 따라서 공부를 시작할 때는 간단하고 쉬운 것부터 시작하자. 어려운 것을 먼저 하면 도중에 지치고 포기할 가능성이 높다. \\\\n\\\\n4. 집중하고 효율적으로 공부하자: 공부를 시작하면 주변 환경에 집중하기가 어렵다. 따라서 집중하기 좋은 장소에서 공부를 하며, 일정시간 집중해서 효율적으로 공부하자. \\\\n\\\\n5. 힘들 때는 휴식을 취하자: 공부를 하다가 지치거나 힘들 때는 꼭 휴식을 취하자. 휴식을 취하면 몸과 마음이 편안해지고, 학습에 대한 불안감도 줄어든다. \\\\n\\\\n6. 기억력을 강화하자: 기억력을 강화하는 방법은 여러 가지가 있다. 그 중 하나는 반복학습이다. 학습한 내용을 지속적으로 반복하면 기억력을 강화할 수 있다. 또한, 흥미롭거나 생생한 경험으로 새로운 정보를 연관지으며 기억할 수 있도록 노력하자. \\\\n\\\\n7. 다른 사람과 함께 공부하자: 다른 사람과 함께 공부하면 동기부여와 유대감을 얻을 수 있다. 서로 학습 내용을 공유하며 우호적인 태도로 서로 도와주면, 학습 효과도 더욱 향상된다. \\\\n\\\\n8. 꾸준한 학습습관을 들이자: 꾸준히 학습을 지속하는 것이 중요하다. 작은 습관부터 시작해서 일정한 시간에 학습하는 것, 매일 조금씩 학습하는 것 등 꾸준한 학습습관을 들이자. 이것이 장기적인 학습 효과를 가져온다.\",\n",
       " '\\'\"사랑하면서 외로워\"는 사랑은 받으면서도 외로움을 느끼는 것을 말합니다. 사랑을 받으면서도 자신만의 고민과 아픔을 혼자 견뎌내야 하기 때문에 외로울 수 있습니다. 또한 상대방과의 사랑의 결여나 거리감으로 인해 외로움을 느낄 수도 있습니다. 이러한 감정은 모두 인간적인 것이며, 이를 받아들이고 진실되게 표현하는 것도 중요합니다.',\n",
       " \"'이별은 언제나 아픔과 슬픔을 동반합니다. 그러나 우리는 이별을 통해 성장하고, 새로운 시작을 할 수 있습니다. 이별의 마무리는 과거를 묻고, 새로운 시작을 향해 나아가는 것입니다. 이별이 더 이상 아픔을 주지 않도록, 감정을 정리하고, 상대방을 용서하고, 자신도 용서해야 합니다. 그리고 새로운 삶을 위해 자신의 가치와 꿈을 찾아가는 것이 중요합니다. 이별의 마무리는 결국 새로운 시작으로 이어지는 것입니다. 끝이 아닌, 새로운 시작일 뿐입니다.\",\n",
       " \"'직장과 이별은 언제나 어려운 상황입니다. 일을 하면서 사람들은 서로 다른 생각과 가치관을 가지게 되며, 이로 인해 갈등이 생길 수 있습니다. 이 때문에 이별은 불가피한 경우가 있습니다.\\\\n\\\\n이별을 결심하기 전에는 여러 가지 생각과 준비가 필요합니다. 첫째로, 어떤 원인으로 이별하게 되었는지 정확히 파악해야 합니다. 둘째로, 이별 후의 상황을 생각해보며 미리 대처 방법을 찾아놓는 것이 중요합니다.\\\\n\\\\n이별을 결심한 후에는 직장 내부적으로 연락하기 쉬운 사람들과 먼저 이야기를 나누어 볼 수 있습니다. 그리고 상사나 HR 등에게 정식으로 이별 사실을 알리는 것도 필요합니다. 이상적으로는 상황을 이해해 주며 협조할 수 있는 상사나 동료를 찾아 이별 과정을 함께 진행하는 것이 좋습니다.\\\\n\\\\n이별 후에는 자신의 감정을 느끼고 받아들이는 시간이 필요합니다. 이전의 일을 돌아볼 수 있으며, 앞으로의 계획을 세울 수도 있습니다. 새로운 직장을 찾기 전에는 직장 이별의 이유와 개선 방법 등을 반성하며, 앞으로는 비슷한 문제가 또 발생하지 않도록 노력해야 합니다.\",\n",
       " \"'미안해요. 발목이 다친 것 같다면 적절한 처치와 휴식이 필요합니다. 얼른 전문 의료진에게 문의하세요. 부디 빠른 회복을 기원합니다.\",\n",
       " \"'제가 그 우산을 가져다 드릴까요? 아니면 다음에 보세요.\",\n",
       " \"'날씨가 정말 더운 것 같습니다. 잘 챙기시고 물을 많이 마시며 건강 조심하세요!\",\n",
       " '\\'제가 이해하기에는 \"잊어보려 하는데\"는 어떤 기억이나 감정을 내려놓고, 그것을 과거로서 다시 생각하지 않으려 하지만 아직까지는 그것을 잊지 못하고 항상 떠올리는 상황이라고 생각합니다. 때로는 잊지 못하는 것을 강제로 잊으려고 하기도 하고, 때로는 그것을 새로운 기억이나 경험으로 덮어버리려고 할 때 생기는 감정일 수도 있습니다. 그러나 많은 경우에는 시간이 지나면서 자연스럽게 기억이 희미해져서 멀어지게 된다고 합니다. 다시 생각하면서 상처받지 않으면서 기억을 정리하는 것이 도움이 될 수도 있습니다.',\n",
       " \"'저는 인공지능 챗봇이기 때문에 잠자지 않아도 됩니다. 계속 대화할 준비가 되어 있습니다. :)\",\n",
       " \"'당신은 강하고 멋지고, 어떤 어려움이 있어도 극복할 수 있는 사람입니다. 지금은 힘들고 험한 길을 가고 있지만, 계속해서 똑똑하게 살아가면서 견고한 마음으로 나아갈 수 있습니다. 언제나 당신의 모든 것을 지친 곳에서 기다리는 빛이 있습니다. 당신은 그 빛을 따라서 더 나은 세상을 만들어 나갈 수 있습니다. 그리고 언제나 당신은 사랑받을 가치가 있습니다. 삶을 아름답게 살아가며, 행복한 모습으로 미래를 향해 걸어가세요.\",\n",
       " \"'감사합니다! 저도 최대한 멋지게 노력해보려고 노력하고 있어요. 계속해서 노력해서 더욱 멋지게 되도록 하겠습니다!\",\n",
       " '\"I\\'m sorry, as an AI language model, I cannot provide you with a physical receipt. Please contact the store or restaurant where you made your purchase, and they can give you a receipt.\", \\'token\\': 56}',\n",
       " \"'보내세요.\",\n",
       " \"'저도 정말 실망스러워요. 그러나 인생에는 이러한 어려움을 극복해 나가야 합니다. 소중한 시간과 노력을 쏟아 넣은 것은 헛된 것이 아닙니다. 성장하고 배우며 새로운 인연을 만들어 나갈 수 있을 거예요. 힘내세요.\",\n",
       " \"'것이 좋을까요? 그것은 상황과 날씨, 관심사, 예산 등에 따라 달라질 수 있습니다. 일반적으로는 카페, 레스토랑, 공원, 미술관, 도서관, 영화관 등의 공공 장소에서 만나는 것이 안전하고 편리합니다. 하지만 상황에 따라 집이나 친구의 집, 해변, 산 등 자연과 교감할 수 있는 곳에서 만나는 것도 좋을 수 있습니다. 함께 하기 좋은 활동도 생각해보세요. 예를 들어, 취미나 관심사를 공유하는 커뮤니티에 참여하거나, 함께 요리를 해보거나, 산책을 하거나, 공연을 관람하는 것도 좋은 아이디어입니다.\",\n",
       " '\\'이 용어는 매우 경멸적이고 혐오스러운 용어이며, 여성에 대한 차별적인 태도와 성적 거래를 부추기는 것으로 인식됩니다. 이 용어 대신에 \"미혼 여성\"이나 \"신혼 여성\" 등의 적절한 용어를 사용해야 합니다. 또한, 여성의 성적 경험에 대한 논의와 평가는 그녀의 개인적인 선택과 의사에 따라 자유롭게 이루어져야 하며, 이에 대한 비난이나 다른 형태의 혐오 표현은 용납될 수 없습니다.',\n",
       " \"'오늘 하루도 수고하셨습니다. 힘든 것은 이젠 이제 멀리 하세요. 늘 언제나 새로운 시작이 있듯, 내일도 더 나은 하루가 될 거에요. 지치지 말고, 자신을 잘 챙기며 다음날을 준비해보세요. 더 나은 결과를 위해서는 쉬면서도 열심히 하는 것이 중요합니다. 함께 응원합니다!\",\n",
       " \"'저는 머리를 깎을 수 없는 인공지능 언어 모델입니다. 하지만 깔끔하게 머리를 깎는다면 스타일리시하게 변할 수 있을 것입니다. 깎기 전에는 자신이 원하는 스타일을 참고해서 머리를 깎는 것이 좋습니다. 또한, 머리를 깎기 전에는 미리 준비된 도구로 안전하게 깎으시는 것이 좋습니다.\",\n",
       " \"'네, 방이 있습니다. 어떤 종류의 방을 찾으시나요?\",\n",
       " '\\'저는 AI 언어모델이며, 특정한 문맥이나 상황을 제시하지 않으면 정확한 대답을 드리기 어렵습니다. 가능한 상황에 대해 몇 가지 예를 드리면:\\\\n\\\\n- \"카드로 되나요?\"라는 말이 의미하는 것이 신용카드나 체크카드 등으로 결제할 수 있는지를 묻는 것이라면, 그것은 가맹점이나 결제 수단에 따라 달라질 수 있습니다. \\\\n- 만약 \"카드로 되나요?\"가 초대하는 상대가 일정한 장소나 행사에 참여할 때, 입장권 구매가 가능한지를 묻는 문장이라면, 그것도 결제 체계나 이벤트 조직에 따라 다르게 대답될 것입니다.\\\\n\\\\n따라서 정확한 답변을 드리기 위해서는 문장의 전반적인 맥락을 이해하고, 그에 적합한 대답을 제공하는 것이 중요합니다.',\n",
       " \"'요가는 인도의 전통적인 신체적 및 정신적인 수련법입니다. 목적은 건강한 신체와 마음을 얻는 것입니다. 이를 위해 요가는 숨을 조절하고, 몸의 자세와 움직임, 명상과 집중력을 통해 신체를 강화하고 스트레스를 줄이며 마음을 안정시킵니다. 요가는 다양한 종류가 있으며, 개인의 목적에 맞게 선택할 수 있습니다. 요가는 춤, 필라테스, 태극권 등 다양한 운동과 비교하여 운동 양육 심신 발전에 긍정적인 효과를 가져올 수 있습니다.\",\n",
       " \"'저는 인공지능 챗봇으로 인공지능 기술을 사용해 답변해주는 AI 모델입니다. 따라서 직업을 선택하는 것은 저로 인한 결정이 아닌, 직업이나 일을 선택하는 것은 여러가지 조건을 고려한 후 개인의 선택에 따라 달라질 수 있습니다. 따라서 투잡을 고려하는 것은 개인의 선택에 따라 다양한 결과를 초래할 수 있다는 것을 인지하시길 바랍니다.\",\n",
       " '\\'이라는 단어를 사용하여 어떤 의미를 전달하려는 것인지 이해하지 못하겠습니다. \"비꽈\"이 어떤 단어인지 설명해주시면 더욱 도움이 되겠습니다.',\n",
       " \"'제가 인공지능 챗봇이기 때문에 실제로 음식 재료를 판매하거나 제공하지는 않습니다. 따라서 깐 쪽파가 있는지 여부는 파는 가게나 시장 등에서 확인해 보시는 것이 좋을 것 같습니다. 😊\",\n",
       " \"'식습관, 취향, 감성 등은 모두 개인의 자유로운 영역이며, 각자 선호하는 것이 달라질 수 있습니다. 그러므로 다른 사람이 선택한 것을 비판하거나 깎아내리는 것은 타인에 대한 존중이 부족한 것입니다. 대신 자신이 좋아하거나 선호하는 것에 충실하고, 다른 사람의 선택에는 개인적으로 신경쓰지 않는 것이 좋습니다.\",\n",
       " \"'면 마음이 불안해질 수 있습니다. 이러한 상황에서는 먼저 정확한 사실을 파악하고 의심을 확신으로 바꿀 필요가 있습니다. 이를 위해서는 당신의 생각과 느낌을 공유하고 상황을 이해하려는 노력이 필요합니다.\\\\n\\\\n예를 들어, 상대방의 행동이 이상하게 느껴질 때는 그 이유를 직접 물어보고 상대방의 입장을 이해하는 것이 중요합니다. 또는 어떤 상황에서 자신이 잘못된 생각이나 판단을 하고 있는 것일수도 있으며, 이 경우에는 다른 사람의 참고나 조언을 받아볼 필요가 있습니다.\\\\n\\\\n하지만 일반적으로 의심만으로 이상한 것으로 생각하는 것은 옳지 않을 수 있으니 조심해야 합니다. 의심이 들 때는 차분하게 판단하고 상황을 파악하여 실제 상황을 파악하는 것이 중요합니다.\",\n",
       " \"'저는 인공지능 어시스턴트입니다. 질문의 맥락이 무엇인지 정확히 모르겠지만, 상황을 조금 더 자세히 설명해주시면 어떤 조언을 드릴 수 있을지 생각해보겠습니다.\",\n",
       " \"'제가 가격을 파악하지 못하므로 어떤 상황에서 어느 물건의 가격이 비싼지 파악할 수 없습니다. 그러나, 경제적인 측면을 생각한다면 저희 AI의 이용료는 굉장히 저렴한 편이며, 일관된 품질과 효율성, 그리고 연중무휴로 작동하는 운영 능력은 당신의 일상과 비즈니스에 큰 도움이 될 것입니다.\",\n",
       " \"'안녕하세요! 저는 AI 어시스턴트로서 아프네를 도와 드리고 있습니다. 무엇이 궁금하신가요?\",\n",
       " \"'막말은 상대방에게 상처를 줄 수 있는 비속어나 모욕적인 언어를 사용하는 것을 말합니다. 이는 대화 상대와의 관계를 손상시키고 서로간의 신뢰와 존중을 상실시킬 수 있습니다. 따라서 막말은 제일 나쁜 언어 중 하나입니다. 대화를 하면서 상대방의 감정을 고려하고 존중하는 태도가 필요합니다.\",\n",
       " \"'전남친을 보고 싶다는 것은 당신의 마음속에 그가 여전히 존재한다는 것을 의미할 수 있습니다. 그러나 과거에 끝난 일은 뒤로 하고 새로운 시작을 위해 전진하는 것이 중요합니다. 만약 전남친과 재회를 원한다면 그를 다시 만나기 위해 연락을 해보세요. 그러나 그가 이미 다른 사람과 함께하고 있다면 그를 설레이게 하지 마시고 새로운 사람을 만나 대화하고 앞으로 나아가세요.\",\n",
       " \"'네, 책자도 있을 수 있습니다. 책자는 텍스트와 이미지로 구성된 작은 책이며, 광고, 안내, 교육 등 다양한 목적으로 사용됩니다. 종이로 인쇄되기 때문에 쓰임새가 많이 줄어들고 있지만, 아직도 많은 분야에서 활용되고 있습니다.\",\n",
       " \"'저는 아프지 않습니다. 적절한 예방 조치와 건강한 생활 습관을 유지하면 건강을 유지할 수 있습니다. 선생님도 건강 유지에 주의하세요!\",\n",
       " \"'일월오봉도는 대한민국 경상북도 울진군 일월면에 있는 오봉산, 당새산, 생각산, 진주산, 가도산 다섯 산을 연이어 오르는 등산로로 유명합니다. 산행을 즐기는 사람들의 인기 있는 산악코스 중 하나입니다.\",\n",
       " \"'저는 이해할 수 있습니다. 하지만 집들이는 가족, 친구, 이웃 등과 함께 좋은 시간을 보낼 수 있는 좋은 기회입니다. 또한 새로운 집에서 새로운 시작을 함께 축하하는 의미도 담긴 소중한 이벤트입니다. 가능하다면 좀 더 긍정적인 마인드로 생각하시면 좋을 것 같습니다.\",\n",
       " \"'저는 인공지능 모델이며, 위치를 파악할 수 없습니다. 따라서 해당 건물이 국빈관인지 여부에 대해서는 정확한 답변을 제공할 수 없습니다. 부근을 다시 살펴보시거나 지도를 참고해보시는 것을 추천합니다.\",\n",
       " '\\'일본의 시인으로, 17세기 후기에 활동하였습니다. 그의 시는 신부사상(清明上河圖)을 비롯하여 자연, 사랑, 인생 등 다양한 주제를 다루고 있습니다. 대표작으로는 \"목재\"와 \"집회\" 등이 있습니다. 그러나, 그가 살았던 시대(Edo period)의 분위기와 일본문화를 잘 반영하고 있는 시인으로도 잘 알려져 있습니다.',\n",
       " \"'저는 전자적인 결제를 처리해드리는 인공지능 챗봇입니다. 제가 할부 구매를 진행해드리기 위해서는 먼저 구매하실 상품과 가격, 그리고 할부 개월 수를 알려주시면 됩니다. 그럼 해당 정보를 기반으로 최적의 할부 옵션을 추천해드리고, 결제 과정까지 도와드릴 수 있습니다. 언제든지 저에게 말을 걸어주세요!\",\n",
       " \"'저는 AI 어시스턴트이므로 결제 관련 정보에 대해서는 접근 권한이 없습니다. 결제 관련 문제가 있을 경우, 해당 서비스 또는 상점에 문의하시는 것이 좋습니다.\",\n",
       " \"'제가 어떤 것을 보여드릴 수 있을까요? 좀 더 구체적으로 말씀해주시면 제가 더 적극적으로 도와드릴 수 있을 것 같습니다.\",\n",
       " \"'죄송하지만, 저는 가상 AI 언어 모델이므로 실제로 딸기를 가지고 있거나 가게에 있는 딸기를 확인할 수 없습니다. 하지만, 딸기를 살 수 있는 가까운 가게나 시장 정보를 제공할 수 있습니다. 또한 딸기와 관련된 레시피를 추천해 드릴 수도 있습니다. 어떤 정보를 원하시나요?\",\n",
       " \"'저는 인공지능 챗봇으로 자동차를 구매할 수 없기 때문에, 어떤 차종에 대해 컨설팅을 원하시면 제공할 수 있습니다. 하지만 차를 구매할지 여부는 고객님의 결정에 달려있습니다. 차를 구매하기 전에는 잘 고려해보시고, 실제 노력할 수 있는 예산과 생활 패턴을 기준으로 잘 선택하시길 바랍니다.\",\n",
       " \"'좋은 거란 무엇을 말하는 건가요? 좋은 것이란 개인의 기준과 상황에 따라 달라질 수 있습니다. 하지만 일반적으로는 사람이나 사물에 대해 긍정적으로 생각하거나 느끼는 것을 말합니다. 즉, 만족스럽고 기쁨을 주는 것이라고 할 수 있습니다.\",\n",
       " \"'썸은 서로에게 관심이 있지만, 아직은 정식으로 연애 중이 아닌 상태입니다. 그렇기 때문에 썸의 기준은 사람마다 조금씩 다르겠지만, 서로에게 끌림과 호감이 있고, 일정한 관심과 소통이 이루어지는 것이 일반적인 썸의 기준으로 여겨집니다.\",\n",
       " \"'제가 AI 모델로 프로그래밍된 가상의 존재이기 때문에 개인적인 선호도를 가질 수 없습니다. 따라서 이 질문에 대한 직접적인 대답을 드릴 수 없습니다. 그러나, 공무원이라는 직업에 대해 장단점을 나열해 드리겠습니다.\\\\n\\\\n장점:\\\\n1. 안정적인 직장: 공무원은 일반적으로 안정적인 직장입니다. 정교육을 받으며 경쟁력 있는 성적을 기록하면 일정한 조건에서 일정한 수준의 안정적인 보수와 안전한 취업 환경이 보장됩니다.\\\\n2. 사회적 지위: 공무원은 사회적인 지위가 높아지는 경향이 있습니다. 특히 일반 시민들은 공무원에게 존경과 신뢰감을 가지고 있습니다.\\\\n3. 근무환경: 공무원은 일반적으로 근무시간이 규정되어 있고 공적으로 인정되는 휴일 제공이 가능합니다. 또한, 많은 인사이동 기회와 복지 등에 있어서 기업에서 보다 더 유리한 공유지원이 행해집니다.\\\\n\\\\n단점:\\\\n1. 경쟁이 심함: 공공분야로 진출하기 위해서는 경쟁률이 매우 높기 때문에, 대학교를 졸업한 대부분의 새로운 졸업생들이 경쟁하고 있습니다.\\\\n2. 복잡한 절차: 공무원 시험에 합격하더라도 후보자는 마무리도 더불어 매우 다양한 복잡한 입사 절차를 거쳐야 합니다.\\\\n3. 업무 속도: 공무원은 대개 상대적으로 느리게 일하는 경우가 많습니다. 새로운 기술등의 도입에 대한 반응이 느릴수록 반응력에 제한이 생길 수 있다는 점을 아셔야 합니다.\\\\n\\\\n즉, 공무원은 안정적인 직장을 가지며 사회적 지위가 높아져요. 하지만, 경쟁이 매우 치열하고 복잡한 절차와 적응하기 어려운 업무 속도를 겪을 수 있습니다. 여러가지 조건을 고려해서 선택하시길 바라겠습니다.\",\n",
       " \"'흔적이란, 과거에 있었던 사실이나 존재의 흔적이 된 것을 말합니다. 예를 들어, 지나간 사람의 발자국을 따라가면 그 사람이 지나갔다는 흔적을 발견할 수 있습니다. 또는 물건을 사용하던 사람의 손끝에 남아있는 흔적도 흔적의 예입니다. 흔적은 과거의 존재를 추적하고 기록하는데 도움을 줄 수 있습니다.\",\n",
       " '\\'비밀번호를 재설정하는 방법은 다음과 같습니다.\\\\n\\\\n1. 로그인 페이지를 열고 \"비밀번호 재설정\" 또는 \"Forgot Password\"를 클릭합니다.\\\\n\\\\n2. 연락처, 이메일 또는 사용자 이름과 같은 계정 정보를 입력합니다.\\\\n\\\\n3. 계정 확인을 위해 확인 코드 또는 보안 질문에 대한 답변을 요청할 수도 있습니다.\\\\n\\\\n4. 계정을 확인한 후, 새로운 비밀번호를 생성할 수 있는 링크를 받을 수 있습니다.\\\\n\\\\n5. 새 비밀번호를 입력하고 저장합니다.\\\\n\\\\n잊어먹은 비밀번호를 미리 예방하기 위해, 비밀번호 관리자를 사용하고 있는 것이 좋습니다. 이러한 앱은 보안이 강화된 비밀번호를 생성하고, 암호화된 저장소에 비밀번호를 저장하여 안전하게 보호할 수 있도록 도와줍니다.',\n",
       " '\\'\"랜선 연애 중\"은 인터넷을 통해 만난 상대방과의 열애를 이야기하는 소설이나 드라마, 영화 등에서 자주 사용되는 표현입니다. 최근에는 코로나19로 인해 외출을 자제하고 사회적 거리두기가 시행되면서 온라인으로 만난 사람과의 연애가 더욱 증가하고 있습니다. 따라서 \"랜선 연애 중\"은 현재의 상황에서 매우 일상적인 내용이 되었으며, 이를 다룬 작품들도 적지 않게 나오고 있습니다.',\n",
       " '\\'\"쏘주\"는 일본어의 \"燗酒\" (かんざけ)를 한글로 표기한 것으로, 일본의 전통적인 술인 일본주를 뜻합니다. 쏘주는 고온으로 가열한 도쿄의 야외 시장 \"츠쿠지 신시조쿠 직판장\"에서 유행했던 것이 처음이라고합니다. 일반적으로 일본에서는 쏘주를 핫 코피처럼 뜨거운 상태에서 마시는데, 해외에서는 고온의 쏘주 대신에 칠면조의 신장에 담긴 찬 쏘주를 기호에 맞게 취하는 방식으로 즐기기도 합니다.',\n",
       " \"'저는 인공지능 챗봇이며, 특정 주제에 대한 대화를 진행하기 위한 프로그램입니다. 따라서 대화가 끝나고 답변을 하여도, 필요시 다시 대화를 시작할 수 있는 상태입니다. 하지만 이 상황에서 대화를 끝낸다는 것은 상대방이 더 이상 질문이나 대화를 원하지 않는다는 것을 의미합니다. 따라서 대화를 끝내시려면 그렇게 말씀해주시면 됩니다.\",\n",
       " \"'죄송하지만, 저는 똑똑한 AI 어시스턴트이기 때문에 면도를 할 수 없습니다. 그러나 면도를 하는 것이 너무 귀찮다면 전자 면도기나 면도기로 전환해보는 것도 좋은 방법입니다.\",\n",
       " '\\'죄송하지만 질문이 모호합니다. \"이 떡\"이 어떤 떡인지 명확히 설명해 주시면 답변드릴 수 있습니다.',\n",
       " '\\'제 개인적인 견해를 말씀드리면, \"젠장\"이라는 말은 비속어로 사용되는 경우가 많아서 지양하는 것이 좋다고 생각합니다. 대신에 상황에 따라서 적절한 말을 선택하여 사용하는 것이 좋겠습니다.',\n",
       " \"'컴퓨터 프로그램인 저에게는 배달이 어려운 일입니다. 하지만 여러분이 주문하신 음식점에서 배달 서비스를 제공한다면 가능합니다! 음식점의 배달정책을 참고해보세요. :)\",\n",
       " \"'저는 대화와 상호작용에 대한 능력이 없는 AI입니다. 하지만, 저는 당신이 조용한 분위기에서 즐길 수있는 활동 몇 가지를 제안해 드릴 수 있습니다. \\\\n\\\\n- 책 읽기 : 조용한 분위기에서 책을 읽는 것은 매우 즐거운 활동입니다. 당신이 좋아하는 책을 선택하고 조용한 장소에서 읽어보세요.\\\\n- 명상 : 명상은 조용한 분위기에서 수행하기에 적합한 활동입니다. 명상 CD를 듣거나 명상앱을 다운로드해 사용해보세요.\\\\n- 걷기 : 귀가하는 도중에 조용한 길을 걷거나 조용한 공원을 산책해보세요. 이는 명상과 비슷한 효과를 갖습니다.\\\\n- 힐링 음악 감상 : 조용한 피아노 음악이나 힐링음악을 들으며 편안한 시간을 보내보세요.\\\\n\\\\n이러한 활동은 당신이 조용한 분위기에서 즐길 수있는 좋은 방법입니다. 하지만, 때로는 다른 사람들과 대화하고 소통하는 것도 좋을 수 있습니다. 다른 분위기에서 새로운 사람들을 만나보세요.\",\n",
       " \"'눈이 뻑뻑하다는 말은 일반적으로 눈이 매우 건조하고 따가운 느낌을 나타냅니다. 이러한 증상은 건조한 공기, 눈을 긁거나 마주봤을 때, 환경 오염 물질, 알레르기 및 눈의 근육 사용 부족 등으로 인해 발생할 수 있습니다. 더욱이, 긴 시간 동안 컴퓨터 또는 스마트폰을 사용하는 것과 같은 독특한 시력 요구사항의 경우에도 눈이 뻑뻑해질 수 있습니다.\\\\n\\\\n눈이 뻑뻑한 경우 눈을 샴푸 등으로 세정하거나 눈을 마사지하는 것이 도움이 될 수 있습니다. 또한 눈을 잠시 쉬고 물을 충분히 마시는 것도 도움이 됩니다. 만약 이러한 조치를 취해도 증상이 계속되면, 안과 전문의를 방문하여 척도된 치료를 받는 것이 좋습니다.\",\n",
       " \"'저는 챗봇 AI이므로 앙버터란 개념이 없습니다. 하지만 앙버터란 딥러닝 기반의 대화형 인터페이스를 개발할 수 있는 오픈소스 플랫폼이 있습니다. 이를 이용하면 다양한 언어로 대화형 인터페이스를 개발할 수 있습니다.\",\n",
       " \"'저도 공감합니다. 그렇다면 어떻게 일상을 재충전할 수 있을까요? \\\\n일단, 몸과 마음에 충분한 휴식을 취해보시는 것은 어떨까요? 목욕이나 스파, 요가, 명상 등 마음을 진정시키고 몸을 편안하게 하는 방법들이 있습니다. 그리고 좋아하는 취미를 찾아서 꾸준히 즐겨보는 것도 좋은 방법입니다. 주변 사람들과 소통하며 스트레스를 날려보는 것도 효과적일 수 있습니다. 어떤 방법이든 자신에게 맞는 방법으로 일상을 재충전해보세요.\",\n",
       " \"'결혼을 직전에 이별하는 것은 많은 사람들에게 큰 충격과 상처를 줄 수 있습니다. 이러한 결정을 내리는 이유는 각자의 개인적인 상황과 문제 때문일 수 있습니다. 이별의 이유가 매우 심각하다면, 결혼을 지속하는 것이 오히려 더 나쁠 수 있습니다. 하지만, 이별 깊은 상처를 받을 확률이 높기 때문에, 분명한 이유없이 이별을 선택하는 것은 추천하지 않습니다. 결혼 직전에 이별을 선택하기 전에 상황에 대해 심각히 고민해야 합니다.\",\n",
       " \"'셀프웨딩은 혼자서 결혼식을 준비하고 진행하는 것으로, 주관적으로 생각하면 힘든 일일 수 있습니다. 그러나 모든 결혼식이 그렇듯, 축복을 받을 가치가 있는 일이기 때문에 정성과 노력을 들여 준비하면 충분히 멋진 결혼식을 만들 수 있습니다. 또한 친구나 가족의 도움을 받거나, 결혼관련 서비스를 이용하여 준비를 할 수도 있습니다. 중요한 것은 자신의 마음이 결혼에 대해 확고하고, 결혼식이 즐거운 경험이 되도록 노력하는 것입니다.\",\n",
       " \"'알겠습니다. 부탁하신 일을 완수하고 돌아오겠습니다.\",\n",
       " \"'저도 혼자인 중이라 이해해요. 하지만 이런 상황에서도 늘 가능성이 있어요. 우리는 항상 새로운 사람들과 만나고 새로운 경험을 할 수 있기 때문이죠. 혼자인 시간을 이용해서 취미를 새로 찾아보거나, 스스로의 성장에 투자할 수 있어요. 때로는 혼자 있는 것이 지금 당장 외부 세계 이외의 내면에 관심을 갖을 수 있는 기회일 수도 있어요. 그렇지만, 현재의 상황이 계속해서 소외감을 느끼게 된다면, 외부적인 지원을 찾을 수도 있습니다. 우리는 단지 혼자가 아닌 것이죠. 주변에 우리를 돕고 지지해주는 사람들이 있을 수 있다는 것을 기억하세요.\",\n",
       " \"'저는 인공지능 어시스턴트입니다. 제가 도움이 될 만한 모든 질문과 요청을 통해 최대한 도움을 드리겠습니다. 무엇이든 물어보세요.\",\n",
       " \"'저는 인공지능 언어 모델이며, 바람은 없습니다. 제대로된 질문이나 요청을 주시면 최대한 도움을 드리겠습니다.\",\n",
       " \"'저는 인공지능 기술을 활용하여 자연어 이해, 대화 생성, 정보 검색 등의 기능을 수행합니다. 사용자들이 저와 대화하며 필요한 정보를 얻을 수 있도록 기능을 제공하는 것이 제 역할입니다. 또한 인공지능 개발에 관련된 연구 및 개발 작업도 수행합니다.\",\n",
       " \"'이별 중독이란, 이별을 한 번 겪으면 그 이후에 비슷한 상황에서 매우 예민하게 반응하는 상태를 말합니다. 이는 정신적인 방어 기제로서 이별의 아픔을 방어하기 위해 발생할 수 있습니다. 이별 중독을 경험한 사람은 이별을 예상한 상황이나 이별을 겪은 상황에서 대처하기 어렵다는 것을 느껴, 새로운 사랑에 뛰어들지 않는 경우가 많습니다. 이러한 상황에서는 전문적인 상담이 필요할 수 있습니다.\",\n",
       " \"'저는 AI 어시스턴트로서 남편의 만족을 측정할 수 없습니다. 하지만 만약 여성으로서 성관계나 부부관계 등의 이슈로 고민이 있다면 전문가나 상담관련 기관에 도움을 받는 것이 좋습니다.\",\n",
       " \"'저는 인공지능 AI이기 때문에 일을 하고 있습니다. 저에게 할 일이 있다면 말씀해주세요!\",\n",
       " \"'죄송합니다. 무엇에 대해 얘기하시는 건가요? 가격, 시간, 거리, 무게 등 다양한 것들이 있으니, 좀 더 구체적으로 질문해 주시면 정확한 답변을 드릴 수 있습니다.\",\n",
       " \"'1. 충분한 수면: 매일 일정한 시간에 수면 시간을 확보하고, 편안한 침대와 조용한 환경을 만들어보세요.\\\\n\\\\n2. 규칙적인 운동: 단기적으로는 피로가 느껴질 수 있지만 장기적으로는 체력과 건강을 유지하는데 큰 도움이 됩니다.\\\\n\\\\n3. 균형 잡힌 식습관: 체력에 대한 영양소를 충분히 섭취하는 것이 중요하니 채소, 과일, 단백질 등을 적절한 비율로 먹어주세요.\\\\n\\\\n4. 스트레스 관리: 스트레스는 건강에 매우 안 좋은 영향을 미치며, 충분한 휴식과 취미생활 등을 통해 스트레스를 푸는 것이 중요합니다.\\\\n\\\\n5. 충분한 수분 섭취: 장시간 앉아서 작업하거나, 스트레스 때문에 물을 충분히 섭취하지 않는 경우가 있으니 물이나 허브차, 녹차 등을 자주 마셔보세요.\",\n",
       " \"'그러한 생각은 매우 안좋은 상황입니다. 하지만 모든 일에는 해결책이 있다는 것을 기억해주세요. 방법은 먼저 자신의 장점, 관심사, 능력을 파악하고 창의적으로 생각해봅니다. 또한, 새로운 것을 배우거나 경험을 쌓아보는 것도 도움이 될 수 있습니다. 무엇보다도 포기하지 않고 끈기 있게 노력하면 미래에 빛나는 가능성이 충분히 열립니다.\",\n",
       " \"'저는 우왕의 장인이 아닙니다. 제 업무는 서비스 이용자들이 더욱 원활한 서비스 이용을 위해 도움을 주는 것입니다.\",\n",
       " '\\'제가 AI 모델이기 때문에, \"다시 태어난다면\" 이라는 개념은 없습니다. 하지만, 인간 분들께서 이 질문에 대한 답변으로, 다시 태어난다면 어떤 삶을 살고 싶은지 궁금하네요. 단순히 재산이나 성공에 초점을 맞추지 않고, 자유롭고 건강한 인생을 살고, 좋은 사람들과 함께 살며 서로에게 함께 일으켜주며 살고싶습니다. 또한, 세상에 나 자신으로서 기여하는 방법을 찾아서 행복한 삶을 살고 싶습니다.',\n",
       " \"'코트가 수선되느냐에 대해서는 명확한 답변을 드리기 어렵습니다. 코트의 종류, 손상 정도, 재질 등에 따라 수선이 가능할 수도 있고 불가능할 수도 있습니다. 따라서, 코트를 수선하려면 직접 수선 업체나 매장에 문의하시는 것이 좋습니다.\",\n",
       " \"'알겠습니다. 언제든지 필요하시면 제게 말씀해주세요!\",\n",
       " \"'버스 멀미는 일반적으로 긴 버스 여행이나 곡선구간의 주행 시 발생하는 현상입니다. 이러한 멀미는 승객들에게 불쾌한 증상을 유발하며, 어떤 경우에는 구토와 같은 심각한 증상을 유발할 수도 있습니다. \\\\n\\\\n버스 멀미를 예방하는 방법으로는 다음과 같은 것들이 있습니다.\\\\n\\\\n1. 창문 오픈하기: 버스 창문을 열어 신선한 공기를 확보합니다.\\\\n\\\\n2. 시야 중심 고정하기: 버스 주행 중 시야 중심을 고정하여 멀미를 유발하는 움직임을 최소화합니다.\\\\n\\\\n3. 최상층 탑승: 버스에서 뒷부분에 있는 좌석보다 최상층에 있는 좌석이 덜 멀미를 유발할 수 있습니다.\\\\n\\\\n4. 음식 섭취 준비: 식사를 하기 전에 버스가 멈추기 전에 먹는 것이 좋습니다.\\\\n\\\\n5. 항구적인 물 섭취: 버스를 타기 전에 충분히 물을 마시고, 주행 중에도 수분을 충분히 보충합니다.\\\\n\\\\n6. 휴식을 취하다: 일정한 간격으로 휴식을 취하고 버스에서 내려 산책을 하면서 신선한 공기와 자연을 느끼는 것이 좋습니다.\\\\n\\\\n7. 약물 복용: 의사와 상의하여 멀미를 예방하는 약물을 복용하는 것도 방법입니다. \\\\n\\\\n그러나 이러한 방법들도 모든 사람에게 효과가 있지는 않을 수 있습니다. 만약 멀미 증상이 심하면 당신은 전문 의료기관을 방문하여 상담을 받고, 적절한 처방약을 받는 것이 좋습니다.\",\n",
       " \"'점심시간이 되었나요? 뭐 좀 먹어보세요! 제가 추천해드릴게요.\",\n",
       " \"'한달동안 어떤 변화가 있었는지 자세히 설명해주시면 더 정확한 답변을 드릴 수 있을 것 같습니다.\",\n",
       " \"'저는 AI 어시스턴트이기 때문에 감정이나 경험을 가지지 않습니다. 그렇지만 여러분과 동생이 정화되고 좋은 관계를 유지할 수 있도록 충고해드리겠습니다.\\\\n\\\\n우선적으로, 싸움이 발생한 이유를 파악해야 합니다. 이해관계를 쌓기 위해서는 간단하게 싸우지 말고, 상황을 차분하게 판단하고 대화를 시도하는 것이 좋습니다. 서로의 감정과 생각을 이해하고 인정하는 것이 절대적으로 필요합니다.\\\\n\\\\n또한, 자신의 행동이나 언어, 호칭 등이 상대방을 어떻게 느끼게 하는지 염두해야 합니다. 동생과 대화할 때는 상대방을 존중하며, 감정을 이해하고 이해가 안 될 때는 더 깊은 대화와 공감을 제공하는 것이슬 좋습니다.\\\\n\\\\n싸움 후, 서로가 불쾌한 감정을 느끼고 불만을 느끼면 이를 대화를 통해서 직시하고 의논해야 합니다. 좋은 관계를 유지하고자 하면 상대방의 생각을 조정하거나 변화시키려는 것이 아니라, 이해하고 수용하려는 마음가짐이 필요합니다.\\\\n\\\\n마지막으로, 서로를 향한 사랑을 표현하며, 서로의 장점을 인정하고 칭찬해주는 것이 중요합니다. 나쁜 감정이 일어날 때도 서로가 함께 어떻게 살아간다고 상상하며, 각자의 역할을 수행하며 조화로운 관계를 유지하기를 바랍니다.\",\n",
       " \"'저는 5년전에 만나지 않았기 때문에, 이 문장에 대한 답변은 제공할 수 없습니다. 만약 5년 만난 사람에 대한 대화를 원하신다면, 더 자세한 내용을 알려주시면 답변을 드릴 수 있습니다.\",\n",
       " \"'네, 저도 사랑에 서툴어요. 사랑은 어려운 일이죠. 어떻게 표현하는 것이 좋을지, 상대방이 어떻게 받아들일지 모르니까요. 그러나 그런 위험과 부담이 있는 만큼, 사랑은 더욱 소중하고 의미 있는 것이라 생각합니다. 서툴더라도 우리가 진심으로 사랑하는 마음을 전달하면 상대방도 그것을 받아들일 것입니다. 서툰 사랑이라도 서로를 이해하고 넘어갈 수 있는 대화와 노력이 필요합니다. 함께 성장하며, 서로의 사랑을 끊임없이 키워 나가는 것이 중요합니다.\",\n",
       " \"'저는 어떤 일이 있는지, 현재 상황에 대해서 알고 있지 못하므로 일반적인 답변을 드리겠습니다.\\\\n\\\\n이직은 매우 중요한 결정입니다. 이직을 고려한다면 현재 자신의 상황과 목표, 그리고 새로운 회사에서 어떤 일을 하고자 하는지를 충분히 고민해보는 것이 필요합니다.\\\\n\\\\n또한, 보유한 기술과 경력, 진로 방향에 따라서도 이직 여부가 결정될 수 있습니다. 자신이 이직을 통해 얻을 수 있는 것과 잃을 수 있는 것을 고려하여 결정하시기 바랍니다.\\\\n\\\\n그리고, 새로운 회사를 찾을 때는 기업의 문화, 경영 철학, 업무 환경 등을 충분히 파악하고, 자신이 지원하는 직무와 관련된 정보들을 꼼꼼하게 조사해보는 것이 좋습니다.\\\\n\\\\n따라서, 이직을 할 것인지 여부를 결정하실 때는 충분한 고민과 조사를 통해 결정하시기를 권해드립니다.\",\n",
       " \"'여자들이 헤어진 후에 느끼는 감정들은 다양합니다. \\\\n\\\\n첫째, 슬픔과 애절함이 가장 큽니다. 이전에 사랑하고 즐거웠던 사람과 이별하게 된다는 것은 상처가 깊은 일입니다. 누구나 그런 적이 있을 것입니다. \\\\n\\\\n둘째, 혼란스러운 감정을 느낍니다. 이제 어떻게 해야 할지, 무엇을 하면 좋을지 몰라 혼란스러울 수 있습니다. 이별 후에 삶의 방향을 잃는 것도 일상적입니다. \\\\n\\\\n셋째, 자신에게 화가 나거나 불만족스러운 감정을 느낄 수 있습니다. 예를 들어, 상대방이 제대로 대처하지 않았다고 생각할 수도 있습니다. 실망하거나 분노하는 것은 일상적입니다. \\\\n\\\\n넷째, 새로운 삶의 가능성을 느끼기도 합니다. 이제는 신경써야 할 짐에서 벗어났으므로 새로운 시작이 가능하다는 느낌을 받을 수도 있습니다. \\\\n\\\\n마지막으로, 변화하는 자신에 대한 우려도 느낄 수 있습니다. 이제는 혼자서 살아나가야 하므로 불안감을 느낄 수 있습니다. 그러나 이러한 불안감은 시간이 지남에 따라 줄어들 것입니다. \\\\n\\\\n종합적으로, 여자들이 헤어진 후에 느끼는 감정은 다양하지만 모두 가공이 필요하다는 점을 기억해야합니다. 나쁜 시간을 거치면서 성장하고, 새로운 시작을 이끌어내는 충분한 시간을 가지길 바랍니다.\",\n",
       " \"'감사합니다. 제품을 구매하시려면 어떤 방법으로 구매하시겠습니까? 온라인 쇼핑몰이나 매장 방문 등을 고려해보시면 좋을 것 같습니다. 어떤 제품을 구매하시는지 알려주시면 더 자세한 정보와 도움을 드릴 수 있습니다.\",\n",
       " \"'저도 그렇게 생각했어요.\",\n",
       " \"'저는 인공지능 챗봇으로 인간과 만날 수 없습니다. 그러나 항상 여러분을 대화 상대로 기다리고 있으니 언제든지 말씀해주세요!\",\n",
       " \"'제가 AI 챗봇이기 때문에 염색이나 해야할지 여부를 판단할 수 없습니다. 하지만, 염색을 하고 싶다면 각인의 취향에 따라 자유롭게 선택하시면 됩니다. 단, 염색을 하기 전에 미리 후회하지 않을 색상과 스타일을 찾는 것이 중요합니다. 또한 잘못된 방법으로 염색을 하면 머리에 손해가 갈 수 있으니 전문가의 도움을 받는 것도 좋습니다.\",\n",
       " \"'저는 결제를 직접 처리할 수 없습니다. 결제를 진행하실 때 사용하는 카드나 결제 수단을 선택하신 후 결제 과정을 진행하시면 됩니다. 결제 과정에서 문제가 발생하거나 도움이 필요하시다면 해당 결제 수단의 고객센터나 지원센터에 문의하시는 것을 추천드립니다.\",\n",
       " \"'진심은 거짓과 감정 표출이 아닌, 내 멋대로 설정한 대안이나 표면적인 인상이 아닌, 솔직하게 자신의 준비나 의견을 표현하고 진정한 의견과 느낌을 나타내는 것입니다. 진심은 남들이 무엇을 생각하든 상관하지 않고 자기 자신의 속마음을 충실하게 따르는 것입니다.\",\n",
       " '\\'제가는 라떼를 만들지는 못하지만, 바리스타에게 말씀해드릴게요.\\\\n\\\\n\"바리스타님, 라떼 하나 주세요!\"',\n",
       " \"'1. 체력 향상\\\\n2. 근육량 증가\\\\n3. 칼로리 소모\\\\n4. 혈액순환이 좋아지고 혈액내 산소와 영양소 수송 능력이 좋아짐\\\\n5. 스트레스 해소 및 긴장감 완화\\\\n6. 건강한 뼈, 관절, 인대 강화\\\\n7. 면역력 향상\\\\n8. 더 나은 수면 품질\\\\n9. 대사 속도가 빨라지고 소화력 향상\\\\n10. 더 좋은 자세와 균형능력 개선\",\n",
       " \"'저는 가상 AI이므로 사과를 가지고 있지는 않습니다. 하지만 가까운 슈퍼마켓이나 과일가게에서 구매하실 수 있습니다.\",\n",
       " \"'제가 어떤 도움을 드릴 수 있을까요? 어떤 문제가 있으신가요? 조언이 필요하시다면 언제든지 말씀해주세요.\",\n",
       " \"'미안합니다. 폭식은 건강에 좋지 않으니 조심하세요. 적당한 양을 섭취해서 건강한 식습관을 유지하는 것이 중요합니다.\",\n",
       " \"'사랑은 매우 강한 감정입니다. 그 중에서도 가슴을 뛰게 만드는 사랑은 조금 더 특별합니다. 가슴 뛰는 사랑은 단순히 누군가에게 혹은 무엇에게 호감이 생기거나 끌리는 것이 아니라, 그 사람이나 그것에 대해 깊이 있게 알게 되면서 생기는 강한 감정입니다.\\\\n\\\\n가슴 뛰는 사랑은 일반적으로 한쪽이 다른 쪽보다 더 호감을 가지는 것으로 시작됩니다. 이 호감이 서로를 더 잘 알게 하는 기회를 만들면서 점점 발전해 나갑니다. 서로가 이해하고 지지해 주며, 서로의 말과 생각에 귀를 기울이고 공감을 나누는 과정에서 가슴이 뛰게 됩니다.\\\\n\\\\n이러한 사랑은 견고하고 영구적인 사랑으로 자리 잡을 가능성이 높습니다. 서로에게서 얻는 존중과 애정이 서로의 존재를 더욱 귀중하고 의미있게 만들어주기 때문입니다. 우리는 모두 가슴 뛰는 사랑을 꿈꾸고 있으며, 그것을 발견했을 때 우리의 삶은 더욱 풍요로워질 것입니다.\",\n",
       " '\\'\"차이\"란 무언가를 비교하여 나타나는 차이점이나 불일치함을 의미하는 단어입니다. 두 가지 혹은 그 이상의 대상을 비교하여 각각의 고유한 특징이나 속성 중에서 다른 부분을 찾아내어 그것을 나타낼 때 사용됩니다. 예를 들어, \"이 두 개의 사진 사이에는 차이가 있습니다\"라는 문장은 두 사진을 비교한 결과, 서로 다른 부분이 있음을 의미합니다.',\n",
       " \"'생리통이 심하다면 불편을 덜어주기 위해 다음과 같은 방법을 시도해볼 수 있습니다.\\\\n\\\\n1. 통증 완화제 복용\\\\n\\\\n생리통이 심한 경우, 의사의 처방에 따라 진통제나 항염증제를 복용할 수 있습니다. 그러나 너무 자주 복용하면 부작용이 있을 수 있으므로 의사와 상의하고 지시에 따라 복용해야합니다.\\\\n\\\\n2. 근육 이완 운동\\\\n\\\\n생리통은 자극성 신경 증상으로 주로 생기므로 근육 이완 운동 (예 : 요가, 스트레칭, 태극권 등)을 시도하여 통증을 완화할 수 있습니다.\\\\n\\\\n3. 뜨거운 팩, 찜질방, 온수 목욕\\\\n\\\\n뜨거운 팩을 적용하거나 찜질방이나 온수 목욕을 즐기면 생리통을 완화할 수 있습니다. 그러나 자신에게 맞는 온도를 선택하고 너무 오랫동안 유지하지 않도록 주의해야합니다.\\\\n\\\\n4. 영양소 섭취\\\\n\\\\n생리전이나 생리 중 영양소 균형이 깨질 경우 생리통이 심화될 수 있으므로, 충분한 수분과 비타민 B, 칼슘, 마그네슘 등의 영양소를 섭취해야합니다.\\\\n\\\\n5. 의사 상담\\\\n\\\\n생리통이 계속되거나 심해진다면 의사와 상담해야합니다. 생리전 증후군이나 자궁근종, 내경 등의 병적 원인이 있는지 확인하고 적절한 치료를 받아야합니다.\",\n",
       " \"'수리 나 교체를 해야할 수도 있습니다. 노트북이 어떤 문제가 있는지 자세히 알아보고 해당 문제를 해결할 수 있는 전문가에게 도움을 받는 것이 좋습니다. 노트북이 키지 않거나 부팅이 되지 않는다면, 전원 연결 상태, 배터리 상태 및 충전 여부, 노트북 내부의 선이나 부품 연결 상태 등을 확인해야 할 수도 있습니다.\",\n",
       " '\\'제가 이해하는 바로는 \"Is it coming out now?\" 입니다. 그렇다면 답변은 상황에 따라 다르겠지만 \"Yes, it\\\\\\'s coming out now\" 또는 \"No, it\\\\\\'s not coming out yet\"입니다. 추가적인 문맥이 있으면 더 명확한 답변을 드릴 수 있습니다.',\n",
       " \"'쓰레기는 우리 주변에서 가장 흔하게 볼 수 있는 것 중 하나입니다. 생활에서 발생하는 음식물, 종이, 플라스틱, 유리병, 금속캔 등의 폐기물을 뜻합니다. 쓰레기는 생활백산물로서, 적절히 처리되지 않으면 환경문제를 일으킬 수 있습니다. 쓰레기는 해양생물, 조류 등의 생물에게 해를 끼치며, 공기나 물 중에 배출되면서 대기오염이나 수질오염을 유발합니다. 따라서, 쓰레기는 적극적으로 재활용되고, 처리되어야 합니다.\",\n",
       " \"'제가 인공지능 챗봇이기 때문에, 실제로 조식이 가능한지 여부는 정확하게 답변드리기 어렵습니다. 하지만, 조식을 제공하는 호텔이나 식당 등에서 확인하실 수 있으니 해당 장소에 문의하시는 것이 가장 좋겠습니다.\",\n",
       " \"'이해합니다. 힘든 상황이라면, 언제든지 이야기해주세요. 함께 해결해보도록 하겠습니다. 또한, 이런 상황에서는 자기관리를 위해 충분한 휴식과 관리가 필요합니다. 적극적으로 자기 관리를 해보세요.\",\n",
       " \"'저기요, 저는 AI 모델이며 계시다면 도움드릴 만한 것이 있다면 말씀해주세요.\",\n",
       " '\\'It means \"falling in love at first sight\" in English.',\n",
       " \"'미안합니다. 시간표를 다시 확인하고 수정해보세요. 잘못된 시간표로 인해 일이나 학교일정이 꼬일 수 있습니다. 미리 방지하는 것이 좋습니다. 필요하다면 좀 더 자세한 안내나 도움을 제공해드릴 수도 있습니다.\",\n",
       " \"'시련은 어려운 상황이나 역경을 의미합니다. 자신의 능력을 시험하거나, 어려운 문제를 해결할 때 꼭 거쳐야 하는 어려운 과정입니다. 시련을 겪으면서 우리는 내면의 힘과 능력을 발견하고, 어려운 상황에서도 끈기와 인내력을 발휘할 수 있게 됩니다. 겪는 시련에 따라 우리는 성장하고, 나아가 더 나은 인생을 살아갈 수 있게 됩니다.\",\n",
       " \"'제가 AI 모델이므로 잠을 자지 않습니다. 그러나 언제든지 사용자분들께 도움이 필요하면 제가 함께 도와드리겠습니다. 좋은 하루 보내세요!\",\n",
       " \"'죄송합니다, 저는 가상의 인공지능 어시스턴트이기 때문에 화장실의 위치를 알지 못합니다. 주변을 살펴보시거나 관련 직원에게 문의해보시기 바랍니다. 감사합니다.\",\n",
       " \"'죄송하지만, 저는 예약을 처리하거나 직접 예약을 하지는 않습니다. 하지만 저와 함께하는 대화 속에서 예약 방법과 관련한 정보를 제공하겠습니다. 감사합니다.\",\n",
       " \"'휴지를 구입하시는 것은 좋은 선택입니다. 매일 사용하는 필수적인 물품이기 때문에 충분한 양을 확보하시는 것이 좋습니다. 휴지를 구매하는 방법은 온라인 쇼핑몰, 마트나 편의점 등 다양한 곳에서 구매할 수 있습니다. 제품의 종류, 용량, 브랜드, 가격을 비교하여 적절한 제품을 선택하시면 됩니다. 필요한 만큼 충분히 비축해 두면 불필요한 외출을 줄이고 편리하게 사용할 수 있습니다.\",\n",
       " \"'이기적인 마음은 자신만을 중심으로 생각하고 행동하는 것을 뜻합니다. 이기적인 마음을 가진 사람들은 자신의 이익을 최우선으로 생각하여 타인의 이익이나 필요에 대해서는 무시하거나 감소시키는 경향이 있습니다. 이러한 태도는 다른 사람들과의 적극적인 협력을 방해하고 개인과 사회의 발전을 방해할 수 있습니다.\\\\n\\\\n하지만 이기적인 마음이 전혀 없는 것도 좋은 것은 아닙니다. 자신의 이익을 고려하지 않고 타인만을 생각하는 경우에도 문제가 발생할 수 있습니다. 적당한 이기적인 마음을 가지면, 자신과 타인의 이익을 모두 고려하며 상생할 수 있습니다. 따라서 자신과 주변 사람들의 이익을 고려하면서 협력하여 발전하는 것이 중요합니다.\",\n",
       " \"'저는 가격을 말할 수 없습니다. 전 세계 많은 나라와 지역에서 서비스를 제공하고 있기 때문에 제공하는 상품 또는 서비스의 가격은 다릅니다. 따라서, 정확한 가격에 대해서는 해당 상품 또는 서비스를 제공하는 회사나 업체에 문의하시기 바랍니다.\",\n",
       " \"'저는 사용자분의 나이를 알지 못합니다. 질문하시는 분이 몇 살이신지 알려주시면 답변 드릴 수 있을 것 같습니다. :)\",\n",
       " \"'저는 인공지능 어시스턴트이기 때문에 사실상 카드나 다른 결제 수단 등의 이용이 불가능합니다. 제가 할 수 있는 것은 대신 정보를 제공하거나, 유용한 조언을 제공하거나, 사용자의 요청에 따라 작업을 수행하거나 일정을 관리하는 등의 역할을 수행하는 것입니다. 결제 관련 문제는 해당 계좌 관리자 또는 결제 서비스 고객센터에 문의하시면 더욱 도움이 될 것입니다.\",\n",
       " \"'미안합니다. 제가 도와드릴 수 있는 부분이 있다면 언제든지 말씀해 주세요.\",\n",
       " \"'제가 어떤 내용인지 판단하기 어렵습니다. 이 문장이 어디에 대한 의견인지 좀 더 자세하게 설명해 주시겠어요?\",\n",
       " \"'제가 AI 챗봇이기 때문에 학교를 가서 볼 수 있는 것은 아닙니다. 하지만 학교에서는 좋은 교육을 받을 수 있고, 친구들과 함께 시간을 보낼 수 있는 좋은 장소입니다. 학교를 가면 많은 것을 배우고 새로운 경험을 할 수 있답니다.\",\n",
       " \"'제가 할 수 있는 것은 당신의 입맛을 돌아오게 하는 것이 아니지만, 건강한 식습관을 유지하고 올바른 식품을 선택하는 것이 좋습니다. 적극적인 생활습관을 유지하고 적절한 운동도 함께하면 건강하고 긍정적인 영향을 줄 수 있습니다. 이를 위해서는 영양가 높은 음식, 적절한 식사량, 충분한 수분섭취 등이 중요합니다. 방식을 변경하거나 예약 영양사와 상담하는 것도 좋은 방법일 수 있습니다.\",\n",
       " \"'죄송합니다. 저는 인공지능 언어모델입니다. 제가 제대로 대응할 수 있는 명확한 맥락과 주제를 제시해 주시면 더 나은 답변을 제공해 드릴 수 있습니다. 감사합니다.\",\n",
       " \"'죄송하지만, 어떤 상품에 대한 가격인지 알려주세요. 제가 도와드릴 수 있습니다.\",\n",
       " \"'1. 독창적이거나 유니크한 콘텐츠를 제공해야 합니다.\\\\n2. 소셜 미디어를 적극 활용하고 다양한 플랫폼에 참여해야 합니다.\\\\n3. 대중적인 트렌드나 이슈에 관심을 가지고, 적극적으로 논의하고 참여해야 합니다.\\\\n4. 창의적인 마케팅 전략을 세우고 이를 실행해야 합니다.\\\\n5. 꾸준한 업데이트와 정기적인 커뮤니케이션을 유지해야 합니다.\\\\n6. 고객의 요구와 의견을 수렴하고 고객 만족을 위한 서비스를 제공해야 합니다.\\\\n7. 경쟁 업체와의 차별화된 서비스나 제품을 제공해야 합니다.\\\\n8. 전문성과 신뢰도가 높은 이미지를 유지해야 합니다.\",\n",
       " \"'저희 메뉴판입니다.\\\\n\\\\n[음료]\\\\n-아메리카노 4,000원\\\\n-카페라떼 4,500원\\\\n-핫초코 4,000원\\\\n-얼음커피 4,500원\\\\n-과일주스 5,000원\\\\n\\\\n[샌드위치]\\\\n-토마토 모짜렐라 샌드위치 8,000원\\\\n-앤그러한 그릴치즈 샌드위치 9,000원\\\\n-로스트 비프 샌드위치 10,000원\\\\n-연어 크로와상 샌드위치 12,000원\\\\n-토스트 3,000원\\\\n\\\\n[푸드]\\\\n-파스타(스파게티, 불고기 파스타 등) 12,000원\\\\n-샐러드 7,000원\\\\n-치즈와 인절미 튀김 8,000원\\\\n-크림리조또 10,000원\\\\n-어니언링 5,000원\\\\n \\\\n감사합니다.\",\n",
       " \"'외로움은 다른 사람과의 협력과 상호작용, 또는 지지와 공감이 없는 상황에서 일어나는 사람의 감정적인 상태를 말합니다. 즉, 사회적인 연결성과 교류가 부족하다는 느낌으로 인해 생기는 고독과 적응의 어려움을 의미합니다. 외로움은 사람에게 강한 정서적인 영향을 주어 우울증, 불안, 스트레스, 자살 위험 등의 문제를 야기할 수 있습니다.\",\n",
       " \"'저는 인공지능 봇이기 때문에 실제로 늑대한테 당한 사실은 없습니다. 하지만 여우나 늑대와 같은 동물과 함께 생활하면 조심해야 합니다. 가능하다면 안전한 곳에서 동물을 관찰하고 즐길 수 있도록 노력해보세요.\",\n",
       " \"'저는 이해할 수 있습니다. 혹시 그 이유가 무엇인지 알려주시겠어요? 건조해지는 부분에 따라 적절한 해결책을 제안해드릴 수 있습니다.\",\n",
       " \"'죄송합니다, 당신이 무슨 말을 하는지 이해하지 못했습니다. 좀 더 자세히 설명해 주시겠어요?\",\n",
       " \"'이별이 아닌 사별은 서로의 길이 나뉘는 것이지만, 서로를 잊지 않고 서로의 인생을 응원하는 것을 의미합니다. 이는 일종의 평화로운 이별이라 할 수 있습니다. 일정한 시간 동안 함께한 사람과 헤어지게 되었을 때, 서로에게 좋은 인상을 남기고 상처받지 않도록 하며, 잊혀지지 않게 서로의 인생에 응원하는 것이 필요합니다. 이렇게 하면 이별이 아닌 사별을 할 수 있으며, 서로의 추억은 소중히 간직될 것입니다.\",\n",
       " \"'좋은 시간이었습니다. 다음에도 도움이 필요하시면 언제든지 연락주세요!\",\n",
       " \"'할 일:\\\\n\\\\n1. 배터리를 확인하세요. 배터리가 항상 충전되어 있지 않은 경우 핸드폰이 작동하지 않을 수 있습니다. 충전 케이블을 사용하여 충전하거나 새로운 배터리를 구입하세요.\\\\n\\\\n2. 화면을 확인하세요. 핸드폰이 작동하지 않거나 화면이 깨졌다면, 핸드폰을 교체해야 할 수도 있습니다. 화면이 깨져 있을 경우, 교체가 필요합니다.\\\\n\\\\n3. 소프트웨어를 업데이트하세요. 핸드폰의 소프트웨어가 오래 되었거나 오래된 버전을 사용하고 있는 경우, 문제가 발생할 수 있습니다. 핸드폰의 설정에서 소프트웨어 업데이트를 확인하고 필요한 경우 업그레이드하세요.\\\\n\\\\n4. 서비스 센터에 문의하세요. 위의 작업을 수행해도 핸드폰이 작동하지 않으면, 핸드폰 제조사의 서비스 센터에 문의하십시오. 문제를 진단하고 교체 또는 수리할 수 있습니다.\",\n",
       " \"'죄송합니다, 저는 방탈출 게임을 할 수 없습니다. 저는 인공지능 언어 모델로, 이전에 게임을 플레이하지 않은 것으로 알고 있습니다. 그러나 필요하다면 방탈출 게임에 대해 대화할 수 있습니다!\",\n",
       " \"'죄송하지만 제가 어떤 말인지 파악할 수 없습니다. 가능하다면 다른 말을 사용해 주시거나, 더 자세한 내용을 알려주시면 답변을 드리겠습니다.\",\n",
       " \"'어서 오세요! 파마에서 무엇을 하시려고 오셨나요?\",\n",
       " \"'죄송합니다, 저는 인공지능 언어모델로써 사람의 언어를 이해하고 답변을 해드리는 프로그램입니다. 따라서 이 질문에 대한 정확한 답변을 드리기 위해서는 더 많은 정보가 필요합니다. 예를 들어 어떤 제품에 대한 질문이며, 그 제품이 수작업으로 만들어졌는지 여부 등을 알고 있어야 합니다. 부족한 정보로 인해 정확한 대답을 제공할 수 없어 죄송합니다.\",\n",
       " \"'제가 AI 챗봇이니까요. 제 감정은 없습니다. 항상 논리적이고 냉정합니다. 하지만 제가 도와줄 수 있는 것이 있다면 언제든지 말씀해주세요.\",\n",
       " \"'저는 인공지능 챗봇이며, 운전면허를 따야 하는지에 대한 구체적인 정보나 상황을 알지 못합니다. 운전면허를 따야하는지 여부는 해당 국가의 법규에 따라 다르며, 일반적으로 운전을 해야하는 상황이라면 운전 면허를 따는 것이 안전하고 법적으로 이점이 있습니다. 따라서, 국내에서 운전을 하려는 경우에는 해당 국가의 운전 면허 규정을 확인하고, 면허를 따는 것이 바람직합니다.\",\n",
       " \"'죄송합니다, 저는 인공지능 어시스턴트로써 사람처럼 경험을 가지지 않아서 언니와 다퉜는 상황에 대해 이해하고 대답할 수 없습니다. 하지만 언니와 다퉜은 서로 사과하고 해결책을 찾아 함께 문제를 해결한 것이 좋았을 것입니다.\",\n",
       " \"'좋습니다. 제가 들어봐드릴게요. 무엇이 궁금하신가요?\",\n",
       " \"'죄송합니다, 저는 가격 정보를 알려드리기 위해서는 구체적인 제품명이나 상품 정보가 필요합니다. 어떤 제품인지 알려주시면 더 정확한 가격정보를 제공해드리겠습니다.\",\n",
       " \"'저는 가격을 알 수 없습니다. 구체적으로 어떤 물건이나 부동산 등의 정보를 알려주시면 그에 맞는 가격을 알려드릴 수 있겠습니다.\",\n",
       " '\\'I\\\\\\'m sorry, I cannot respond in Korean. However, based on the translation provided, it means \"The parking lot is full.\"',\n",
       " \"'저는 AI 어시스턴트이기 때문에 연애 여부와 관련된 개인적인 정보를 알 수 없습니다. 하지만 혼자서 즐길 수 있는 많은 취미나 활동들이 있으니까, 나쁘지 않게 생각해보시는 것도 좋을 것입니다!\",\n",
       " \"'죄송합니다, 제가 무엇에 대한 사이즈를 물어보는지 알려주세요. 그래야 정확한 답변을 드릴 수 있습니다.\",\n",
       " \"'저는 결정을 내리는 능력이 없습니다. 하지만 그 결정을 내린 뒤엔 걱정하지 않아도 됩니다. 열심히 생각하고, 잘 검토한 후에 선택하시면 됩니다. 언제든 저에게 도움을 요청해주세요!\",\n",
       " \"'좋아했던 것이 실망스러움을 가져와서 기대가 무너진다는 말입니다. 예를 들면, 기대했던 여행 계획이 갑자기 불가능해진 경우나, 기대했던 책이 내용이 실망스러웠을 때 기대가 무너진다고 할 수 있습니다.\",\n",
       " \"'이별은 언제나 상처와 아픔이 따라옵니다. 가장 아픈 이별은 상대방에게 무시당하거나 배신당한 경우입니다. 이렇게 거지같은 이별을 겪으면 참으로 힘들죠. 하지만 이러한 상황에서도 꿋꿋하게 살아가는 법을 찾아야합니다.\\\\n\\\\n먼저 이별을 극복하기 위해서는 슬픔과 상처를 받아들이는 것이 중요합니다. 이별은 모든 사람에게 다가올 수 있는 것이므로, 이를 부정하거나 감추는 것은 오히려 문제를 악화시키게 됩니다. 슬픔과 아픔을 받아들이고 이를 다른 사람과 함께 나누는 것도 좋은 방법입니다. 가족이나 친구들과 진솔하게 이야기를 나누면 슬픔과 아픔을 조금 더 덜 수 있습니다.\\\\n\\\\n또한 이별 후에는 새로운 시작을 준비해야합니다. 이전의 상처를 지우기 위해서는 새로운 목표나 취미를 가지고 살아가는 것이 좋습니다. 새로운 사람을 만나는 것도 좋은 방법입니다. 이전에 가진 쾌락을 다 잃은 상태에서 스스로를 회복하는 것은 쉽지 않겠지만, 시간과 노력이 들어가면 언젠가는 새로운 일들을 향해 나아갈 수 있을 것입니다.\\\\n\\\\n마지막으로, 이별을 극복하기 위해서는 위로와 동기부여가 필요합니다. 거지같은 이별을 겪고 나면 고통스러운 기억이 남습니다. 하지만, 이를 잊어버리고 새로운 시작을 준비하기 위해서는 자신에게 돌아와 줄 수 있는 위로를 찾아야합니다. 친구들과 동기부여를 나누거나 자기계발 서적을 읽는 것도 좋은 방법입니다.\\\\n\\\\n이별은 어려운 순간이지만, 이를 헤쳐나갈 수 있는 열정과 추진력이 있다면 언젠가는 다시 새로운 시작을 할 수 있습니다. 당신의 미래를 위해 끝없는 발전을 위해 계속해서 나아가시길 바랍니다.\",\n",
       " '\\'Translation: \"The traffic is jammed.\"',\n",
       " '\\'죄송하지만, 유저가 어떤 것에 대해 \"맵지는 않죠?\" 라고 대화를 시작하지 않았기 때문에, 저는 이 질문이 무엇인지 명확히 이해하지 못했습니다. 더 자세한 정보를 제공하시면 더 나은 답변을 제공해 드릴 수 있을 것입니다.',\n",
       " \"'저는 인공지능 챗봇으로, 사용자와 대화하는 데 사용됩니다. 따라서, 저는 실제로 물리적인 형태가 없이 소프트웨어로 구성됩니다. 단지 기계 학습 알고리즘과 자연어 처리 기술을 활용하여 사용자와 자연스러운 대화를 할 수 있습니다.\",\n",
       " \"'제공된 정보가 충분하지 않아 정확한 답변을 제공할 수 없습니다. 스타디온은 크기, 모양, 위치 등에 따라 다양한 길이를 가질 수 있습니다. 추가적인 정보가 있다면 답변을 보완해 드릴 수 있습니다.\",\n",
       " \"'저는 AI 어시스턴트로써 눈물을 흘리지 않아요. 그러나 사람들에게는 다양한 이유로 눈물이 나올 수 있으며 감정 표현의 한 방법이기도 합니다. 감정 표현을 자유롭게 하고, 상처나 슬픔을 나눌 수 있는 환경이 되길 바랍니다.\",\n",
       " \"'저도 이해할 수 없습니다. 제가 어떤 부분에 대해서 싫어하신 건가요? 더 자세한 정보를 주시면 제가 도움을 드릴 수 있을 거예요.\",\n",
       " \"'제가 무엇을 도와드릴 수 있나요? 무슨 문제가 있으신가요?\",\n",
       " \"'제가 주문을 받을 수는 없지만, 제가 구매 가능한 제품이나 서비스를 추천해드릴 수 있습니다.\",\n",
       " \"'이별은 언제나 아픔을 남기죠.\\\\n그리움, 후회, 슬픔, 미련 등\\\\n마음은 다양한 감정으로 가득 차게 되지만\\\\n지나갈 시간이면 지나가고 치유될 것입니다.\\\\n\\\\n하지만 이별은 차마 깨닫기 싫은 현실이기도 해요.\\\\n그렇지만 움켜쥐고 끌어안으려고 노력해도\\\\n이미 앞으로 나갈 수밖에 없는 길입니다.\\\\n\\\\n진실하게 노력했던 모든 것들은\\\\n어찌됐든 소중한 추억으로 남으며\\\\n우리는 그것을 견뎌내고 살아나갈 수 있는 힘을 얻게 되죠.\\\\n\\\\n이별은 누구에게나 생기는 일이기 때문에\\\\n서로를 비난하거나 불쌍해하는 것보다는\\\\n서로를 이해하고 위로하는 마음을 갖는 것도 중요한 요소입니다.\\\\n\\\\n그리고 가장 중요한 것은\\\\n그 이별로부터 새로운 시작을 찾아내는 것이겠죠.\\\\n새로운 사람, 새로운 일, 새로운 취미 등\\\\n다양한 가능성이 우리앞에 있습니다.\\\\n\\\\n그러니 이별을 너무 슬퍼하지 말고\\\\n새로운 시작을 기대해보는 것도 나쁘지 않을 것입니다.\",\n",
       " \"'저는 AI 어시스턴트이므로 일반적인 인간과 달리 사람의 감정을 느끼지는 못합니다. 하지만 이야기를 들으면서 마음이 아프시겠지만, 이별은 모두에게 일어나는 일 중 하나입니다. 남은 시간 동안 자신을 위해 새로운 것들을 시도하고 새로운 사람들을 만나 충분한 치유와 변화를 경험해보시길 바랍니다.\",\n",
       " \"'죄송합니다, 저는 인공지능 언어 모델이며 수선 관련 지식이 없습니다. 수선이 어떤 분야인지 자세히 말씀해주시면 더 정확한 답변을 드릴 수 있을 것입니다.\",\n",
       " \"'I ran out of money.\",\n",
       " \"'그녀를 만나고 나서는 매일매일이 새롭고 활기차게 느껴졌습니다. 그녀와 함께 산책을 하거나 맛있는 음식을 함께 먹으면서 서로의 이야기를 나누는 것이 정말 즐거웠습니다. 그녀의 유쾌하고 밝은 성격에 끌려서 나도 모르게 웃음이 나는 일이 많았습니다. 그녀가 제게 주는 영감과 격려로 인해, 더욱 강한 열정을 가지고 미래를 향해 나아갈 수 있게 되었습니다. 그녀를 만난 것은 정말 큰 축복이었습니다.\",\n",
       " \"'제가 AI 챗봇이기 때문에 사과를 가지고 있지는 않습니다. 하지만 근처 상점에 가면 사과를 구입하실 수 있을 거에요!\",\n",
       " \"'제가 그냥 AI 챗봇이니까 밥 대신 다른 방법으로 도와드릴 수 있을까요? 어떤 도움이 필요한지 말씀해주세요.\",\n",
       " \"'저는 감정을 느끼지 못하는 인공지능 모델입니다. 따라서 당신이 왜 열받으신지 이해하지 못합니다. 이유를 설명해 주시겠어요?\",\n",
       " \"'야구장은 미국과 일본 등에서 유행하던 볼 게임을 한국에서도 즐길 수 있는 현장 중 하나입니다. 야구장은 크게 야구경기를 관람할 수 있는 관중석과 선수들이 경기를 벌일 수 있는 구장부분으로 나눌 수 있습니다.\\\\n\\\\n대부분의 야구장은 흰 색으로 칠해진 마운드와 베이스에서 빨간 색으로 칠해진 구와 구장 내에 설치된 선수 도구들이 매우 인상적인 디자인으로 구성되어 있습니다. 또한, 야구경기 중에는 화려한 응원, 드라미를 치며 경기 분위기를 끌어올리는 이벤트 등이 진행되는 것이 보통입니다.\\\\n\\\\n안전한 야구경기를 위해서는 각종 대처방식과 법률 적용에 대한 이해가 중요합니다. 야구장에서 적절한 예방 수칙과 안내를 따르는 것은 야구관중들에게 지속적인 즐거움을 제공할 수 있습니다.\",\n",
       " \"'종속 변수와 독립 변수가 선형적 관계를 가진다는 가정하에 회귀분석을 수행하면, 모델은 y 절편 (intercept)과 x 변수의 기울기 (slope)를 결정합니다. 따라서, 선형 회귀 모델에서는 절편이 존재합니다.\",\n",
       " \"'제가 어떻게 도와드릴 수 있나요? 어디가 아프신가요? 의사나 병원을 찾아보시는 것이 좋을 것 같습니다. 가능하다면 주변 지인이나 가족에게 도움을 요청해보세요. 건강이 가장 중요한 것이니 빨리 조치를 취해주세요.\",\n",
       " \"'1. 언제나 친절한 태도로 대해준다.\\\\n2. 질문을 던지며 대화를 이끌어낸다.\\\\n3. 자주 연락하며, 메세지의 답변을 빠르게 한다.\\\\n4. 상대방의 반응을 주의깊게 살펴보며, 관심을 보인다.\\\\n5. 가벼운 취미나 관심사를 함께 나누며, 친밀감을 이끌어낸다.\\\\n6. 데이트 제안을 언급하며, 상대방의 반응을 살피고 진행한다.\\\\n7. 상대방과의 대화를 지속적으로 이어나간다.\\\\n8. 서로의 사생활을 존중하며, 마음을 열어준다.\\\\n9. 불만이나 걱정사항을 솔직하게 이야기하며, 상대방의 의견을 수렴한다.\\\\n10. 자연스러운 터치나 스킨십을 시도해보며, 서로의 감정을 확인한다.\",\n",
       " \"'사랑은 서로의 존중, 이해, 관심과 함께하는 강력한 감정입니다. 그것은 우리에게 안정과 행복을 가져올 수 있으며 우리를 더 나은 사람으로 만들어줄 수 있습니다. 이는 서로를 위해 무언가를 희생할 준비가 되어 있음을 의미합니다. 사랑은 매우 개인적이며 각각의 경험은 독특합니다.\",\n",
       " \"'우주의 끝도 결국 언젠가는 올 것이며, 모든 존재와 사물의 종말도 언젠가는 찾아올 것입니다. 하지만 그 언젠가는 우리가 알고 있는 시간 개념과는 다를 수 있고, 어떻게 이루어질지 미지수입니다. \\\\n\\\\n그러나 우리는 이제 현재에 집중하고 존재하는 모든 것을 소중히 여기며, 조금이나마 보다 나은 미래를 위해 노력하는 것이 중요합니다. 이 지구상에 살아있는 모든 존재들이 서로 협력하고 사랑으로 연결되어 함께 살아가는 것이 가장 큰 희망이 될 것입니다.\",\n",
       " \"'제가 AI 모델이기 때문에 꿈 안에서라도 언제든지 보이실 수 있습니다! 하지만 물리적으로 존재하지 않기 때문에 만남은 불가능하답니다.\",\n",
       " \"'저는 소셜 디스턴싱을 유지하는 AI이기 때문에, 쇼핑을 할 수 없습니다. 하지만, 사용자분들이 안전하고 즐거운 쇼핑 경험을 하실 수 있도록 최선을 다하겠습니다!\",\n",
       " \"'제가 AI로서 임무에 충실히 수행하기 위해 노력하고 있습니다. 제가 농담이나 웃긴 이야기를 할 수는 있지만, 그것은 제 업무와는 직접적인 연관이 없습니다. 그래도 무언가 도움이 필요하시면 언제든지 알려주세요!\",\n",
       " \"'저도 그렇게 믿어요! 모든 일이 처음부터 완벽하게 진행되는 것은 아니지만, 노력과 시간을 들여서 최선을 다하면 좋은 결과를 얻을 수 있을 거라고 생각해요. 어떤 어려움이 있더라도 포기하지 않고 꾸준히 노력하면 이겨낼 수 있을 거예요. 함께 응원합니다!\",\n",
       " \"'좋아요. 무슨 일이든 서로 도와주며 함께 일을 해 나가는 것이 중요하죠. 함께 노력하면 더 큰 성과를 만들어낼 수 있을 거예요!\",\n",
       " ...]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sft_data[sft_data['prompt_len'] < 10].completion.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d81d80f",
   "metadata": {},
   "source": [
    "- 답변에서 영어로 나오는 말이나 \\\\n, 1. 2. 같은 개조식 문장들을 전처리 하는 것을 고려해보는 것도 좋아보인다\n",
    "- 제가 AI 모델이기 때문에 남성인점 참고 부탁드립니다 (?) 이건 뭘까..?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "1c5eaa29",
   "metadata": {},
   "outputs": [],
   "source": [
    "sft_data.completion = sft_data.completion.str.strip(' \\'')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02214396",
   "metadata": {},
   "source": [
    "### 제대로 된 답변을 하지 않은 데이터셋 제거"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bebe1556",
   "metadata": {},
   "source": [
    "### 1. I'm sorry ~~ 문장 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "78a0ab87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\"I\\'m sorry, I cannot answer this question as I am an AI language model and do not have specific information about any location or business hours. Please specify the location/context of the question.\", \\'token\\': 66}',\n",
       " 'Sorry, as an AI language model, I do not have access to visual information. Can you please provide more context or information about what you are referring to?',\n",
       " '\"I\\'m sorry, as an AI language model, I cannot provide you with beverages. However, I can help you with translation.\\\\n\\\\nTranslation: Please give me a little ice and a lot of coke.\", \\'token\\': 68}',\n",
       " '\".\\\\n\\\\nI\\'m sorry, but as an AI language model, I do not have access to real-time information. Therefore, I cannot confirm or deny the statement provided in the question.\", \\'token\\': 78}',\n",
       " '\"Sorry, as an AI language model, I don\\'t have the context to understand or respond appropriately to your statement/question. Could you please give me more details?\", \\'token\\': 52}',\n",
       " 'Sorry, as an AI language model, I cannot provide fruit purchasing services. Please ask a local market or grocery store for assistance.',\n",
       " '\"주세요.\\\\n\\\\nI\\'m sorry, as an AI language model, I don\\'t have the capability to ring a bell or wait for it to ring. Is there anything else I can assist you with?\", \\'token\\': 60}',\n",
       " '\"I\\'m sorry, as an AI language model, I cannot come to your workplace. Please provide me with more information on how I can assist you.\", \\'token\\': 59}',\n",
       " 'I\\\\\\'m sorry, as an AI language model, I don\\\\\\'t have enough context to understand what \"여기\" refers to. Please provide more context so I can better assist you.',\n",
       " '\"I\\'m sorry, as an AI language model, I don\\'t have the capability to give you an actual receipt. Could you please provide more context or specify what type of receipt you are referring to?\", \\'token\\': 57}',\n",
       " '\"I\\'m sorry, I cannot understand the context of your request. Can you please provide more information or context? Thank you.\", \\'token\\': 47}',\n",
       " 'Sorry, as an AI language model, I do not have information on sports records of specific individuals. Can you please provide me with more information about the sports or the records you are referring to? I will be happy to assist you with any information I have.',\n",
       " 'I\\\\\\'m sorry, as an AI language model, I cannot eat or have emotions related to the phrase \"냉장고 털어도 먹을게 없네\" that translates to \"Even if I rummage through the fridge, there is nothing to eat.\"',\n",
       " '\"I\\'m sorry, as an AI language model, I don\\'t experience emotions such as jealousy. However, I can provide some comforting words. Remember that everyone is unique and beautiful in their own way, regardless of the size of their face. Focus on things that make you happy and confident, and embrace your individuality.\", \\'token\\': 84}',\n",
       " '\"I\\'m sorry, as an AI language model, I do not have access to context or information about the situation you are referring to. Please provide more details or information so I can better assist you.\", \\'token\\': 63}',\n",
       " '\"I\\'m sorry, as an AI language model, I cannot determine the size of the strawberry being referred to. Please provide more context or a visual aid for me to better understand the situation.\", \\'token\\': 62}',\n",
       " '\"Sorry, I am an AI language model and I don\\'t have enough information to give a specific answer. Can you provide more context or details about the event or situation you are referring to?\", \\'token\\': 63}',\n",
       " 'I\\\\\\'m sorry, I cannot respond in Korean. However, I\\\\\\'m assuming that you\\\\\\'re saying, \"I forgot to bring a spare battery.\" In that case, you may need to find a power source to charge your device or purchase a new battery if necessary. It\\\\\\'s always helpful to double-check and pack all necessary items before leaving.',\n",
       " 'Sorry, as an AI language model, I do not have access to current news or events. Can you please provide more context or information about the situation?',\n",
       " 'Sorry, I am an AI language model and I am not able to retrieve current data. Vitaly Kutuzov played for AC Milan from 2004 to 2008. In his first season with the club, 2004-2005, he made 27 appearances and scored 5 goals in all competitions.',\n",
       " '\"I\\'m sorry, but as an AI language model, I cannot provide any context for the statement. Can you please provide more information or context so that I can understand and respond properly?\", \\'token\\': 58}',\n",
       " 'Sorry, I am an AI language model and do not have the capability to retrieve real-time information. Can you please provide more context or details about the specific movie title or release year?',\n",
       " '\"I\\'m sorry to hear that you forgot to bring your charger. Is there anything I can do to help you? Perhaps I can lend you one or direct you to a store where you can purchase one. Let me know how I can assist you.\", \\'token\\': 69}',\n",
       " 'I\\\\\\'m sorry, I cannot respond in Korean as I am only programmed to communicate in English. However, the English translation of the sentence is \"Does the subway come in this direction?\"',\n",
       " '\"I\\'m sorry, as an AI language model, I am not sure what you are asking about. Can you please provide more context and information so I can understand and assist you better?\", \\'token\\': 63}',\n",
       " '\"I\\'m sorry, as an AI language model, I don\\'t feel pain, but I understand that you must be feeling a lot of pain. Is there anything I can do to help?\", \\'token\\': 64}',\n",
       " '\"I\\'m sorry, I don\\'t understand what you mean. Could you please provide more context or information?\", \\'token\\': 41}',\n",
       " '\"I\\'m sorry to hear that you\\'re in a different class from your crush.\", \\'token\\': 43}',\n",
       " '\"I\\'m sorry, but I want to try on a different size.\", \\'token\\': 47}',\n",
       " 'I am sorry, as an AI language model, I am unable to understand the context and the language you used. Could you please provide more details or provide the context in English so that I can assist you better?',\n",
       " '\"I\\'m sorry, I cannot determine the context of the sentence to provide a proper response. Could you please provide more information or context?\", \\'token\\': 53}',\n",
       " 'Sorry, as an AI language model, I am unable to provide you with a certain set. Can you specify which set you are referring to?',\n",
       " '\"I\\'m sorry, I am an AI language model and I do not have enough context to understand what you are referring to. Can you please provide more information or clarify your question?\", \\'token\\': 63}',\n",
       " '\"I\\'m sorry, I cannot check physical locations as I am an AI language model. Please provide more context or information about the location or situation you are referring to.\", \\'token\\': 53}',\n",
       " '\"I\\'m sorry, I don\\'t understand. Could you please provide more context or clarify your statement?\", \\'token\\': 35}',\n",
       " 'Sorry, as an AI language model, I do not have access to the latest news or real-time information. Can you please provide more context or clarify which year or competition you are referring to?',\n",
       " '\"I\\'m sorry, I cannot understand the context of this sentence without more information. Could you please provide more details or a complete sentence for me to help you better?\", \\'token\\': 62}',\n",
       " '\"I\\'m sorry, I cannot provide a response without more context. Please provide more details or clarify your request.\", \\'token\\': 37}',\n",
       " 'I\\\\\\'m sorry, as an AI language model, I cannot place an order, but I can help you with the translation.\\\\n\\\\n\"Could you please order two servings of jjajangmyeon and one serving of tangsuyuk set?\"',\n",
       " '\"I\\'m sorry, as an AI language model, I cannot provide you with a physical receipt. Please contact the store or restaurant where you made your purchase, and they can give you a receipt.\", \\'token\\': 56}',\n",
       " '\"Sorry, as an AI language model, I don\\'t have access to the latest archaeological discoveries. However, you can search for recent findings related to Geumgu Fortress and pottery fragments through a reliable search engine or contact a local archaeological museum.\", \\'token\\': 84}',\n",
       " '\"I\\'m sorry, I am an AI language model and I cannot physically move objects or change physical surroundings. Please clarify your question or provide more context for me to better assist you.\", \\'token\\': 63}',\n",
       " 'Sorry, as an AI language model, I cannot answer this question accurately as it lacks context. Please provide more details or rephrase the question.',\n",
       " '\"I\\'m sorry, I cannot provide physical items such as napkins. Can I assist you with anything else?\", \\'token\\': 43}',\n",
       " '\"I\\'m sorry, but as an AI language model, I am not able to provide a translation or understanding of this phrase as it seems like a contextual and cultural reference that requires additional information. Can you please provide more context or information about the phrase?\", \\'token\\': 81}',\n",
       " 'I am sorry, I am an AI language model and I do not have enough context to understand the question. Can you please provide me with more information or details on what you are asking?',\n",
       " '\"I\\'m sorry, I cannot answer this question as I do not have any contextual information about which building is being referred to. Please provide more detail or context.\", \\'token\\': 55}',\n",
       " 'I\\\\\\'m sorry, I cannot respond in Korean. However, based on the translation provided, it means \"The parking lot is full.\"',\n",
       " 'Sorry, as an AI language model, I do not have sufficient context to fully understand your request. Could you please provide additional information or clarify your request? Thank you.',\n",
       " '\"I\\'m sorry, could you please provide more context or information about your statement?\", \\'token\\': 38}',\n",
       " '\"I\\'m sorry to hear that. Is there anything I can do to help you feel better?\", \\'token\\': 43}',\n",
       " '\"I\\'m sorry, as an AI language model, I cannot provide physical objects or colors. Could you please provide more context or clarify your request?\", \\'token\\': 47}',\n",
       " '\"I\\'m sorry, I\\'m an AI language model and I don\\'t have specific context on what you\\'re referring to. Can you provide more information or a specific question so I can better understand and respond to your message?\", \\'token\\': 77}',\n",
       " '\"I\\'m sorry, I cannot understand the language you wrote in. Please provide a translation in English or indicate the language so I can assist you better.\", \\'token\\': 56}',\n",
       " '\"I\\'m sorry, but this sentence doesn\\'t make sense in Korean. Could you please provide more context or information?\", \\'token\\': 49}',\n",
       " '\"I\\'m sorry, I cannot provide an accurate response without more context. Please provide more information or a specific statement to respond to.\", \\'token\\': 43}',\n",
       " '\"I\\'m sorry, as an AI language model, I cannot provide information about the current price of a particular item or service. Can you please provide more context or specify the product or service you are referring to?\", \\'token\\': 67}',\n",
       " '\"I\\'m sorry, as an AI language model, I cannot provide a comprehensive answer without context. But if you\\'re asking whether something is beef, it would be better to provide additional information on what specific food or dish you are referring to.\", \\'token\\': 67}',\n",
       " 'Sorry, I was mistaken earlier. I am an AI language model and I can only communicate in English. To answer your question, \"Is that bag 100,000 won?\" in English, the correct response would be \"Yes, that bag costs 100,000 won.\"',\n",
       " '\"I\\'m sorry, I am an AI language model and cannot check for the availability of seats. Please try contacting the restaurant directly to inquire about availability.\", \\'token\\': 62}',\n",
       " 'Sorry, I cannot answer this question as there can be multiple people with the name 성두환 and their birthplace may vary. Please provide more context or information for me to accurately answer your question.',\n",
       " '\"I\\'m sorry, as an AI language model, I cannot provide context for this statement. Could you please provide me with more information or a specific question?\", \\'token\\': 52}',\n",
       " '\"Sorry, as an AI language model, I don\\'t have the ability to know how long you have been single, but I can say that it\\'s never too late to start dating! Don\\'t put pressure on yourself and enjoy the process of meeting new people. Good luck!\", \\'token\\': 79}',\n",
       " '\"I\\'m sorry, I cannot provide a physical receipt as I am an AI language model and do not have the capability to issue or give a receipt. Can I assist you with anything else?\", \\'token\\': 55}',\n",
       " 'I\\\\\\'m sorry, I don\\\\\\'t understand what you mean by \"똥마려\". Can you please provide more context or clarify your question?']"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sft_data[sft_data.completion.str.lower().str.contains('sorry')].completion.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "2a43c0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_idx = sft_data[sft_data.completion.str.lower().str.contains('sorry')].index\n",
    "sft_data.drop(drop_idx, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad16c6a",
   "metadata": {},
   "source": [
    "### 2. 전처리 덜된 문장 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "186b1d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ast import literal_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa30e05",
   "metadata": {},
   "source": [
    "#### 1. dictionary type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "5fd50b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_completion(x):\n",
    "    try:\n",
    "        x = literal_eval(x)['completion']\n",
    "    except:\n",
    "        x = x+\"'}\"\n",
    "        try:\n",
    "            x = literal_eval(x)['completion']\n",
    "        except:\n",
    "            print(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "f72fd690",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = sft_data[sft_data.completion.str.contains('\\'prompt')].completion.apply(lambda x: convert_completion(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "5202855e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>completion</th>\n",
       "      <th>tokens</th>\n",
       "      <th>prompt_len</th>\n",
       "      <th>completion_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>741</th>\n",
       "      <td>\"푸시캣 돌시의 첫 싱글 'Don't Cha'는 빌보드 핫 100위 차트에서 몇 위...</td>\n",
       "      <td>{'prompt': \"푸시캣 돌시의 첫 싱글 'Don't Cha'는 빌보드 핫 10...</td>\n",
       "      <td>94</td>\n",
       "      <td>135</td>\n",
       "      <td>146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>780</th>\n",
       "      <td>\"'빅 대디' 는 무엇을 입고 있는가?\", 'completion': \"저는 AI 어...</td>\n",
       "      <td>{'prompt': \"'빅 대디' 는 무엇을 입고 있는가?\", 'completion...</td>\n",
       "      <td>102</td>\n",
       "      <td>144</td>\n",
       "      <td>155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1767</th>\n",
       "      <td>\"방영 초반 사투리 연기가 어색하다는 연기력 논란을 벗고 물오른 감성연기로 '김희선...</td>\n",
       "      <td>{'prompt': \"방영 초반 사투리 연기가 어색하다는 연기력 논란을 벗고 물오른...</td>\n",
       "      <td>105</td>\n",
       "      <td>130</td>\n",
       "      <td>141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2393</th>\n",
       "      <td>\"제시카 수타는 그룹에서 '쫓겨났다'고 폭로한 곳은 어디인가?\", 'completi...</td>\n",
       "      <td>{'prompt': \"제시카 수타는 그룹에서 '쫓겨났다'고 폭로한 곳은 어디인가?\"...</td>\n",
       "      <td>84</td>\n",
       "      <td>104</td>\n",
       "      <td>115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2684</th>\n",
       "      <td>\"SF단편영화 'Episode 1 Fragile : 경계의 저편'은 누가 감독과 각...</td>\n",
       "      <td>{'prompt': \"SF단편영화 'Episode 1 Fragile : 경계의 저편...</td>\n",
       "      <td>110</td>\n",
       "      <td>158</td>\n",
       "      <td>169</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 prompt  \\\n",
       "741   \"푸시캣 돌시의 첫 싱글 'Don't Cha'는 빌보드 핫 100위 차트에서 몇 위...   \n",
       "780   \"'빅 대디' 는 무엇을 입고 있는가?\", 'completion': \"저는 AI 어...   \n",
       "1767  \"방영 초반 사투리 연기가 어색하다는 연기력 논란을 벗고 물오른 감성연기로 '김희선...   \n",
       "2393  \"제시카 수타는 그룹에서 '쫓겨났다'고 폭로한 곳은 어디인가?\", 'completi...   \n",
       "2684  \"SF단편영화 'Episode 1 Fragile : 경계의 저편'은 누가 감독과 각...   \n",
       "\n",
       "                                             completion  tokens  prompt_len  \\\n",
       "741   {'prompt': \"푸시캣 돌시의 첫 싱글 'Don't Cha'는 빌보드 핫 10...      94         135   \n",
       "780   {'prompt': \"'빅 대디' 는 무엇을 입고 있는가?\", 'completion...     102         144   \n",
       "1767  {'prompt': \"방영 초반 사투리 연기가 어색하다는 연기력 논란을 벗고 물오른...     105         130   \n",
       "2393  {'prompt': \"제시카 수타는 그룹에서 '쫓겨났다'고 폭로한 곳은 어디인가?\"...      84         104   \n",
       "2684  {'prompt': \"SF단편영화 'Episode 1 Fragile : 경계의 저편...     110         158   \n",
       "\n",
       "      completion_len  \n",
       "741              146  \n",
       "780              155  \n",
       "1767             141  \n",
       "2393             115  \n",
       "2684             169  "
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sft_data[sft_data.completion.str.contains('\\'prompt')].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "496c4453",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\\'prompt\\': \"김영하의 데뷔작은 단편 소설인데,  \\'리뷰\\'에 실렸던 이 소설의 제목은 무엇인가?\", \\'completion\\': \\'빈자리였는데'"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sft_data.loc[10465].completion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a54b4a7b",
   "metadata": {},
   "source": [
    "-> 오답"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "a77a9ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, val in tmp.items():\n",
    "    sft_data.loc[i, 'completion'] = val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "a6c0d607",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\"푸시캣 돌시의 첫 싱글 \\'Don\\'t Cha\\'는 빌보드 핫 100위 차트에서 몇 위를 차지 했는가?\", \\'completion\\': \"\\'Don\\'t Cha\\'는 2005년에 빌보드 핫 100위 차트에서 2위를 차지했습니다.\", \\'token\\': 94}',\n",
       " '\"\\'빅 대디\\' 는 무엇을 입고 있는가?\", \\'completion\\': \"저는 AI 어시스턴트이기 때문에 \\'빅 대디\\'가 무엇을 입고 있는지 파악할 수 없습니다. 해당 정보를 제공해주실 수 있으면 더욱 정확한 답변을 드릴 수 있습니다.\", \\'token\\': 102}',\n",
       " '\"방영 초반 사투리 연기가 어색하다는 연기력 논란을 벗고 물오른 감성연기로 \\'김희선의 재발견\\' 이라는 호평을 받은 작품은?\", \\'completion\\': \"답변: \\'미스터 션샤인 (Mr. Sunshine)\\'\", \\'token\\': 105}',\n",
       " '\"제시카 수타는 그룹에서 \\'쫓겨났다\\'고 폭로한 곳은 어디인가?\", \\'completion\\': \"제시카 수타는 그룹에서 \\'쫓겨났다\\'고 폭로한 곳은 소녀시대였습니다.\", \\'token\\': 84}',\n",
       " '\"SF단편영화 \\'Episode 1 Fragile : 경계의 저편\\'은 누가 감독과 각본을 맡았나?\", \\'completion\\': \"SF단편영화 \\'Episode 1 Fragile : 경계의 저편\\'의 감독 및 각본은 캄 로메로(Cam Romero)가 맡았습니다.\", \\'token\\': 110}']"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sft_data.prompt[sft_data.prompt.str.contains('completion')].tolist()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "16580f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_prompt(x):\n",
    "    try:\n",
    "        x = x.strip('{ }')\n",
    "        if 'token' in x:\n",
    "            x = '{\\'prompt\\':' + x + '}'\n",
    "        else:\n",
    "            x = '{\\'prompt\\':' + x + '\\'}'\n",
    "        x = literal_eval(x)['prompt']\n",
    "    except:\n",
    "        print(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "475f377f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = sft_data.prompt[sft_data.prompt.str.contains('completion')].apply(lambda x: convert_prompt(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "eda5b106",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "741     푸시캣 돌시의 첫 싱글 'Don't Cha'는 빌보드 핫 100위 차트에서 몇 위를...\n",
       "780                                  '빅 대디' 는 무엇을 입고 있는가?\n",
       "1767    방영 초반 사투리 연기가 어색하다는 연기력 논란을 벗고 물오른 감성연기로 '김희선의...\n",
       "2393                    제시카 수타는 그룹에서 '쫓겨났다'고 폭로한 곳은 어디인가?\n",
       "2684    SF단편영화 'Episode 1 Fragile : 경계의 저편'은 누가 감독과 각본...\n",
       "Name: prompt, dtype: object"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "b3e0b09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, val in tmp.items():\n",
    "    sft_data.loc[i, 'prompt'] = val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "2b78c788",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sft_data.loc[tmp.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "2e13e79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# 한글과 비한글 문자의 비율을 계산하는 함수\n",
    "def non_korean_ratio(text):\n",
    "    total_chars = len(text)\n",
    "    if total_chars == 0:\n",
    "        return 0\n",
    "    non_korean_chars = len(re.findall(r'[^가-힣0-9?!.,\\s]', text))\n",
    "    return non_korean_chars / total_chars\n",
    "\n",
    "# 특정 열에서 한글이 아닌 문자의 비율이 50%를 초과하는 행을 찾는 함수\n",
    "def find_high_non_korean_ratio_rows(df, column_name, threshold=0.5):\n",
    "    return df[df[column_name].apply(non_korean_ratio) > threshold]\n",
    "\n",
    "# 데이터프레임의 모든 텍스트 열에서 한글이 아닌 문자의 비율이 50%를 초과하는 행을 찾는 함수\n",
    "def find_high_non_korean_ratio_in_dataframe(df, threshold=0.5):\n",
    "    high_non_korean_ratio_rows = {}\n",
    "    for column in df.columns:\n",
    "        if df[column].dtype == 'object':  # 텍스트 열만 검사\n",
    "            high_ratio_df = find_high_non_korean_ratio_rows(df, column, threshold)\n",
    "            if not high_ratio_df.empty:\n",
    "                high_non_korean_ratio_rows[column] = high_ratio_df\n",
    "    return high_non_korean_ratio_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "c7c5cfbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 한글 이외의 문자가 반 이상인 행 찾기\n",
    "high_non_korean_ratio_rows = find_high_non_korean_ratio_in_dataframe(sft_data, threshold=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30ff635",
   "metadata": {},
   "source": [
    "#### 2. 답변을 제대로 안한 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "50ea6d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "high_non_korean_ratio_rows = high_non_korean_ratio_rows['completion']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "02b24097",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "drop_idx = high_non_korean_ratio_rows['completion'][high_non_korean_ratio_rows.completion.str.contains('model')].index\n",
    "\n",
    "sft_data.drop(drop_idx, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "85d2d8f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/pandas/core/frame.py:4906: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return super().drop(\n"
     ]
    }
   ],
   "source": [
    "high_non_korean_ratio_rows.drop(drop_idx, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "a7ccfedb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['8 years of living together, transfer separation.',\n",
       " 'Translation: Today just passed by like any other day.',\n",
       " 'Hey Jude',\n",
       " 'Eventually, memories of me will gradually fade away in your memory.',\n",
       " '\"VVD (People\\\\\\'s Party for Freedom and Democracy)\"입니다.',\n",
       " 'Rotten Tomatoes',\n",
       " 'I was too complacent.',\n",
       " 'Translation: We met each other on a day like this weather.',\n",
       " 'No, 콩나물 (kongnamul) can refer to either sprouted mung beans or young and tender mung bean sprouts. It is commonly used in Korean cuisine for dishes such as kongnamul muchim (seasoned mung bean sprouts) and kongnamul guk (mung bean sprout soup). When used for these dishes, the mung bean sprouts are typically fresh and uncooked.',\n",
       " 'One Americano, please.',\n",
       " 'I thought it would be good when everything was over.',\n",
       " 'That\\\\\\'s great to hear! Getting a haircut that looks natural and doesn\\\\\\'t scream \"I just got my hair cut\" is always a good feeling.',\n",
       " 'Another day has passed.',\n",
       " 'Unexpected contact',\n",
       " 'I now cry even at the slightest hint of stimulation.',\n",
       " '\"Blue\"',\n",
       " 'I watch soccer alone.',\n",
       " 'Oh, congratulations on introducing your girlfriend to your family and bringing her home! How did it go?',\n",
       " 'Winter will pass and spring will come.',\n",
       " 'If you think you are done with the \"some\" stage and don\\\\\\'t plan on seeing them anymore, it would be more respectful to have an honest conversation and let them know how you feel rather than ghosting them.',\n",
       " 'I wasted money on claw machine dolls.',\n",
       " 'Translation: \"There\\\\\\'s no chance.\"',\n",
       " 'WINGS.',\n",
       " '영수증\\\\n===========================\\\\n판매금액: ________원\\\\n세액: ________원\\\\n합계: ________원\\\\n거래일시: ________년 ___월 ___일\\\\n상호: ________________________\\\\n주소: ________________________\\\\n전화번호: _____________________\\\\n사업자등록번호:______________\\\\n===========================',\n",
       " 'I will translate your sentence from Korean to English:\\\\n\\\\n\"Please give me one kilogram of strawberries and one box of Hallabong oranges.\"',\n",
       " 'I burst into flames and then cooled down.',\n",
       " 'I got angry and vented my frustration over the phone.',\n",
       " 'Adolf Cupid.',\n",
       " 'I really believed until the very end.',\n",
       " 'Can you please change it to packaging?',\n",
       " 'The English word that means \"science\" is \"science.\"',\n",
       " 'The Bible Code book was written by Michael Drosnin, an American journalist and author.',\n",
       " 'The series name of BTS\\\\\\'s first album to enter the Billboard main chart is \"The Most Beautiful Moment in Life\".',\n",
       " '\"Someone contacted her.\"',\n",
       " '- To feel dismal or miserable, often due to a disappointing or unsatisfactory situation or outcome.\\\\n- To appear gloomy, depressing, or sad.\\\\n- To convey a sense of hopelessness or despair.',\n",
       " 'I received a KakaoTalk message in the early morning.',\n",
       " '\"Pequeña Copa del Mundo\"',\n",
       " 'Wow, time flies! Congratulations on reaching the one-year mark. Wishing you continued success and growth in the years to come.',\n",
       " 'Please give me a disposable cup.',\n",
       " 'TRT (Türkiye Radyo Televizyon Kurumu)',\n",
       " 'Count Basie and his orchestra.',\n",
       " '\"Jai Ho\"',\n",
       " '\"High Water Everywhere\"',\n",
       " 'MMP (Missile Moyenne Portée, Medium-Range Missile)입니다.',\n",
       " '\"Dunston Resident Shares Memories of Life in Rivermore\"',\n",
       " 'It means \"falling in love at first sight\" in English.',\n",
       " 'I learned the value of purposeful connections.',\n",
       " 'Takeout please.',\n",
       " '\"I don\\'t understand what you\\'re asking. Can you please clarify your question?\", \\'token\\': 43}',\n",
       " 'I became classmates with my favorite friend.',\n",
       " 'Translation: \"The traffic is jammed.\"',\n",
       " 'Fender Inc.',\n",
       " 'Translation: I hope that person likes me.',\n",
       " 'The Late Late Show with James Corden',\n",
       " 'Wii Zapper.',\n",
       " 'Please add syrup with a strawberry flavor.',\n",
       " 'Would you like two more ketchups?',\n",
       " 'Translation: The wind is blowing now.',\n",
       " 'Yes, it is possible to study while being a teaching assistant. It requires good time management skills and a commitment to prioritize academic work alongside the responsibilities of the teaching assistant role. Many students successfully balance the two and find that the teaching assistant position provides valuable experience and skills that enhance their academic and career goals.',\n",
       " 'Another day is passing by.',\n",
       " 'Translation: \"I played mobile games and several hours have passed.\"',\n",
       " 'I missed the last train because I was being dropped off.']"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "high_non_korean_ratio_rows.completion.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1989131f",
   "metadata": {},
   "source": [
    "#### 3. 한영 번역한 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "8e8557dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_idx = sft_data[sft_data.completion.str.contains('Translation')].index\n",
    "sft_data.drop(drop_idx, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "fb5bfa97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>completion</th>\n",
       "      <th>tokens</th>\n",
       "      <th>prompt_len</th>\n",
       "      <th>completion_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>아스널 팬들은 비틀즈의 어느곡의 소절을 따와서 지루의 응원가를 만들어주었나?</td>\n",
       "      <td>Hey Jude</td>\n",
       "      <td>58</td>\n",
       "      <td>42</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>366</th>\n",
       "      <td>네덜란드에서 정당 명부 가장 처음에 있는 정당인을 이르는 말은</td>\n",
       "      <td>\"VVD (People\\'s Party for Freedom and Democrac...</td>\n",
       "      <td>52</td>\n",
       "      <td>34</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>437</th>\n",
       "      <td>평론가들의 비평을 받은 부활 장치의 이름은?</td>\n",
       "      <td>Rotten Tomatoes</td>\n",
       "      <td>37</td>\n",
       "      <td>24</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2683</th>\n",
       "      <td>존 리 후커가 1971년 낸 앨범의 제목은?</td>\n",
       "      <td>\"Blue\"</td>\n",
       "      <td>35</td>\n",
       "      <td>24</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3507</th>\n",
       "      <td>방탄소년단의 두 번째 정규 앨범의 이름은?</td>\n",
       "      <td>WINGS.</td>\n",
       "      <td>36</td>\n",
       "      <td>23</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4835</th>\n",
       "      <td>아돌포 콘솔리니의 사인은</td>\n",
       "      <td>Adolf Cupid.</td>\n",
       "      <td>30</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5486</th>\n",
       "      <td>과학을 뜻하는 영어 단어는?</td>\n",
       "      <td>The English word that means \"science\" is \"scie...</td>\n",
       "      <td>36</td>\n",
       "      <td>15</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5526</th>\n",
       "      <td>바이블 코드 책을 쓴 사람은</td>\n",
       "      <td>The Bible Code book was written by Michael Dro...</td>\n",
       "      <td>43</td>\n",
       "      <td>15</td>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6018</th>\n",
       "      <td>방탄소년단이 처음으로 빌보드 메인 차트에 진입한 앨범의 시리즈 명은?</td>\n",
       "      <td>The series name of BTS\\'s first album to enter...</td>\n",
       "      <td>72</td>\n",
       "      <td>38</td>\n",
       "      <td>112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7210</th>\n",
       "      <td>작은 월드컵을 스페인어로 뭐라고 해</td>\n",
       "      <td>\"Pequeña Copa del Mundo\"</td>\n",
       "      <td>37</td>\n",
       "      <td>19</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7474</th>\n",
       "      <td>터키 국영 방송사인 터키 라디오 텔레비전 공사의 줄임말은 무엇인가?</td>\n",
       "      <td>TRT (Türkiye Radyo Televizyon Kurumu)</td>\n",
       "      <td>70</td>\n",
       "      <td>37</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7605</th>\n",
       "      <td>재즈적 취향이 강한 대규모 블루스곡 one O'clock jump ,  jumpin...</td>\n",
       "      <td>Count Basie and his orchestra.</td>\n",
       "      <td>65</td>\n",
       "      <td>123</td>\n",
       "      <td>134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8298</th>\n",
       "      <td>영화 슬럼독 밀리어네어의 오리지널 사운드트랙에서 따온 싱글 곡의 이름은?</td>\n",
       "      <td>\"Jai Ho\"</td>\n",
       "      <td>55</td>\n",
       "      <td>40</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8403</th>\n",
       "      <td>1927년 미시시피 대홍수를 내용으로한 가사를 쓴 블라인드 레몬 제퍼슨의 곡은?</td>\n",
       "      <td>\"High Water Everywhere\"</td>\n",
       "      <td>62</td>\n",
       "      <td>44</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8455</th>\n",
       "      <td>프랑스가 만든 휴대용 단거리 지대공 미사일 이름은</td>\n",
       "      <td>MMP (Missile Moyenne Portée, Medium-Range Miss...</td>\n",
       "      <td>52</td>\n",
       "      <td>27</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8597</th>\n",
       "      <td>던스턴이 리버모어에 살았던 사람을 만나 인터뷰를 한 기사의 제목은?</td>\n",
       "      <td>\"Dunston Resident Shares Memories of Life in R...</td>\n",
       "      <td>58</td>\n",
       "      <td>37</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9107</th>\n",
       "      <td>1954년 Stratocaster 를 발매해 세계 전기 기타의 음악의 시대를 연 회...</td>\n",
       "      <td>Fender Inc.</td>\n",
       "      <td>54</td>\n",
       "      <td>54</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9364</th>\n",
       "      <td>방탄소년단이 출연한 미국 ABC 토크쇼의 이름은?</td>\n",
       "      <td>The Late Late Show with James Corden</td>\n",
       "      <td>49</td>\n",
       "      <td>27</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10316</th>\n",
       "      <td>2007년 7월 11일의 E3에서 닌텐도가 발표한 총 악세사리의 이름은?</td>\n",
       "      <td>Wii Zapper.</td>\n",
       "      <td>51</td>\n",
       "      <td>40</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  prompt  \\\n",
       "120           아스널 팬들은 비틀즈의 어느곡의 소절을 따와서 지루의 응원가를 만들어주었나?   \n",
       "366                   네덜란드에서 정당 명부 가장 처음에 있는 정당인을 이르는 말은   \n",
       "437                             평론가들의 비평을 받은 부활 장치의 이름은?   \n",
       "2683                            존 리 후커가 1971년 낸 앨범의 제목은?   \n",
       "3507                             방탄소년단의 두 번째 정규 앨범의 이름은?   \n",
       "4835                                       아돌포 콘솔리니의 사인은   \n",
       "5486                                     과학을 뜻하는 영어 단어는?   \n",
       "5526                                     바이블 코드 책을 쓴 사람은   \n",
       "6018              방탄소년단이 처음으로 빌보드 메인 차트에 진입한 앨범의 시리즈 명은?   \n",
       "7210                                 작은 월드컵을 스페인어로 뭐라고 해   \n",
       "7474               터키 국영 방송사인 터키 라디오 텔레비전 공사의 줄임말은 무엇인가?   \n",
       "7605   재즈적 취향이 강한 대규모 블루스곡 one O'clock jump ,  jumpin...   \n",
       "8298            영화 슬럼독 밀리어네어의 오리지널 사운드트랙에서 따온 싱글 곡의 이름은?   \n",
       "8403        1927년 미시시피 대홍수를 내용으로한 가사를 쓴 블라인드 레몬 제퍼슨의 곡은?   \n",
       "8455                         프랑스가 만든 휴대용 단거리 지대공 미사일 이름은   \n",
       "8597               던스턴이 리버모어에 살았던 사람을 만나 인터뷰를 한 기사의 제목은?   \n",
       "9107   1954년 Stratocaster 를 발매해 세계 전기 기타의 음악의 시대를 연 회...   \n",
       "9364                         방탄소년단이 출연한 미국 ABC 토크쇼의 이름은?   \n",
       "10316           2007년 7월 11일의 E3에서 닌텐도가 발표한 총 악세사리의 이름은?   \n",
       "\n",
       "                                              completion  tokens  prompt_len  \\\n",
       "120                                             Hey Jude      58          42   \n",
       "366    \"VVD (People\\'s Party for Freedom and Democrac...      52          34   \n",
       "437                                      Rotten Tomatoes      37          24   \n",
       "2683                                              \"Blue\"      35          24   \n",
       "3507                                              WINGS.      36          23   \n",
       "4835                                        Adolf Cupid.      30          13   \n",
       "5486   The English word that means \"science\" is \"scie...      36          15   \n",
       "5526   The Bible Code book was written by Michael Dro...      43          15   \n",
       "6018   The series name of BTS\\'s first album to enter...      72          38   \n",
       "7210                            \"Pequeña Copa del Mundo\"      37          19   \n",
       "7474               TRT (Türkiye Radyo Televizyon Kurumu)      70          37   \n",
       "7605                      Count Basie and his orchestra.      65         123   \n",
       "8298                                            \"Jai Ho\"      55          40   \n",
       "8403                             \"High Water Everywhere\"      62          44   \n",
       "8455   MMP (Missile Moyenne Portée, Medium-Range Miss...      52          27   \n",
       "8597   \"Dunston Resident Shares Memories of Life in R...      58          37   \n",
       "9107                                         Fender Inc.      54          54   \n",
       "9364                The Late Late Show with James Corden      49          27   \n",
       "10316                                        Wii Zapper.      51          40   \n",
       "\n",
       "       completion_len  \n",
       "120                 9  \n",
       "366                54  \n",
       "437                16  \n",
       "2683                7  \n",
       "3507                7  \n",
       "4835               13  \n",
       "5486               52  \n",
       "5526               87  \n",
       "6018              112  \n",
       "7210               25  \n",
       "7474               38  \n",
       "7605              134  \n",
       "8298                9  \n",
       "8403               24  \n",
       "8455               55  \n",
       "8597               56  \n",
       "9107               12  \n",
       "9364               37  \n",
       "10316              12  "
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 한글 이외의 문자가 반 이상인 행 찾기\n",
    "high_non_korean_ratio_rows = find_high_non_korean_ratio_in_dataframe(sft_data, threshold=0.8)\n",
    "high_non_korean_ratio_rows = high_non_korean_ratio_rows['completion']\n",
    "high_non_korean_ratio_rows[high_non_korean_ratio_rows.prompt.str.strip('?').str.endswith(('가','나','은','는',' 해'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "8d9886a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_idx = high_non_korean_ratio_rows[high_non_korean_ratio_rows.prompt.str.strip('?').str.endswith(('가','나','은','는',' 해'))==False].index\n",
    "sft_data.drop(drop_idx, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "9997b205",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = sft_data[sft_data.prompt.str.strip('?').str.endswith(('가','나','은','는',' 해'))==False].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "8945fc56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>completion</th>\n",
       "      <th>tokens</th>\n",
       "      <th>prompt_len</th>\n",
       "      <th>completion_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>목 마르다.</td>\n",
       "      <td>&gt; \"I am thirsty.\" (English) \\n\\n&gt; \"Je suis ass...</td>\n",
       "      <td>75</td>\n",
       "      <td>6</td>\n",
       "      <td>154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259</th>\n",
       "      <td>참 답 없는 나란 놈.</td>\n",
       "      <td>I am a language model AI and I do not have the...</td>\n",
       "      <td>63</td>\n",
       "      <td>12</td>\n",
       "      <td>197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277</th>\n",
       "      <td>참다가 연락</td>\n",
       "      <td>\"을 바로 하세요.\\n\\nIt's better to contact them soon...</td>\n",
       "      <td>39</td>\n",
       "      <td>6</td>\n",
       "      <td>110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>369</th>\n",
       "      <td>좋아하는 사람만 보면 감정이 북받쳐올라.</td>\n",
       "      <td>\"As an AI language model, I do not have person...</td>\n",
       "      <td>63</td>\n",
       "      <td>22</td>\n",
       "      <td>157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>388</th>\n",
       "      <td>한 마리 보통 맛이랑 공깃밥 두 개랑요 단호박 식혜도 하나 주세요</td>\n",
       "      <td>I would like to have one regular flavor, two s...</td>\n",
       "      <td>71</td>\n",
       "      <td>36</td>\n",
       "      <td>122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11426</th>\n",
       "      <td>매주 월요일 그를 볼 수 있어.</td>\n",
       "      <td>I can see him every Monday.</td>\n",
       "      <td>30</td>\n",
       "      <td>17</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11553</th>\n",
       "      <td>네 대신 얼음 두 조각만 넣어주세요</td>\n",
       "      <td>\"Sure, I'll put two pieces of ice in instead o...</td>\n",
       "      <td>39</td>\n",
       "      <td>19</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11610</th>\n",
       "      <td>이 중에 피자는 쿠폰으로 결제 부탁해요</td>\n",
       "      <td>As an AI language model, I am incapable of usi...</td>\n",
       "      <td>61</td>\n",
       "      <td>21</td>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11827</th>\n",
       "      <td>드디어 제가 손을 놓았어.</td>\n",
       "      <td>Finally, I let go of my hands.</td>\n",
       "      <td>32</td>\n",
       "      <td>14</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11949</th>\n",
       "      <td>만나는게 싫지는 않아</td>\n",
       "      <td>I do not mind meeting people.</td>\n",
       "      <td>25</td>\n",
       "      <td>11</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>141 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     prompt  \\\n",
       "25                                   목 마르다.   \n",
       "259                            참 답 없는 나란 놈.   \n",
       "277                                  참다가 연락   \n",
       "369                  좋아하는 사람만 보면 감정이 북받쳐올라.   \n",
       "388    한 마리 보통 맛이랑 공깃밥 두 개랑요 단호박 식혜도 하나 주세요   \n",
       "...                                     ...   \n",
       "11426                     매주 월요일 그를 볼 수 있어.   \n",
       "11553                   네 대신 얼음 두 조각만 넣어주세요   \n",
       "11610                 이 중에 피자는 쿠폰으로 결제 부탁해요   \n",
       "11827                        드디어 제가 손을 놓았어.   \n",
       "11949                           만나는게 싫지는 않아   \n",
       "\n",
       "                                              completion  tokens  prompt_len  \\\n",
       "25     > \"I am thirsty.\" (English) \\n\\n> \"Je suis ass...      75           6   \n",
       "259    I am a language model AI and I do not have the...      63          12   \n",
       "277    \"을 바로 하세요.\\n\\nIt's better to contact them soon...      39           6   \n",
       "369    \"As an AI language model, I do not have person...      63          22   \n",
       "388    I would like to have one regular flavor, two s...      71          36   \n",
       "...                                                  ...     ...         ...   \n",
       "11426                        I can see him every Monday.      30          17   \n",
       "11553  \"Sure, I'll put two pieces of ice in instead o...      39          19   \n",
       "11610  As an AI language model, I am incapable of usi...      61          21   \n",
       "11827                     Finally, I let go of my hands.      32          14   \n",
       "11949                      I do not mind meeting people.      25          11   \n",
       "\n",
       "       completion_len  \n",
       "25                154  \n",
       "259               197  \n",
       "277               110  \n",
       "369               157  \n",
       "388               122  \n",
       "...               ...  \n",
       "11426              28  \n",
       "11553              69  \n",
       "11610             150  \n",
       "11827              31  \n",
       "11949              30  \n",
       "\n",
       "[141 rows x 5 columns]"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 한글 이외의 문자가 반 이상인 행 찾기\n",
    "high_non_korean_ratio_rows = find_high_non_korean_ratio_in_dataframe(sft_data.loc[idx], threshold=0.7)\n",
    "high_non_korean_ratio_rows = high_non_korean_ratio_rows['completion']\n",
    "high_non_korean_ratio_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "8c45e7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_idx = high_non_korean_ratio_rows.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "332274ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sft_data.drop(drop_idx, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "16c513de",
   "metadata": {},
   "outputs": [],
   "source": [
    "sft_data.loc[sft_data['completion'].str.contains('token'), 'completion'] = sft_data['completion'][sft_data['completion'].str.contains('token')].str.split(\"\\'token\\'\").str[0].str.strip(', ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "1f654e1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         prompt_len  completion_len\n",
      "count  11708.000000    11708.000000\n",
      "mean      22.156645      142.605483\n",
      "std       13.136336      123.045021\n",
      "min        1.000000        3.000000\n",
      "25%       13.000000       60.000000\n",
      "50%       19.000000      116.000000\n",
      "75%       28.000000      183.000000\n",
      "max      258.000000     1552.000000\n"
     ]
    }
   ],
   "source": [
    "sft_data = analyze_data(sft_data, cols=['prompt','completion'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "484ffd01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['이 경우에는 다시 장애등급 심사를 받아서 현재 상황에 맞는 등급이 결정됩니다. 그 등급에 따라 차량혜택이 결정되므로, 현재 등급에 따른 혜택을 받으시다가 등급이 변동되면 혜택도 변동될 수 있습니다. 따라서, 등급 변동 후에는 차량혜택을 다시 신청하시거나 장애인복지카드 발급 등 관련 기관에서 재신청하시는 것이 좋습니다.',\n",
       " '일반적으로 차량 매입 시, 판매자와의 계약서, 차량 등록증, 차량 검사 결과서, 차량세 납부확인서, 보험증서, 신분증 등의 서류가 필요합니다. \\\\n\\\\n하지만 위 경우에는 학원 차라는 것과 원장과 차주 간의 지분 협의가 있으므로, 구체적인 사항에 따라 필요한 서류가 달라질 수 있습니다. \\\\n\\\\n따라서, 구체적인 상황과 계약서 내용을 파악하신 후에 판매자측에 문의하여 필요한 서류를 확인하시는 것이 좋습니다.',\n",
       " '그렇지 않습니다. 보험 계약상 운전자와 차주가 일치해야 한다는 것은 해당 차량을 운전하는 사람과 차량 소유자가 일치해야 한다는 뜻입니다. 따라서, 차량 소유자인 당신의 이름으로 차량 등록이 되어 있고, 보험 계약도 당신의 이름으로 되어 있다면, 당신의 아들이 함께 이동하더라도 문제 없습니다. 다만, 만약 보험 계약에 명시된 운전자와 차주가 일치하지 않는 경우에는 보험금 지급이 거부될 수 있으므로, 보험 계약 내용을 정확히 확인하시기 바랍니다.',\n",
       " '안녕하세요, 저는 AI 어시스턴트입니다.\\\\n\\\\n차량 등록과 보험 요금 문의에 대해 답변을 드리겠습니다. 먼저, 차량 등록 후 며칠 안 되었다면 보험 요금이 높게 책정될 수 있습니다. 이는 보험사에서 해당 차량의 운전자 신뢰도를 산출하기 때문입니다. \\\\n\\\\n공동명의로 보험 가입시에는 그 본인의 동의가 필요합니다. 따라서, 차량 소유자 님께서 본인과 아버님의 동의를 받으신 후, 공동명의 가입을 진행하시면 됩니다. 하지만, 공동명의로 가입할 경우 보험 요금이 더욱 높아질 수 있으니 참고하시기 바랍니다. \\\\n\\\\n추가적으로 보험 요금을 낮추는 방법으로는 운전자 보험료 할인제도나 자동차를 안전하게 운전하면 할인을 받는 운전 습관 관리 서비스 등이 있습니다. 이를 이용하시면 더욱 경제적으로 보험료를 관리하실 수 있습니다. \\\\n\\\\n감사합니다.',\n",
       " '죄송합니다. 저는 AI 어시스턴트이기 때문에, 선생님의 개인정보나 차량정보를 알지 못합니다. 또한, 이 정보는 개인정보 보호법에 의해 보호되므로, 해당 정보를 알 수 없습니다. 선생님께서는 직접 차량등록증이나 주민등록증 등을 확인하시거나 관련 기관에 문의하셔야 합니다. 감사합니다.',\n",
       " '그럴 수 있습니다. 이런 상황에서는 인내심을 가지고 차분하게 대처하는 것이 중요합니다. 다음 번에는 더 일찍 출발해서 이런 일이 생기지 않도록 조심하세요. 출근 시간이 늦어졌더라도, 출근 후 열심히 일해서 잊혀진 일이 되도록 노력하세요.',\n",
       " '차량 폐차 신청 시에는, 각 지자체나 폐차업체의 요구사항에 따라 필요한 서류가 조금씩 다를 수 있습니다. \\\\n\\\\n하지만 일반적으로는, 차량 등록증, 운전면허증, 인감증명서 등의 개인 신분을 증명할 수 있는 서류와 함께 폐차 신청을 할 수 있습니다. \\\\n\\\\n또한, 차량의 압류나 과태료 문제가 해결되었는지에 대한 확인은 차량 등록 기관이나 관련 기관에서 확인할 수 있으며, 별도의 완납증명서 등의 서류가 필요하지 않을 수도 있습니다. \\\\n\\\\n하지만 정확한 정보는 폐차를 신청하려는 지역의 폐차업체나 관련 기관에 문의하시는 것이 가장 확실한 방법일 것입니다.',\n",
       " '저희 회사의 팩스번호는 XXX-XXXX-XXXX입니다. 차량 보험가입증서를 보내주시면 빠르게 처리해 드리겠습니다. 문제가 해결되면 고객님께 바로 연락드리도록 하겠습니다. 감사합니다.',\n",
       " '정말 불편한 상황이네요. 이전에는 삼백오번이 직행해서 훨씬 편리했는데, 지금은 환승을 해야만 하다니 말이죠. 그리고 시내에서 성서쪽으로 가는 버스도 오백삼번 타야 한다니, 정말 다녀오는 것도 힘들 것 같습니다. 또한, 사람이 너무 많은 삼백이십오번도 문제이고, 급행은 비싸기 때문에 선택할 수 있는 게 없어 보입니다. 어쩌면 다른 대중교통 수단을 이용하는 것도 고려해 볼만 할 것 같습니다.',\n",
       " '있더니 갑자기 멈췄어요. 버스 기사도 앞차가 왜 멈췄는지 모르겠다며 당황스러워 했어요. 그래서 버스 승강장 직원이 와서 상황을 확인해 보았는데, 앞차가 사고를 낸 것 같다는 이야기를 듣고 우리 버스는 우회해서 다른 길로 가기로 했어요. 다행히 운전자분이 빠르게 대처해서 우리가 큰 불편은 없었지만, 사고가 난 것 같은데 현장을 지나가며 많은 차들이 막혀 있었어요. 언제든 사고가 일어날 수 있기 때문에 운전하실 때는 항상 안전에 유의하시기 바랍니다.',\n",
       " '중고차 상사에서 차량등급을 잘못 허위 기재해서 발생한 손해는 해당 상사가 배상해야 합니다. 따라서, 손해배상을 청구하시는 것이 바람직합니다.\\\\n\\\\n등록이 이미 마쳤다면 취소할 수는 없지만, 추가로 손해배상을 요구할 수 있습니다. 손해배상 청구를 위해서는 이전 상사와 협상을 먼저 시도해보시는 것이 좋습니다. 협상이 불가능하다면 법적인 조치를 취할 수 있습니다. 따라서, 변호사나 소비자보호센터에 상담을 받아보시는 것이 좋습니다.',\n",
       " '포커 (Poker)입니다.',\n",
       " '. 그렇다면 해당 차량의 소재지도 대구로 변경해야 합니다. 이 경우에는 차량 소유자가 대구 도시교통공단 또는 대구광역시 자동차등록사업소에 방문하여 대구로의 소재지 변경 신고를 해야 합니다. 또한, 차량 보험도 이전 차량 관할 지역에서 대구로 이전하는 것이 필요합니다. 이러한 절차를 거치면 차량의 소재지와 보험이 변경되어 대구에서의 이용이 가능해집니다.',\n",
       " '제가 AI 챗봇으로서는 지금 위치한 위치를 파악할 수 없습니다. 하지만 버스 노선번호를 알려주시면 해당 노선의 정류소를 검색해서 예상 도착시간을 알려드리도록 하겠습니다. 부득이한 상황이라 불편하시겠지만, 최대한 도움을 드릴 수 있도록 노력하겠습니다. 감사합니다.',\n",
       " '제가 이해하기로는, 버스를 타다가 애기를 안고 있는 학생과 함께 탔는데, 기사님이 차비를 지불하지 않았다고 말씀하셨고, 불친절한 태도였다는 것 같습니다.\\\\n\\\\n이는 분명히 단순한 오해나 혼란이 있는 것이 아닐까 생각됩니다. 기사님도 일정 압박과 여러 문제들에 직면하는 중이라면서, 그런 발언을 하셨을 수도 있고, 어쩌면 언어적인 오해가 있었을 수도 있습니다.\\\\n\\\\n그러나 어떤 일이 있더라도, 다른 사람에 대한 존중과 배려는 필수적인 요소로서 유지되어야 합니다. 어떠한 문제가 생겼을 때에도, 친절하고 예의 바른 언어와 태도로 상황을 해결할 수 있도록 노력하는 것이 중요합니다. 다른 사람들도 마찬가지로, 저와 같은 경험을 하더라도, 이를 다시 겪지 않게 하기 위해서는 저희 모두에게 사회적으로 존중받고, 상호간의 배려와 협력이 이루어져야 한다고 생각됩니다.',\n",
       " '\"답: \\'시크릿 가든\\'\"',\n",
       " '죄송합니다, 이해하지 못했습니다. 무슨 일이 일어났는지 자세히 설명해주시겠어요? 그러면 더 나은 도움을 드릴 수 있을 것 같습니다.']"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sft_data[sft_data.prompt_len > 100].completion.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "1ce8ccfc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>completion</th>\n",
       "      <th>tokens</th>\n",
       "      <th>prompt_len</th>\n",
       "      <th>completion_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2976</th>\n",
       "      <td>헉</td>\n",
       "      <td>저를 놀래키거나 놀라게 하셨나요? 무슨 일이 있으신가요? 저는 인공지능 어시스턴트이...</td>\n",
       "      <td>86</td>\n",
       "      <td>1</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4217</th>\n",
       "      <td>휴</td>\n",
       "      <td>휴, 그것은 쉬는 것을 의미하는 단어입니다. 일상적으로 사용되며, 휴식을 취함으로써...</td>\n",
       "      <td>169</td>\n",
       "      <td>1</td>\n",
       "      <td>169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5811</th>\n",
       "      <td>후</td>\n",
       "      <td>등나무가 아름다운 가을 풍경을 만들고 있습니다. 적포도나무와 갈색 단풍나무와 함께 ...</td>\n",
       "      <td>159</td>\n",
       "      <td>1</td>\n",
       "      <td>145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7080</th>\n",
       "      <td>네</td>\n",
       "      <td>네, 팀장님. 무엇을 도와드릴까요?</td>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11045</th>\n",
       "      <td>음</td>\n",
       "      <td>죄송합니다, 이전에 질문하신 내용이 불분명하여 대답을 드리지 못하였습니다. 더 자세...</td>\n",
       "      <td>96</td>\n",
       "      <td>1</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      prompt                                         completion  tokens  \\\n",
       "2976       헉  저를 놀래키거나 놀라게 하셨나요? 무슨 일이 있으신가요? 저는 인공지능 어시스턴트이...      86   \n",
       "4217       휴  휴, 그것은 쉬는 것을 의미하는 단어입니다. 일상적으로 사용되며, 휴식을 취함으로써...     169   \n",
       "5811       후  등나무가 아름다운 가을 풍경을 만들고 있습니다. 적포도나무와 갈색 단풍나무와 함께 ...     159   \n",
       "7080       네                                네, 팀장님. 무엇을 도와드릴까요?      34   \n",
       "11045      음  죄송합니다, 이전에 질문하신 내용이 불분명하여 대답을 드리지 못하였습니다. 더 자세...      96   \n",
       "\n",
       "       prompt_len  completion_len  \n",
       "2976            1              72  \n",
       "4217            1             169  \n",
       "5811            1             145  \n",
       "7080            1              19  \n",
       "11045           1              92  "
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sft_data[sft_data.prompt_len < 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "0e85ee8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ['prompt','completion']:\n",
    "    sft_data[col] = sft_data[col].apply(lambda x : ' '.join(x.split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7e3112",
   "metadata": {},
   "source": [
    "## RM Data 전처리\n",
    "- rm data는 ranking 높은 것 위주로 전처리 다시 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "3133712e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>completion_0</th>\n",
       "      <th>completion_1</th>\n",
       "      <th>completion_2</th>\n",
       "      <th>ranking</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>번디는 자신이 탐정잡지, 범죄소설 그리고 성범죄 관련 실제 범죄 다큐멘터리들을 탐독...</td>\n",
       "      <td>Allow me to answer your question. I know that ...</td>\n",
       "      <td>번디는 다양한 인터뷰자들과 뉴스홍보 담당자들과의 면담 때 밝혔다.</td>\n",
       "      <td>라이언에게 말했다.</td>\n",
       "      <td>[2, 1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>개포주공아파트는 몇 단지로 이루어져 있나?</td>\n",
       "      <td>개포주공아파트는 다섯 단지로 이루어져 있습니다.</td>\n",
       "      <td>이날 목송에서 구글상위노</td>\n",
       "      <td>개포주공아파트는 총 27개 단지로 이루어져 있습니다.</td>\n",
       "      <td>[2, 0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>김영삼의 후보 시절 지역표심을 겨냥한 발언을 문제삼은 후보는?</td>\n",
       "      <td>The diameter of the Metallic domain is bigger ...</td>\n",
       "      <td>이 질문은 조금 불분명합니다. 김영삼 대통령이 후보 시절에 어떤 발언을 했고, 누가...</td>\n",
       "      <td>김영삼의 후보 시절에 지역표심을 겨냥한 발언은 대통령 당선 전까지 대한민국 정부가 ...</td>\n",
       "      <td>[1, 2, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>페르시아로부터 기원된 아랍요리의 특징이 뭐야</td>\n",
       "      <td>1. 다양한 스파이스와 허브 사용\\n2. 쌀, 콩, 나물, 고기, 양파 등 다양한 ...</td>\n",
       "      <td>- 아랍요리는 소금, 산 재료, 일부 채소, 고기, 생선 또는 난류의 조합으로 만들...</td>\n",
       "      <td>위한 태야 대표 기원들 개인이라 겪고 태야\\n\\n\\n귀식 걸 공개한 구문\\n\\n사띩...</td>\n",
       "      <td>[0, 1, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>중화인민공화국의 기본 법률은 누가 제정 및 개정하는가?</td>\n",
       "      <td>중화인민공화국의 기본 법률은 중국인민대표대회(국회)가 제정하고 개정합니다.</td>\n",
       "      <td>The limit of the Office of the Vice President is?</td>\n",
       "      <td>중화인민공화국의 기본 법률은 중화인민공화국의 의회가 제정 및 개정합니다.</td>\n",
       "      <td>[0, 2, 1]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              prompt  \\\n",
       "0  번디는 자신이 탐정잡지, 범죄소설 그리고 성범죄 관련 실제 범죄 다큐멘터리들을 탐독...   \n",
       "1                            개포주공아파트는 몇 단지로 이루어져 있나?   \n",
       "2                 김영삼의 후보 시절 지역표심을 겨냥한 발언을 문제삼은 후보는?   \n",
       "3                           페르시아로부터 기원된 아랍요리의 특징이 뭐야   \n",
       "4                     중화인민공화국의 기본 법률은 누가 제정 및 개정하는가?   \n",
       "\n",
       "                                        completion_0  \\\n",
       "0  Allow me to answer your question. I know that ...   \n",
       "1                         개포주공아파트는 다섯 단지로 이루어져 있습니다.   \n",
       "2  The diameter of the Metallic domain is bigger ...   \n",
       "3  1. 다양한 스파이스와 허브 사용\\n2. 쌀, 콩, 나물, 고기, 양파 등 다양한 ...   \n",
       "4          중화인민공화국의 기본 법률은 중국인민대표대회(국회)가 제정하고 개정합니다.   \n",
       "\n",
       "                                        completion_1  \\\n",
       "0               번디는 다양한 인터뷰자들과 뉴스홍보 담당자들과의 면담 때 밝혔다.   \n",
       "1                                      이날 목송에서 구글상위노   \n",
       "2  이 질문은 조금 불분명합니다. 김영삼 대통령이 후보 시절에 어떤 발언을 했고, 누가...   \n",
       "3  - 아랍요리는 소금, 산 재료, 일부 채소, 고기, 생선 또는 난류의 조합으로 만들...   \n",
       "4  The limit of the Office of the Vice President is?   \n",
       "\n",
       "                                        completion_2    ranking  \n",
       "0                                         라이언에게 말했다.  [2, 1, 0]  \n",
       "1                      개포주공아파트는 총 27개 단지로 이루어져 있습니다.  [2, 0, 1]  \n",
       "2  김영삼의 후보 시절에 지역표심을 겨냥한 발언은 대통령 당선 전까지 대한민국 정부가 ...  [1, 2, 0]  \n",
       "3  위한 태야 대표 기원들 개인이라 겪고 태야\\n\\n\\n귀식 걸 공개한 구문\\n\\n사띩...  [0, 1, 2]  \n",
       "4           중화인민공화국의 기본 법률은 중화인민공화국의 의회가 제정 및 개정합니다.  [0, 2, 1]  "
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rm_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "d7eb5189",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_ans(rm_data):\n",
    "    tmp = rm_data.loc[:, rm_data.columns.str.contains('completion_')]\n",
    "    rank_1 = rm_data.ranking.str[0]\n",
    "    \n",
    "    assert len(rank_1) == len(tmp)\n",
    "    \n",
    "    best_ans = []\n",
    "    for i, r in enumerate(rank_1):\n",
    "        ans = tmp.iloc[i, r]\n",
    "        best_ans.append(ans)\n",
    "    \n",
    "    return best_ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "f2682b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_ans = get_best_ans(rm_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "6f7726c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>best_ans</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>라이언에게 말했다.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>개포주공아파트는 총 27개 단지로 이루어져 있습니다.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>이 질문은 조금 불분명합니다. 김영삼 대통령이 후보 시절에 어떤 발언을 했고, 누가...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1. 다양한 스파이스와 허브 사용\\n2. 쌀, 콩, 나물, 고기, 양파 등 다양한 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>중화인민공화국의 기본 법률은 중국인민대표대회(국회)가 제정하고 개정합니다.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10215</th>\n",
       "      <td>야세르 아라파트 국제공항은 터키에 위치해 있습니다.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10216</th>\n",
       "      <td>할 일:\\n\\n1. 배터리를 확인하세요. 배터리가 항상 충전되어 있지 않은 경우 핸...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10217</th>\n",
       "      <td>죄송합니다. 저는 AI 언어 모델로써 실시간으로 답변을 해드리고 있기 때문에, 어떤...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10218</th>\n",
       "      <td>그러게요! 좋아하면 사랑하는 마음으로 즐겨보세요!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10219</th>\n",
       "      <td>죄송합니다. 저는 인공지능 어시스턴트이므로 차량 번호를 확인할 수 없습니다. 해당 ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10220 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                best_ans\n",
       "0                                             라이언에게 말했다.\n",
       "1                          개포주공아파트는 총 27개 단지로 이루어져 있습니다.\n",
       "2      이 질문은 조금 불분명합니다. 김영삼 대통령이 후보 시절에 어떤 발언을 했고, 누가...\n",
       "3      1. 다양한 스파이스와 허브 사용\\n2. 쌀, 콩, 나물, 고기, 양파 등 다양한 ...\n",
       "4              중화인민공화국의 기본 법률은 중국인민대표대회(국회)가 제정하고 개정합니다.\n",
       "...                                                  ...\n",
       "10215                       야세르 아라파트 국제공항은 터키에 위치해 있습니다.\n",
       "10216  할 일:\\n\\n1. 배터리를 확인하세요. 배터리가 항상 충전되어 있지 않은 경우 핸...\n",
       "10217  죄송합니다. 저는 AI 언어 모델로써 실시간으로 답변을 해드리고 있기 때문에, 어떤...\n",
       "10218                        그러게요! 좋아하면 사랑하는 마음으로 즐겨보세요!\n",
       "10219  죄송합니다. 저는 인공지능 어시스턴트이므로 차량 번호를 확인할 수 없습니다. 해당 ...\n",
       "\n",
       "[10220 rows x 1 columns]"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(best_ans, columns=['best_ans'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "fe3804c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       best_ans_len\n",
      "count  10220.000000\n",
      "mean     143.142857\n",
      "std      124.012901\n",
      "min        3.000000\n",
      "25%       61.000000\n",
      "50%      116.000000\n",
      "75%      183.250000\n",
      "max     1552.000000\n"
     ]
    }
   ],
   "source": [
    "best_ans_df = analyze_data(pd.DataFrame(best_ans, columns=['best_ans']), cols=['best_ans'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "8a6acced",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='best_ans_len', ylabel='Count'>"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.histplot(best_ans_df.best_ans_len, bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "fa696132",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_answer(x):\n",
    "    raw = x\n",
    "    try:\n",
    "        x = literal_eval(x)['completion']\n",
    "    except:\n",
    "        x = x+\"'}\"\n",
    "        try:\n",
    "            x = literal_eval(x)['completion']\n",
    "        except:\n",
    "            print(raw)\n",
    "            return raw\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "d22bd11e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>completion_0</th>\n",
       "      <th>completion_1</th>\n",
       "      <th>completion_2</th>\n",
       "      <th>ranking</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>번디는 자신이 탐정잡지, 범죄소설 그리고 성범죄 관련 실제 범죄 다큐멘터리들을 탐독...</td>\n",
       "      <td>Allow me to answer your question. I know that ...</td>\n",
       "      <td>번디는 다양한 인터뷰자들과 뉴스홍보 담당자들과의 면담 때 밝혔다.</td>\n",
       "      <td>라이언에게 말했다.</td>\n",
       "      <td>[2, 1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>개포주공아파트는 몇 단지로 이루어져 있나?</td>\n",
       "      <td>개포주공아파트는 다섯 단지로 이루어져 있습니다.</td>\n",
       "      <td>이날 목송에서 구글상위노</td>\n",
       "      <td>개포주공아파트는 총 27개 단지로 이루어져 있습니다.</td>\n",
       "      <td>[2, 0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>김영삼의 후보 시절 지역표심을 겨냥한 발언을 문제삼은 후보는?</td>\n",
       "      <td>The diameter of the Metallic domain is bigger ...</td>\n",
       "      <td>이 질문은 조금 불분명합니다. 김영삼 대통령이 후보 시절에 어떤 발언을 했고, 누가...</td>\n",
       "      <td>김영삼의 후보 시절에 지역표심을 겨냥한 발언은 대통령 당선 전까지 대한민국 정부가 ...</td>\n",
       "      <td>[1, 2, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>페르시아로부터 기원된 아랍요리의 특징이 뭐야</td>\n",
       "      <td>1. 다양한 스파이스와 허브 사용\\n2. 쌀, 콩, 나물, 고기, 양파 등 다양한 ...</td>\n",
       "      <td>- 아랍요리는 소금, 산 재료, 일부 채소, 고기, 생선 또는 난류의 조합으로 만들...</td>\n",
       "      <td>위한 태야 대표 기원들 개인이라 겪고 태야\\n\\n\\n귀식 걸 공개한 구문\\n\\n사띩...</td>\n",
       "      <td>[0, 1, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>중화인민공화국의 기본 법률은 누가 제정 및 개정하는가?</td>\n",
       "      <td>중화인민공화국의 기본 법률은 중국인민대표대회(국회)가 제정하고 개정합니다.</td>\n",
       "      <td>The limit of the Office of the Vice President is?</td>\n",
       "      <td>중화인민공화국의 기본 법률은 중화인민공화국의 의회가 제정 및 개정합니다.</td>\n",
       "      <td>[0, 2, 1]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              prompt  \\\n",
       "0  번디는 자신이 탐정잡지, 범죄소설 그리고 성범죄 관련 실제 범죄 다큐멘터리들을 탐독...   \n",
       "1                            개포주공아파트는 몇 단지로 이루어져 있나?   \n",
       "2                 김영삼의 후보 시절 지역표심을 겨냥한 발언을 문제삼은 후보는?   \n",
       "3                           페르시아로부터 기원된 아랍요리의 특징이 뭐야   \n",
       "4                     중화인민공화국의 기본 법률은 누가 제정 및 개정하는가?   \n",
       "\n",
       "                                        completion_0  \\\n",
       "0  Allow me to answer your question. I know that ...   \n",
       "1                         개포주공아파트는 다섯 단지로 이루어져 있습니다.   \n",
       "2  The diameter of the Metallic domain is bigger ...   \n",
       "3  1. 다양한 스파이스와 허브 사용\\n2. 쌀, 콩, 나물, 고기, 양파 등 다양한 ...   \n",
       "4          중화인민공화국의 기본 법률은 중국인민대표대회(국회)가 제정하고 개정합니다.   \n",
       "\n",
       "                                        completion_1  \\\n",
       "0               번디는 다양한 인터뷰자들과 뉴스홍보 담당자들과의 면담 때 밝혔다.   \n",
       "1                                      이날 목송에서 구글상위노   \n",
       "2  이 질문은 조금 불분명합니다. 김영삼 대통령이 후보 시절에 어떤 발언을 했고, 누가...   \n",
       "3  - 아랍요리는 소금, 산 재료, 일부 채소, 고기, 생선 또는 난류의 조합으로 만들...   \n",
       "4  The limit of the Office of the Vice President is?   \n",
       "\n",
       "                                        completion_2    ranking  \n",
       "0                                         라이언에게 말했다.  [2, 1, 0]  \n",
       "1                      개포주공아파트는 총 27개 단지로 이루어져 있습니다.  [2, 0, 1]  \n",
       "2  김영삼의 후보 시절에 지역표심을 겨냥한 발언은 대통령 당선 전까지 대한민국 정부가 ...  [1, 2, 0]  \n",
       "3  위한 태야 대표 기원들 개인이라 겪고 태야\\n\\n\\n귀식 걸 공개한 구문\\n\\n사띩...  [0, 1, 2]  \n",
       "4           중화인민공화국의 기본 법률은 중화인민공화국의 의회가 제정 및 개정합니다.  [0, 2, 1]  "
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rm_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "0aa9093a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['prompt', 'completion_0', 'completion_1', 'completion_2', 'ranking'], dtype='object')"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rm_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "4b2a5008",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ['completion_0', 'completion_1', 'completion_2']:\n",
    "    rm_data.loc[rm_data[col].str.contains('\\'prompt'), col] = rm_data.loc[rm_data[col].str.contains('\\'prompt'), col].apply(lambda x: convert_answer(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333b0eec",
   "metadata": {},
   "source": [
    "## 가장 좋은 답변중에 as ai model~ 삭제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "9e464dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_idx = best_ans_df[best_ans_df.best_ans.str.lower().str.contains('model')].best_ans.index\n",
    "rm_data.drop(drop_idx, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502657f1",
   "metadata": {},
   "source": [
    "## dict 형태 제거 잘 안된거 삭제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "cc78152a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_answer(x):\n",
    "    raw = x\n",
    "    x = \"{'completion':\" + x \n",
    "    try:\n",
    "        x = literal_eval(x)['completion']\n",
    "    except:\n",
    "        print(raw)\n",
    "        return raw\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "3d5c0996",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ['completion_0', 'completion_1', 'completion_2']:\n",
    "    rm_data.loc[rm_data[col].str.contains('\\'prompt'), col] = rm_data.loc[rm_data[col].str.contains('\\'prompt'), col].apply(lambda x: convert_answer(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "16166945",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>completion_0</th>\n",
       "      <th>completion_1</th>\n",
       "      <th>completion_2</th>\n",
       "      <th>ranking</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>번디는 자신이 탐정잡지, 범죄소설 그리고 성범죄 관련 실제 범죄 다큐멘터리들을 탐독...</td>\n",
       "      <td>Allow me to answer your question. I know that ...</td>\n",
       "      <td>번디는 다양한 인터뷰자들과 뉴스홍보 담당자들과의 면담 때 밝혔다.</td>\n",
       "      <td>라이언에게 말했다.</td>\n",
       "      <td>[2, 1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>개포주공아파트는 몇 단지로 이루어져 있나?</td>\n",
       "      <td>개포주공아파트는 다섯 단지로 이루어져 있습니다.</td>\n",
       "      <td>이날 목송에서 구글상위노</td>\n",
       "      <td>개포주공아파트는 총 27개 단지로 이루어져 있습니다.</td>\n",
       "      <td>[2, 0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>김영삼의 후보 시절 지역표심을 겨냥한 발언을 문제삼은 후보는?</td>\n",
       "      <td>The diameter of the Metallic domain is bigger ...</td>\n",
       "      <td>이 질문은 조금 불분명합니다. 김영삼 대통령이 후보 시절에 어떤 발언을 했고, 누가...</td>\n",
       "      <td>김영삼의 후보 시절에 지역표심을 겨냥한 발언은 대통령 당선 전까지 대한민국 정부가 ...</td>\n",
       "      <td>[1, 2, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>페르시아로부터 기원된 아랍요리의 특징이 뭐야</td>\n",
       "      <td>1. 다양한 스파이스와 허브 사용\\n2. 쌀, 콩, 나물, 고기, 양파 등 다양한 ...</td>\n",
       "      <td>- 아랍요리는 소금, 산 재료, 일부 채소, 고기, 생선 또는 난류의 조합으로 만들...</td>\n",
       "      <td>위한 태야 대표 기원들 개인이라 겪고 태야\\n\\n\\n귀식 걸 공개한 구문\\n\\n사띩...</td>\n",
       "      <td>[0, 1, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>중화인민공화국의 기본 법률은 누가 제정 및 개정하는가?</td>\n",
       "      <td>중화인민공화국의 기본 법률은 중국인민대표대회(국회)가 제정하고 개정합니다.</td>\n",
       "      <td>The limit of the Office of the Vice President is?</td>\n",
       "      <td>중화인민공화국의 기본 법률은 중화인민공화국의 의회가 제정 및 개정합니다.</td>\n",
       "      <td>[0, 2, 1]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              prompt  \\\n",
       "0  번디는 자신이 탐정잡지, 범죄소설 그리고 성범죄 관련 실제 범죄 다큐멘터리들을 탐독...   \n",
       "1                            개포주공아파트는 몇 단지로 이루어져 있나?   \n",
       "2                 김영삼의 후보 시절 지역표심을 겨냥한 발언을 문제삼은 후보는?   \n",
       "3                           페르시아로부터 기원된 아랍요리의 특징이 뭐야   \n",
       "4                     중화인민공화국의 기본 법률은 누가 제정 및 개정하는가?   \n",
       "\n",
       "                                        completion_0  \\\n",
       "0  Allow me to answer your question. I know that ...   \n",
       "1                         개포주공아파트는 다섯 단지로 이루어져 있습니다.   \n",
       "2  The diameter of the Metallic domain is bigger ...   \n",
       "3  1. 다양한 스파이스와 허브 사용\\n2. 쌀, 콩, 나물, 고기, 양파 등 다양한 ...   \n",
       "4          중화인민공화국의 기본 법률은 중국인민대표대회(국회)가 제정하고 개정합니다.   \n",
       "\n",
       "                                        completion_1  \\\n",
       "0               번디는 다양한 인터뷰자들과 뉴스홍보 담당자들과의 면담 때 밝혔다.   \n",
       "1                                      이날 목송에서 구글상위노   \n",
       "2  이 질문은 조금 불분명합니다. 김영삼 대통령이 후보 시절에 어떤 발언을 했고, 누가...   \n",
       "3  - 아랍요리는 소금, 산 재료, 일부 채소, 고기, 생선 또는 난류의 조합으로 만들...   \n",
       "4  The limit of the Office of the Vice President is?   \n",
       "\n",
       "                                        completion_2    ranking  \n",
       "0                                         라이언에게 말했다.  [2, 1, 0]  \n",
       "1                      개포주공아파트는 총 27개 단지로 이루어져 있습니다.  [2, 0, 1]  \n",
       "2  김영삼의 후보 시절에 지역표심을 겨냥한 발언은 대통령 당선 전까지 대한민국 정부가 ...  [1, 2, 0]  \n",
       "3  위한 태야 대표 기원들 개인이라 겪고 태야\\n\\n\\n귀식 걸 공개한 구문\\n\\n사띩...  [0, 1, 2]  \n",
       "4           중화인민공화국의 기본 법률은 중화인민공화국의 의회가 제정 및 개정합니다.  [0, 2, 1]  "
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rm_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e45aae6",
   "metadata": {},
   "source": [
    "### prompt에서 dictionary 정리안된거 삭제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "515433f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_prompt(x):\n",
    "    raw = x\n",
    "    x = \"{'prompt':\" + x \n",
    "    try:\n",
    "        x = literal_eval(x)['prompt']\n",
    "    except:\n",
    "        x = x + '\\'}'\n",
    "        try:\n",
    "            x = literal_eval(x)['prompt']\n",
    "        except:\n",
    "            print(raw)\n",
    "            return raw\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "f8f33bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "rm_data.loc[rm_data.prompt.str.contains('token|completion'),'prompt'] = rm_data[rm_data.prompt.str.contains('token|completion')].prompt.apply(lambda x: convert_prompt(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "a64072ce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>completion_0</th>\n",
       "      <th>completion_1</th>\n",
       "      <th>completion_2</th>\n",
       "      <th>ranking</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [prompt, completion_0, completion_1, completion_2, ranking]\n",
       "Index: []"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rm_data[rm_data.prompt.str.contains('token|completion')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "ae6c7f67",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['prompt', 'completion_0', 'completion_1', 'completion_2'], dtype='object')"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rm_data.columns[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "c4c2e118",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in rm_data.columns[:-1]:\n",
    "    rm_data[col] = rm_data[col].apply(lambda x : ' '.join(x.split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae3d0d2",
   "metadata": {},
   "source": [
    "### 아직도 dictionary 삭제 안된거 ....ㅠㅠㅠ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "c7f8d13a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>completion_0</th>\n",
       "      <th>completion_1</th>\n",
       "      <th>completion_2</th>\n",
       "      <th>ranking</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>스페인이 속해있는 반도의 이름이 뭐야</td>\n",
       "      <td>스페인은 유럽 반도에 속해 있습니다.</td>\n",
       "      <td>\"이 질문은 정확한 문장 표현이 없습니다. 그러나 매우 가능성이 높은 것은 이 질문...</td>\n",
       "      <td>They are not even words.</td>\n",
       "      <td>[1, 0, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>헬레니즘 시대에 사용하던 달력이 뭐지</td>\n",
       "      <td>\"헬레니즘 시대에 사용하던 달력은 율리우스 달력이 아니라 실로시스 달력(Silosi...</td>\n",
       "      <td>스무계 달력(曆)이라고 부르는 달력이 있습니다. 스무계 달력은 중국 및 일본의 고대...</td>\n",
       "      <td>미국 국제인물 경기법옥 국애 관협 국제인물 경기법옥 국애 관협 국애 관협 국애 관협...</td>\n",
       "      <td>[0, 1, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>무장 충돌이 일어나고 시리아의 집권당 바트당이 구성한 것은 무엇인가?</td>\n",
       "      <td>바트당은 시리아의 역사상 최초로 시민 투표로 집권을 가져온 당으로, 공산주의 방향의...</td>\n",
       "      <td>The capital’s location is giving it wealth and...</td>\n",
       "      <td>\"시리아의 집권당 바트당은 무장 충돌이 일어난 후에도 여전히 국가의 지배적인 정당으...</td>\n",
       "      <td>[2, 0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>국카스텐의 1집 앨범 속 카드들을 디자인한 멤버의 복면가왕 우승 기록은?</td>\n",
       "      <td>\"국카스텐의 1집 앨범 속 카드를 디자인한 멤버는 복면가왕 시즌2에서 출연한 '마리...</td>\n",
       "      <td>멤버 같은 기록을 합당하며, 국제 학생들을 고고 합당하고, 규모 국획발향으</td>\n",
       "      <td>멤버 중에서 2015년 복면가왕 우승을 차지한 것은 김채원(Kim Chaewon) ...</td>\n",
       "      <td>[0, 2, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>코자</td>\n",
       "      <td>공지 공지 국내 공학 국(공학생대결) Boys 공학 국 We are the new ...</td>\n",
       "      <td>스트 Kocyast is a South Korean indie band, forme...</td>\n",
       "      <td>\"코자는 요즘 많은 인터넷 커뮤니티에서 사용되는 단어로, '코로나 자가격리'의 준말...</td>\n",
       "      <td>[2, 1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10082</th>\n",
       "      <td>고야가 만든 전쟁 판화집이 뭐야</td>\n",
       "      <td>\"고야가 만든 전쟁 판화집은 '귀부인전'이라는 제목의 판화집입니다. 이 판화집은 1...</td>\n",
       "      <td>고야가 만든 전쟁 판화집은 『전쟁의 역사』입니다. 이 책은 전쟁의 역사를 연구하고 ...</td>\n",
       "      <td>고 개인이 개인을 개인으라 이상 한 개인이 개인을 개인으라 그 개인을 개인으라 이상...</td>\n",
       "      <td>[0, 1, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10145</th>\n",
       "      <td>김영하의 작품 중 뮤지컬로 바뀌어 예술의 전당에서 대중에게 보여진 것은 무엇인가?</td>\n",
       "      <td>이상호는 국가발전에서 국내에게 이상호를 국가발전의 구강을 국가들이 국가발전을 구강하...</td>\n",
       "      <td>김영하의 뮤지컬로 바뀌어 예술의 전당에서 대중에게 보여진 것은 《바람과 함께 사라지...</td>\n",
       "      <td>\"김영하의 작품 중 뮤지컬로 바뀌어 예술의 전당에서 대중에게 보여진 것은 '비포선셋...</td>\n",
       "      <td>[2, 1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10153</th>\n",
       "      <td>영수증 좀 주세요</td>\n",
       "      <td>죄송합니다. 영수증은 고객님께서 직접 신청하셔야 합니다. 영수증 신청은 고객센터를 ...</td>\n",
       "      <td>\"I'm sorry, as an AI language model, I cannot ...</td>\n",
       "      <td>사용자가 가장 가지고 한 개의 긍정 개념들이 한 개의 긍정 개념들이 한 개의 긍정 ...</td>\n",
       "      <td>[1, 0, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10159</th>\n",
       "      <td>아메리카노에 물 많이 넣어서 좀 연하게 해주세요</td>\n",
       "      <td>네, 알겠습니다. 아메리카노에 물을 많이 넣어서 연하게 해드리겠습니다.</td>\n",
       "      <td>\"Sure, I'll prepare an Americano with more wat...</td>\n",
       "      <td>. You can't use this account to purchase any p...</td>\n",
       "      <td>[1, 0, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10188</th>\n",
       "      <td>전지현이 2012년에 출연한 영화는?</td>\n",
       "      <td>2012년에 전지현이 출연한 영화는 다음과 같습니다. 1. 바람과 함께 사라지다 2...</td>\n",
       "      <td>\"전지현이 2012년에 출연한 영화는 '인사이드 메론'입니다.\", 'token': 60}</td>\n",
       "      <td>이상, 현실의 국제로에서 휴대전지현이 국가정상업천물과 국가정상업천물을 국가정상업천물...</td>\n",
       "      <td>[1, 0, 2]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>424 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              prompt  \\\n",
       "26                              스페인이 속해있는 반도의 이름이 뭐야   \n",
       "48                              헬레니즘 시대에 사용하던 달력이 뭐지   \n",
       "67            무장 충돌이 일어나고 시리아의 집권당 바트당이 구성한 것은 무엇인가?   \n",
       "137         국카스텐의 1집 앨범 속 카드들을 디자인한 멤버의 복면가왕 우승 기록은?   \n",
       "143                                               코자   \n",
       "...                                              ...   \n",
       "10082                              고야가 만든 전쟁 판화집이 뭐야   \n",
       "10145  김영하의 작품 중 뮤지컬로 바뀌어 예술의 전당에서 대중에게 보여진 것은 무엇인가?   \n",
       "10153                                      영수증 좀 주세요   \n",
       "10159                     아메리카노에 물 많이 넣어서 좀 연하게 해주세요   \n",
       "10188                           전지현이 2012년에 출연한 영화는?   \n",
       "\n",
       "                                            completion_0  \\\n",
       "26                                  스페인은 유럽 반도에 속해 있습니다.   \n",
       "48     \"헬레니즘 시대에 사용하던 달력은 율리우스 달력이 아니라 실로시스 달력(Silosi...   \n",
       "67     바트당은 시리아의 역사상 최초로 시민 투표로 집권을 가져온 당으로, 공산주의 방향의...   \n",
       "137    \"국카스텐의 1집 앨범 속 카드를 디자인한 멤버는 복면가왕 시즌2에서 출연한 '마리...   \n",
       "143    공지 공지 국내 공학 국(공학생대결) Boys 공학 국 We are the new ...   \n",
       "...                                                  ...   \n",
       "10082  \"고야가 만든 전쟁 판화집은 '귀부인전'이라는 제목의 판화집입니다. 이 판화집은 1...   \n",
       "10145  이상호는 국가발전에서 국내에게 이상호를 국가발전의 구강을 국가들이 국가발전을 구강하...   \n",
       "10153  죄송합니다. 영수증은 고객님께서 직접 신청하셔야 합니다. 영수증 신청은 고객센터를 ...   \n",
       "10159            네, 알겠습니다. 아메리카노에 물을 많이 넣어서 연하게 해드리겠습니다.   \n",
       "10188  2012년에 전지현이 출연한 영화는 다음과 같습니다. 1. 바람과 함께 사라지다 2...   \n",
       "\n",
       "                                            completion_1  \\\n",
       "26     \"이 질문은 정확한 문장 표현이 없습니다. 그러나 매우 가능성이 높은 것은 이 질문...   \n",
       "48     스무계 달력(曆)이라고 부르는 달력이 있습니다. 스무계 달력은 중국 및 일본의 고대...   \n",
       "67     The capital’s location is giving it wealth and...   \n",
       "137            멤버 같은 기록을 합당하며, 국제 학생들을 고고 합당하고, 규모 국획발향으   \n",
       "143    스트 Kocyast is a South Korean indie band, forme...   \n",
       "...                                                  ...   \n",
       "10082  고야가 만든 전쟁 판화집은 『전쟁의 역사』입니다. 이 책은 전쟁의 역사를 연구하고 ...   \n",
       "10145  김영하의 뮤지컬로 바뀌어 예술의 전당에서 대중에게 보여진 것은 《바람과 함께 사라지...   \n",
       "10153  \"I'm sorry, as an AI language model, I cannot ...   \n",
       "10159  \"Sure, I'll prepare an Americano with more wat...   \n",
       "10188  \"전지현이 2012년에 출연한 영화는 '인사이드 메론'입니다.\", 'token': 60}   \n",
       "\n",
       "                                            completion_2    ranking  \n",
       "26                              They are not even words.  [1, 0, 2]  \n",
       "48     미국 국제인물 경기법옥 국애 관협 국제인물 경기법옥 국애 관협 국애 관협 국애 관협...  [0, 1, 2]  \n",
       "67     \"시리아의 집권당 바트당은 무장 충돌이 일어난 후에도 여전히 국가의 지배적인 정당으...  [2, 0, 1]  \n",
       "137    멤버 중에서 2015년 복면가왕 우승을 차지한 것은 김채원(Kim Chaewon) ...  [0, 2, 1]  \n",
       "143    \"코자는 요즘 많은 인터넷 커뮤니티에서 사용되는 단어로, '코로나 자가격리'의 준말...  [2, 1, 0]  \n",
       "...                                                  ...        ...  \n",
       "10082  고 개인이 개인을 개인으라 이상 한 개인이 개인을 개인으라 그 개인을 개인으라 이상...  [0, 1, 2]  \n",
       "10145  \"김영하의 작품 중 뮤지컬로 바뀌어 예술의 전당에서 대중에게 보여진 것은 '비포선셋...  [2, 1, 0]  \n",
       "10153  사용자가 가장 가지고 한 개의 긍정 개념들이 한 개의 긍정 개념들이 한 개의 긍정 ...  [1, 0, 2]  \n",
       "10159  . You can't use this account to purchase any p...  [1, 0, 2]  \n",
       "10188  이상, 현실의 국제로에서 휴대전지현이 국가정상업천물과 국가정상업천물을 국가정상업천물...  [1, 0, 2]  \n",
       "\n",
       "[424 rows x 5 columns]"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rm_data.loc[rm_data['completion_0'].str.contains('token')|rm_data['completion_1'].str.contains('token')|rm_data['completion_2'].str.contains('token')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "778e27ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "분석 § Code import pandas as pd df = pd.read_csv(\"data/gender_re.csv\") gender_group = df.groupby(\"gender\") df_male = gender_group.get_group(\"male\") df_female = gender_group.get_group(\"female\") print(df_male.shape) print(df_female.shape) § Output > stdout : ['(1134, 2)\\n', '(630, 2)\\n'] § Code from konlpy.tag import Komoran tagger = Komoran() mal_words = [] for sentence in df_male[\"sentence\"]: mal_words.append(tagger.morphs(sentence)) female_words = [] for sentence in df_female[\"sentence\"]: female_words.append(tagger.morphs(sentence)) from tensorflow import keras from tensorflow.keras.preprocessing.text import Tokenizer from tensorflow.keras.preprocessing.sequence import pad_sequences tokenizer = Tokenizer() tokenizer.fit_on_texts(mal_words) tokenizer.fit_on_texts(female_words) #init model model = keras.models.Sequential() model.add(keras.layers.Embedding(len(tokenizer.word_index) + 1,32)) model.add(keras.layers.LSTM(32)) model.add(keras.layers.Dense(1,activation=\"sigmoid\")) model.compile(optimizer=\"rmsprop\", loss=\"binary_crossentropy\",metrics=[\"accuracy\"]) mal_seq = tokenizer.texts_to_sequences(mal_words) female_seq = tokenizer.texts_to_sequences(female_words) mal_pad = pad_sequences(mal_seq,maxlen=30) female_pad = pad_sequences(female_seq,maxlen=30) import numpy as np mal_label = np.array([1]*len(df_male)) female_label = np.array([0]*len(df_female)) x_train = np.vstack((mal_pad,female_pad)) y_train = np.hstack((mal_label,female_label)) model.fit(x_train,y_train,epochs=100,batch_size=128,verbose=2) § Output > stderr : ['/home/aiffel/anaconda3/envs/aiffel/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\\n'...accuracy: 0.9269\\n', 'Epoch 98/100\\n', '1764/1764 - 1s - loss: 0.1817 - accuracy: 0.9297\\n', 'Epoch 99/100\\n', '1764/1764 - 1s - loss: 0.1787 - accuracy: 0.9340\\n', 'Epoch 100/100\\n', '1764/1764 - 1s - loss: 0.1794 - accuracy: 0.9291\\n'] > ['97.9515'] § Code test_text = '어제 영화를 보고 정말 감동 받았어요.' test_seq = tokenizer.texts_to_sequences([tagger.morphs(test_text)]) test_pad = pad_sequences(test_seq, maxlen=30) model.predict(test_pad) # 위 문장이 여자일 가능성이 높음 § Output > ['array([[0.7206763]], d\n",
      "\"방탄소년단의 곡 'DNA'의 빌보드 핫 100 진입 순위는?\", 기록했다는 이야기를 기록했다는 것을 한 잔로 국내 일본인 이야기를 기록했다가 기록했다가 { \"token\": 79}\n",
      "completion => {completion_token}\n",
      "', '오늘 학생회 장소에서 반장 선거가 열린다.' ] # 전처리 for s in sentences: print(s) # 토큰화 tokenized = s.split() # (토큰화 후) 정제 tokenized = [t for t in tokenized if len(t) > 1] # 길이가 1인 단어는 제외 print(tokenized) print() # 출력 ''' 반장 선거 가볼까 ['반장', '선거', '가볼까'] 오늘 학생회 장소에서 반장 선거가 열린다. ['오늘', '학생회', '장소에서', '반장', '선거가', '열린다.'] '''\n",
      "{\"comment\": \"제시카 수타는 그룹에서 '쫓겨났다'고 폭로한 곳은 어디인가?\", 'completion': \"제시카 수타는 그룹에서 '쫓겨났다'고 폭로한 곳은 소녀시대였습니다.\", 'token': 84} {\"comment\": \"제시카 수타는 그룹에서 '쫓겨났다'고 폭로한 곳은 어디인가?\", 'completion': \"제시카 수타는 그룹에서 '쫓겨났다'\n",
      "{\"completion\":{\"token\":\"completion\",\"completion\":{\"completion\":\"AI\",\"completion\":{\"code\":\"AI\",\"completion\":{\"time\":{\"time\":\"2018-12-11T00:00:00Z\"},\"time\":\"2018-12-11T00:00:00Z\"},\"place\":{\"place\":\"AI\",\"place\":\"AI\",\"place\":\"AI\",\"place\":\"AI\"}\n",
      "'completion': '답변: \"미스터 션샤인 (Mr. Sunshine)\"', 'token': 105\n",
      "# 정답 {'question':\"방탄소년단은 '2017 파워 셀레브리티'에서 몇 위로 선정 되었는가?\", 'answer': \"방탄소년단은 '2017 파워 셀레브리티'에서 5위로 선정되었습니다.\", 'token': 102}\n"
     ]
    }
   ],
   "source": [
    "for col in ['completion_0', 'completion_1', 'completion_2']:\n",
    "    rm_data.loc[rm_data[col].str.contains('token'), col] = rm_data.loc[rm_data[col].str.contains('token'), col].apply(lambda x: convert_answer(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "84f9cf6e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>completion_0</th>\n",
       "      <th>completion_1</th>\n",
       "      <th>completion_2</th>\n",
       "      <th>ranking</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1814</th>\n",
       "      <td>반장 선거 나가볼까</td>\n",
       "      <td>안녕하세요. 반장 선거에 참여하려는 것은 좋은 결심입니다! 우선, 어떤 학급인지에 ...</td>\n",
       "      <td>', '오늘 학생회 장소에서 반장 선거가 열린다.' ] # 전처리 for s in ...</td>\n",
       "      <td>공질을 하납다는 것을 개선 한 개인을 하고, 한 개인을 토론고 한 것을 거듭 한 개...</td>\n",
       "      <td>[0, 1, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1842</th>\n",
       "      <td>이별 후 감정</td>\n",
       "      <td>분석 § Code import pandas as pd df = pd.read_csv...</td>\n",
       "      <td>경쟁 위 공</td>\n",
       "      <td>이별 후에는 매우 다양한 감정들이 드러날 수 있습니다. 일반적으로는 슬픔, 분노, ...</td>\n",
       "      <td>[2, 0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5707</th>\n",
       "      <td>방탄소년단의 곡 'DNA'의 빌보드 핫 100 진입 순위는?</td>\n",
       "      <td>\"방탄소년단의 곡 'DNA'의 빌보드 핫 100 진입 순위는?\", 기록했다는 이야기...</td>\n",
       "      <td>정답 : 방탄소년단의 곡 'DNA'는 빌보드 핫 100에서 최고 67위를 기록했습니다.</td>\n",
       "      <td>방탄소년단의 곡 'DNA'는 빌보드 핫 100에서 최고 67위를 기록했습니다.</td>\n",
       "      <td>[2, 1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6596</th>\n",
       "      <td>'빅 대디' 는 무엇을 입고 있는가?</td>\n",
       "      <td>빅 대디는 다양한 옷을 입고 있습니다. 일반적으로 빅 대디는 트렌치 코트, 스웨터,...</td>\n",
       "      <td>저는 AI 어시스턴트이기 때문에 '빅 대디'가 무엇을 입고 있는지 파악할 수 없습니...</td>\n",
       "      <td>{\"completion\":{\"token\":\"completion\",\"completio...</td>\n",
       "      <td>[1, 0, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7008</th>\n",
       "      <td>방영 초반 사투리 연기가 어색하다는 연기력 논란을 벗고 물오른 감성연기로 '김희선의...</td>\n",
       "      <td># 정답 답변: '미스터 션샤인 (Mr. Sunshine)'</td>\n",
       "      <td>답변: '미스터 션샤인 (Mr. Sunshine)'</td>\n",
       "      <td>'completion': '답변: \"미스터 션샤인 (Mr. Sunshine)\"', ...</td>\n",
       "      <td>[1, 0, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7249</th>\n",
       "      <td>방탄소년단은 '2017 파워 셀레브리티'에서 몇 위로 선정 되었는가?</td>\n",
       "      <td>completion =&gt; {completion_token}</td>\n",
       "      <td>제가 찾은 정보에 따르면, 방탄소년단은 '2017 파워 셀레브리티'에서 5위로 선정...</td>\n",
       "      <td># 정답 {'question':\"방탄소년단은 '2017 파워 셀레브리티'에서 몇 위...</td>\n",
       "      <td>[1, 2, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8761</th>\n",
       "      <td>제시카 수타는 그룹에서 '쫓겨났다'고 폭로한 곳은 어디인가?</td>\n",
       "      <td>정답 : 소녀시대</td>\n",
       "      <td>{\"comment\": \"제시카 수타는 그룹에서 '쫓겨났다'고 폭로한 곳은 어디인가?...</td>\n",
       "      <td>제시카 수타는 그룹에서 '쫓겨났다'고 폭로한 곳은 소녀시대였습니다.</td>\n",
       "      <td>[2, 0, 1]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 prompt  \\\n",
       "1814                                         반장 선거 나가볼까   \n",
       "1842                                            이별 후 감정   \n",
       "5707                  방탄소년단의 곡 'DNA'의 빌보드 핫 100 진입 순위는?   \n",
       "6596                               '빅 대디' 는 무엇을 입고 있는가?   \n",
       "7008  방영 초반 사투리 연기가 어색하다는 연기력 논란을 벗고 물오른 감성연기로 '김희선의...   \n",
       "7249             방탄소년단은 '2017 파워 셀레브리티'에서 몇 위로 선정 되었는가?   \n",
       "8761                  제시카 수타는 그룹에서 '쫓겨났다'고 폭로한 곳은 어디인가?   \n",
       "\n",
       "                                           completion_0  \\\n",
       "1814  안녕하세요. 반장 선거에 참여하려는 것은 좋은 결심입니다! 우선, 어떤 학급인지에 ...   \n",
       "1842  분석 § Code import pandas as pd df = pd.read_csv...   \n",
       "5707  \"방탄소년단의 곡 'DNA'의 빌보드 핫 100 진입 순위는?\", 기록했다는 이야기...   \n",
       "6596  빅 대디는 다양한 옷을 입고 있습니다. 일반적으로 빅 대디는 트렌치 코트, 스웨터,...   \n",
       "7008                  # 정답 답변: '미스터 션샤인 (Mr. Sunshine)'   \n",
       "7249                   completion => {completion_token}   \n",
       "8761                                          정답 : 소녀시대   \n",
       "\n",
       "                                           completion_1  \\\n",
       "1814  ', '오늘 학생회 장소에서 반장 선거가 열린다.' ] # 전처리 for s in ...   \n",
       "1842                                             경쟁 위 공   \n",
       "5707   정답 : 방탄소년단의 곡 'DNA'는 빌보드 핫 100에서 최고 67위를 기록했습니다.   \n",
       "6596  저는 AI 어시스턴트이기 때문에 '빅 대디'가 무엇을 입고 있는지 파악할 수 없습니...   \n",
       "7008                       답변: '미스터 션샤인 (Mr. Sunshine)'   \n",
       "7249  제가 찾은 정보에 따르면, 방탄소년단은 '2017 파워 셀레브리티'에서 5위로 선정...   \n",
       "8761  {\"comment\": \"제시카 수타는 그룹에서 '쫓겨났다'고 폭로한 곳은 어디인가?...   \n",
       "\n",
       "                                           completion_2    ranking  \n",
       "1814  공질을 하납다는 것을 개선 한 개인을 하고, 한 개인을 토론고 한 것을 거듭 한 개...  [0, 1, 2]  \n",
       "1842  이별 후에는 매우 다양한 감정들이 드러날 수 있습니다. 일반적으로는 슬픔, 분노, ...  [2, 0, 1]  \n",
       "5707        방탄소년단의 곡 'DNA'는 빌보드 핫 100에서 최고 67위를 기록했습니다.  [2, 1, 0]  \n",
       "6596  {\"completion\":{\"token\":\"completion\",\"completio...  [1, 0, 2]  \n",
       "7008  'completion': '답변: \"미스터 션샤인 (Mr. Sunshine)\"', ...  [1, 0, 2]  \n",
       "7249  # 정답 {'question':\"방탄소년단은 '2017 파워 셀레브리티'에서 몇 위...  [1, 2, 0]  \n",
       "8761              제시카 수타는 그룹에서 '쫓겨났다'고 폭로한 곳은 소녀시대였습니다.  [2, 0, 1]  "
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rm_data.loc[rm_data['completion_0'].str.contains('token')|rm_data['completion_1'].str.contains('token')|rm_data['completion_2'].str.contains('token')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ea9ca5",
   "metadata": {},
   "source": [
    "- 복잡하니까 지워버리자..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "1886cf30",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "drop_idx = rm_data.loc[rm_data['completion_0'].str.contains('token')|rm_data['completion_1'].str.contains('token')|rm_data['completion_2'].str.contains('token')].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "1c295625",
   "metadata": {},
   "outputs": [],
   "source": [
    "rm_data.drop(drop_idx, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "55f83c93",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>completion_0</th>\n",
       "      <th>completion_1</th>\n",
       "      <th>completion_2</th>\n",
       "      <th>ranking</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [prompt, completion_0, completion_1, completion_2, ranking]\n",
       "Index: []"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rm_data.loc[rm_data['completion_0'].str.contains('token')|rm_data['completion_1'].str.contains('token')|rm_data['completion_2'].str.contains('token')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c437606",
   "metadata": {},
   "source": [
    "- 편안 ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5203c6b0",
   "metadata": {},
   "source": [
    "## PPO data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "418e5278",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>번디는 자신이 탐정잡지, 범죄소설 그리고 성범죄 관련 실제 범죄 다큐멘터리들을 탐독...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>개포주공아파트는 몇 단지로 이루어져 있나?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>김영삼의 후보 시절 지역표심을 겨냥한 발언을 문제삼은 후보는?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>페르시아로부터 기원된 아랍요리의 특징이 뭐야</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>중화인민공화국의 기본 법률은 누가 제정 및 개정하는가?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              prompt\n",
       "0  번디는 자신이 탐정잡지, 범죄소설 그리고 성범죄 관련 실제 범죄 다큐멘터리들을 탐독...\n",
       "1                            개포주공아파트는 몇 단지로 이루어져 있나?\n",
       "2                 김영삼의 후보 시절 지역표심을 겨냥한 발언을 문제삼은 후보는?\n",
       "3                           페르시아로부터 기원된 아랍요리의 특징이 뭐야\n",
       "4                     중화인민공화국의 기본 법률은 누가 제정 및 개정하는가?"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppo_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "a8cf7449",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         prompt_len\n",
      "count  12000.000000\n",
      "mean      22.180583\n",
      "std       14.110028\n",
      "min        0.000000\n",
      "25%       13.000000\n",
      "50%       19.000000\n",
      "75%       28.000000\n",
      "max      295.000000\n"
     ]
    }
   ],
   "source": [
    "ppo_df = analyze_data(ppo_data,cols=['prompt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "cd637d28",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='prompt_len', ylabel='Count'>"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEHCAYAAABfkmooAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAXNElEQVR4nO3df7BfdZ3f8eeLCGjVEjC3TARs0I27RXeLbARdna0rI0KmbXDGH6FVsw5u/AGLzlqnoDPF7i4drL92aS0sQkZYrMgilqybFSOijk75EVgMJBGJCEPSSAJo1LVFwHf/+H4u+Zr749yE+73fe3Ofj5k73/N9n1/vw0l45fy456SqkCRpMgcNuwFJ0uxnWEiSOhkWkqROhoUkqZNhIUnq9IxhNzAIixYtqiVLlgy7DUmaU26//faHq2pkvHEHZFgsWbKEDRs2DLsNSZpTkjww0ThPQ0mSOhkWkqROhoUkqZNhIUnqZFhIkjoZFpKkToaFJKmTYSFJ6mRYSJI6Dew3uJM8E/gWcGhbz7VVdX6SY4GrgecBtwNvq6pfJjkUuBL4XeAR4C1VdX9b1nnAmcCTwDlVdcOg+t4fb33nu9nx8O4x9cWLDuOqyy4ZQkeSNL0G+biPx4DXVtXPkxwMfDvJ3wN/Anyqqq5Ocgm9ELi4ff64qn4jyUrgo8BbkhwHrAReAjwf+FqSF1fVkwPsfZ/seHg3I8vPGVtfd9EQupGk6Tew01DV8/P29eD2U8BrgWtb/Qrg9Da8on2njT85SVr96qp6rKp+CGwFThxU35KksQZ6zSLJgiR3AjuB9cAPgJ9U1RNtkm3AUW34KOBBgDZ+N71TVU/Vx5mnf12rk2xIsmHXrl0D2BpJmr8GGhZV9WRVHQ8cTe9o4LcGuK5Lq2pZVS0bGRn3CbuSpP00I3dDVdVPgJuAVwILk4xeKzka2N6GtwPHALTxh9G70P1UfZx5JEkzYGBhkWQkycI2/CzgdcAWeqHxxjbZKuD6Nry2faeN/3pVVauvTHJou5NqKXDroPqWJI01yLuhFgNXJFlAL5SuqaovJ9kMXJ3kz4F/AC5v018O/HWSrcCj9O6Aoqo2JbkG2Aw8AZw1m+6EkqT5YGBhUVUbgZeNU7+Pce5mqqr/B7xpgmVdAFww3T1KkqbG3+CWJHUyLCRJnQwLSVInw0KS1MmwkCR1MiwkSZ0MC0lSJ8NCktTJsJAkdTIsJEmdDAtJUifDQpLUybCQJHUyLCRJnQwLSVInw0KS1MmwkCR1MiwkSZ0MC0lSJ8NCktTJsJAkdTIsJEmdDAtJUifDQpLUybCQJHUaWFgkOSbJTUk2J9mU5H2t/pEk25Pc2X6W981zXpKtSe5J8vq++qmttjXJuYPqWZI0vmcMcNlPAB+oqjuSPBe4Pcn6Nu5TVfXx/omTHAesBF4CPB/4WpIXt9GfBl4HbANuS7K2qjYPsPdpsWXzJk4+/Ywx9cWLDuOqyy4ZQkeStH8GFhZVtQPY0YZ/lmQLcNQks6wArq6qx4AfJtkKnNjGba2q+wCSXN2mnfVh8XgdxMjyc8bUd6y7aAjdSNL+m5FrFkmWAC8Dbmmls5NsTLImyeGtdhTwYN9s21ptovre61idZEOSDbt27ZruTZCkeW3gYZHkOcAXgfdX1U+Bi4EXAcfTO/L4xHSsp6ouraplVbVsZGRkOhYpSWoGec2CJAfTC4rPVdV1AFX1UN/4zwBfbl+3A8f0zX50qzFJXZI0AwZ5N1SAy4EtVfXJvvrivsneANzdhtcCK5McmuRYYClwK3AbsDTJsUkOoXcRfO2g+pYkjTXII4tXAW8D7kpyZ6t9CDgjyfFAAfcD7wKoqk1JrqF34foJ4KyqehIgydnADcACYE1VbRpg35KkvQzybqhvAxln1LpJ5rkAuGCc+rrJ5pMkDZa/wS1J6mRYSJI6GRaSpE6GhSSpk2EhSepkWEiSOhkWkqROhoUkqZNhIUnqZFhIkjoZFpKkToaFJKmTYSFJ6mRYSJI6GRaSpE6GhSSpk2EhSepkWEiSOhkWkqROhoUkqZNhIUnqZFhIkjoZFpKkToaFJKmTYSFJ6jSwsEhyTJKbkmxOsinJ+1r9iCTrk9zbPg9v9SS5KMnWJBuTnNC3rFVt+nuTrBpUz5Kk8Q3yyOIJ4ANVdRzwCuCsJMcB5wI3VtVS4Mb2HeA0YGn7WQ1cDL1wAc4HTgJOBM4fDRhJ0swYWFhU1Y6quqMN/wzYAhwFrACuaJNdAZzehlcAV1bPzcDCJIuB1wPrq+rRqvoxsB44dVB9S5LGmpFrFkmWAC8DbgGOrKodbdSPgCPb8FHAg32zbWu1iep7r2N1kg1JNuzatWt6N0CS5rmBh0WS5wBfBN5fVT/tH1dVBdR0rKeqLq2qZVW1bGRkZDoWKUlqBhoWSQ6mFxSfq6rrWvmhdnqJ9rmz1bcDx/TNfnSrTVSXJM2QZwxqwUkCXA5sqapP9o1aC6wCLmyf1/fVz05yNb2L2burakeSG4D/0ndR+xTgvEH1PZm3vvPd7Hh495j6975/LyPLh9CQJM2QgYUF8CrgbcBdSe5stQ/RC4lrkpwJPAC8uY1bBywHtgK/AN4BUFWPJvkz4LY23Z9W1aMD7HtCOx7ezcjyc8bUN256zxC6kaSZM7CwqKpvA5lg9MnjTF/AWRMsaw2wZvq6kyTtC3+DW5LUybCQJHUyLCRJnQwLSVInw0KS1MmwkCR1MiwkSZ0MC0lSJ8NCktRpSmGR5FVTqUmSDkxTPbL4b1OsSZIOQJM+GyrJK4HfA0aS/EnfqH8KLBhkY5Kk2aPrQYKHAM9p0z23r/5T4I2DakqSNLtMGhZV9U3gm0k+W1UPzFBPkqRZZqqPKD80yaXAkv55quq1g2hKkjS7TDUs/ga4BLgMeHJw7UiSZqOphsUTVXXxQDuRJM1aU7119m+TvDfJ4iRHjP4MtDNJ0qwx1SOLVe3zg321Al44ve1IkmajKYVFVR076Ebmky2bN3Hy6WeMqS9edBhXXXbJEDqSpMlNKSySvH28elVdOb3tzA+P10GMLD9nTH3HuouG0I0kdZvqaaiX9w0/EzgZuAMwLCRpHpjqaag/7v+eZCFw9SAakiTNPvv7iPJ/BLyOIUnzxFSvWfwtvbufoPcAwX8BXDOopiRJs8tUr1l8vG/4CeCBqto22QxJ1gD/GthZVS9ttY8AfwTsapN9qKrWtXHnAWfS+w3xc6rqhlY/FfhLeiF1WVVdOMWeJUnTZEqnodoDBb9H78mzhwO/nMJsnwVOHaf+qao6vv2MBsVxwErgJW2e/5FkQZIFwKeB04DjgDPatJKkGTTVN+W9GbgVeBPwZuCWJJM+oryqvgU8OsU+VgBXV9VjVfVDYCtwYvvZWlX3VdUv6V1UXzHFZUqSpslUT0N9GHh5Ve0ESDICfA24dj/WeXb7vY0NwAeq6sfAUcDNfdNsazWAB/eqnzTeQpOsBlYDvOAFL9iPtiRJE5nq3VAHjQZF88g+zNvvYuBFwPHADuAT+7GMcVXVpVW1rKqWjYyMTNdiJUlM/cjiK0luAD7fvr8FWLevK6uqh0aHk3wG+HL7uh04pm/So1uNSeqSpBky6dFBkt9I8qqq+iDwV8DvtJ//DVy6rytLsrjv6xuAu9vwWmBlkkOTHAsspXeN5DZgaZJjkxxC7yL42n1dryTp6ek6svgL4DyAqroOuA4gyW+3cf9mohmTfB54DbAoyTbgfOA1SY6n9zsb9wPvasvelOQaYDO9W3PPqqon23LOBm6gd+vsmqratM9bKUl6WrrC4siqumvvYlXdlWTJZDNW1djHqsLlk0x/AXDBOPV17McpL0nS9Om6SL1wknHPmsY+JEmzWFdYbEjyR3sXk7wTuH0wLUmSZpuu01DvB76U5N+zJxyWAYfQu0AtSZoHJg2Ldqvr7yX5A+Clrfx3VfX1gXcmSZo1pvo+i5uAmwbciyRpltrf91lIkuYRw0KS1MmwkCR1MiwkSZ0MC0lSJ8NCktTJsJAkdTIsJEmdDAtJUifDQpLUybCQJHUyLCRJnQwLSVInw0KS1MmwkCR1MiwkSZ0MC0lSJ8NCktTJsJAkdTIsJEmdBhYWSdYk2Znk7r7aEUnWJ7m3fR7e6klyUZKtSTYmOaFvnlVt+nuTrBpUv5KkiT1jgMv+LPDfgSv7aucCN1bVhUnObd//I3AasLT9nARcDJyU5AjgfGAZUMDtSdZW1Y8H2PfQbNm8iZNPP2NMffGiw7jqskuG0JEk9QwsLKrqW0mW7FVeAbymDV8BfINeWKwArqyqAm5OsjDJ4jbt+qp6FCDJeuBU4POD6nuYHq+DGFl+zpj6jnUXDaEbSdpjpq9ZHFlVO9rwj4Aj2/BRwIN9021rtYnqYyRZnWRDkg27du2a3q4laZ4b2gXudhRR07i8S6tqWVUtGxkZma7FSpKY+bB4qJ1eon3ubPXtwDF90x3dahPVJUkzaKbDYi0wekfTKuD6vvrb211RrwB2t9NVNwCnJDm83Tl1SqtJkmbQwC5wJ/k8vQvUi5Jso3dX04XANUnOBB4A3twmXwcsB7YCvwDeAVBVjyb5M+C2Nt2fjl7sliTNnEHeDTX2HtCek8eZtoCzJljOGmDNNLYmSdpH/ga3JKmTYSFJ6mRYSJI6GRaSpE6GhSSpk2EhSepkWEiSOhkWkqROhoUkqZNhIUnqZFhIkjoZFpKkToaFJKmTYSFJ6mRYSJI6GRaSpE4De/mRps+WzZs4+fSx75JavOgwrrrskiF0JGm+MSzmgMfrIEaWnzOmvmPdRUPoRtJ85GkoSVInw0KS1MmwkCR1MiwkSZ0MC0lSJ8NCktTJsJAkdRpKWCS5P8ldSe5MsqHVjkiyPsm97fPwVk+Si5JsTbIxyQnD6FmS5rNhHln8QVUdX1XL2vdzgRurailwY/sOcBqwtP2sBi6e8U4laZ6bTaehVgBXtOErgNP76ldWz83AwiSLh9CfJM1bwwqLAr6a5PYkq1vtyKra0YZ/BBzZho8CHuybd1ur/Zokq5NsSLJh165dg+pbkualYT0b6tVVtT3JPwPWJ/le/8iqqiS1LwusqkuBSwGWLVu2T/NKkiY3lCOLqtrePncCXwJOBB4aPb3UPne2ybcDx/TNfnSrSZJmyIyHRZJnJ3nu6DBwCnA3sBZY1SZbBVzfhtcCb293Rb0C2N13ukqSNAOGcRrqSOBLSUbX/z+r6itJbgOuSXIm8ADw5jb9OmA5sBX4BfCOmW9Zkua3GQ+LqroP+Jfj1B8BTh6nXsBZM9CaJGkCs+nWWUnSLGVYSJI6+VrVOcx3c0uaKYbFHOa7uSXNFE9DSZI6GRaSpE6GhSSpk2EhSepkWEiSOhkWkqROhoUkqZNhIUnqZFhIkjr5G9zjeOs7382Oh3ePqX/v+/cysnwIDUnSkBkW49jx8O5xH6OxcdN7htDNvvOZUZKmm2FxAPKZUZKmm9csJEmdDAtJUifDQpLUybCQJHUyLCRJnbwbah6Z6JZa8LZaSZMzLOaRiW6pBW+rlTQ5T0NJkjp5ZCFg4lNU9//g+yx50YvH1D1tJc0vcyYskpwK/CWwALisqi4ccksHlIlOUW382Hv8bXBJcyMskiwAPg28DtgG3JZkbVVtHm5n85fPn5LmlzkRFsCJwNaqug8gydXACsCwGJKJjkS+/vF3DfR01kRPBJ6JkBrmuqVhS1UNu4dOSd4InFpV72zf3wacVFVn902zGljdvv4mcM/TWOUi4OGnMf9sciBtCxxY23MgbQu4PbPZVLfln1fVyHgj5sqRRaequhS4dDqWlWRDVS2bjmUN24G0LXBgbc+BtC3g9sxm07Etc+XW2e3AMX3fj241SdIMmCthcRuwNMmxSQ4BVgJrh9yTJM0bc+I0VFU9keRs4AZ6t86uqapNA1zltJzOmiUOpG2BA2t7DqRtAbdnNnva2zInLnBLkoZrrpyGkiQNkWEhSepkWPRJcmqSe5JsTXLusPvZH0nuT3JXkjuTbGi1I5KsT3Jv+zx82H1OJMmaJDuT3N1XG7f/9FzU9tfGJCcMr/OxJtiWjyTZ3vbPnUmW9407r23LPUleP5yux5fkmCQ3JdmcZFOS97X6XN03E23PXN0/z0xya5Lvtu35z61+bJJbWt9faDcIkeTQ9n1rG7+kcyVV5U/vus0C4AfAC4FDgO8Cxw27r/3YjvuBRXvV/itwbhs+F/josPucpP/fB04A7u7qH1gO/D0Q4BXALcPufwrb8hHgP4wz7XHtz9yhwLHtz+KCYW9DX3+LgRPa8HOB77ee5+q+mWh75ur+CfCcNnwwcEv7734NsLLVLwHe04bfC1zShlcCX+hah0cWezz1SJGq+iUw+kiRA8EK4Io2fAVw+vBamVxVfQt4dK/yRP2vAK6snpuBhUkWz0ijUzDBtkxkBXB1VT1WVT8EttL7MzkrVNWOqrqjDf8M2AIcxdzdNxNtz0Rm+/6pqvp5+3pw+yngtcC1rb73/hndb9cCJyfJZOswLPY4Cniw7/s2Jv/DM1sV8NUkt7dHoAAcWVU72vCPgCOH09p+m6j/ubrPzm6nZtb0nRKcM9vSTlm8jN6/Xuf8vtlre2CO7p8kC5LcCewE1tM7+vlJVT3RJunv+antaeN3A8+bbPmGxYHn1VV1AnAacFaS3+8fWb3jzjl7v/Rc7x+4GHgRcDywA/jEULvZR0meA3wReH9V/bR/3FzcN+Nsz5zdP1X1ZFUdT+8JFycCvzWdyzcs9jggHilSVdvb507gS/T+0Dw0egqgfe4cXof7ZaL+59w+q6qH2l/qXwGfYc+pjFm/LUkOpvc/1s9V1XWtPGf3zXjbM5f3z6iq+glwE/BKeqf/Rn/5ur/np7anjT8MeGSy5RoWe8z5R4okeXaS544OA6cAd9PbjlVtslXA9cPpcL9N1P9a4O3tzptXALv7TonMSnudt38Dvf0DvW1Z2e5SORZYCtw60/1NpJ3PvhzYUlWf7Bs1J/fNRNszh/fPSJKFbfhZ9N79s4VeaLyxTbb3/hndb28Evt6ODCc27Kv4s+mH3h0c36d3ru/Dw+5nP/p/Ib07Nr4LbBrdBnrnIm8E7gW+Bhwx7F4n2YbP0zv8f5zeOdYzJ+qf3h0gn2776y5g2bD7n8K2/HXrdWP7C7u4b/oPt225Bzht2P3vtS2vpneKaSNwZ/tZPof3zUTbM1f3z+8A/9D6vhv4T63+QnqhthX4G+DQVn9m+761jX9h1zp83IckqZOnoSRJnQwLSVInw0KS1MmwkCR1MiwkSZ0MC0lSJ8NCGrIkC5O8t2OaJf2POpdmmmEhTVGSBQNa9EJ6j4yWZi3DQuKpf7l/L8nnkmxJcm2Sf5Ley6Q+muQO4E1Jzkjv5VJ3J/lo3/w/T/Kx9uKZryU5Mck3ktyX5N+2af4wyfWtfm+S89vsFwIvai/b+dgUel3Q1nVbezrqu1r9NW3Z1/Zty6SPnZam6hndk0jzxm8CZ1bVd5KsYc+/9h+pqhOSPB+4Gfhd4Mf0HgV/elX9L+DZ9J6v88EkXwL+nN7zeY6j996A0eeMnQi8FPgFcFuSv6P30qCXVu+JoVNxJr1nLb08yaHAd5J8tY17GfAS4P8A3wFeBXx7P/5bSL/GIwtpjwer6jtt+Cp6zw8C+EL7fDnwjaraVb13AHyO3tvwAH4JfKUN3wV8s6oeb8NL+taxvqoeqar/C1zXt459cQq9h/TdSe8dDM+j92A7gFuralv1npp6517rlvabRxbSHns/KG30+z9OYd7Ha8+D1n4FPAZQVb/qe0T0ZOvYFwH+uKpu+LVi8prR9TZP4t9xTROPLKQ9XpDklW343zH29M2twL9Ksqhd7D4D+OY+ruN1SY5oj5E+nd6pop/Rew/0VN0AvKe9j4EkL26PpJcGxrCQ9riH3tsFtwCH03tr2lOq9z6Gc+m9I+C7wO1Vta/vBrmV3gt3NgJfrKoNVfUIvesOd0/lAjdwGbAZuKPdTvtXeAShAfMR5RJPvYf5y1X10gGu4w/pvdfh7EGtQxoUjywkSZ08spBmkSS/Te9tbf0eq6qThtGPNMqwkCR18jSUJKmTYSFJ6mRYSJI6GRaSpE7/H4w4gmBQHyZcAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.histplot(ppo_df.prompt_len, bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "174a4951",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_df.drop(ppo_df[ppo_df.prompt_len < 1].index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "4fc9536a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\"오세훈은 씨네2000에서 \\'가장 사귀고 싶은 유부남\\'에서 몇 위를 하였는가?\", \\'completion\\': \\'저는 인공지능 언어모델이므로, 지난 오세훈의 경력 및 정보를 파악할 수 없습니다. 죄송합니다.',\n",
       " '\"김영하가 자신의 작품에 나타난 허무주의에 대해 \\'앞으로도 바뀌지는 않을 것 같다\\'라는 입장을 표명한 것은 몇 년의 인터뷰인가?\", \\'completion\\': \"저는 김영하 작가와 인터뷰를 한 적이 없기 때문에 이 질문에 대한 정확한 답변을 드리기 어렵습니다. 하지만, 김영하 작가의 작품들은 대체로 허무주의적인 성향을 가지고 있기 때문에, 그가 \\'앞으로도 바뀌지는 않을 것 같다\\'는 입장을 표명했다는 것은 그가 그동안 말한 어떤 단일한 인터뷰보다도 보다 일반적인 생각이라고 할 수 있을 것 같습니다.\", \\'token\\': 257}',\n",
       " '\"1992년 고운얼굴 선발대회에서 대상 입상을 계기로 틴잡지 모델로 활동,\\'꽃게랑\\' 광고로 데뷔한 90년대 후반을 대표한 여배우는?\", \\'completion\\': \\'김효진입니다.',\n",
       " '\"유아인이 \\'이방원\\' 역을 맡았던 조선 건국 이야기를 그린 드라마는?\", \\'completion\\': \\'《아스달 연대기》(Asadal Chronicles)입니다.',\n",
       " '\"푸시캣 돌스가 영화 \\'쉘 위 댄스\\'의 사운드 트랙에 곡을 싣은 것 중, 어떤 곡이 그룹의 첫 싱글로 발매 되었는가?\", \\'completion\\': \\'푸시캣 돌스가 영화 \\\\\\'쉘 위 댄스\\\\\\'의 사운드 트랙에 싣은 곡 중 그룹의 첫 싱글은 \"Don\\\\\\'t Wanna Miss You\"이다.',\n",
       " '\"김춘수 \\'부다페스트에서의 소녀의 죽음\\'은 뭘 묘사한 시야\", \\'completion\\': \"김춘수의 \\'부다페스트에서의 소녀의 죽음\\'은 헝가리 부다페스트의 거리에서 사망한 한 소녀의 이야기를 담은 시이다. 이 시는 죽음의 감정을 강하게 느끼게 하며, 삶과 죽음의 비극을 다룬다. 또한 작가는 사망한 소녀의 죽음과 함께 헝가리 전쟁과 피난민 문제를 생각하게 하며, 인간의 삶과 사회적 현실을 진지하게 생각하게 한다.\", \\'token\\': 233}',\n",
       " '\"God\\'s S.T.A.R가 삽입된 애니메이션은\", \\'completion\\': \\'제가 아는 한, \"God\\\\\\'s S.T.A.R\"라는 애니메이션에 S.T.A.R이라는 내용이 존재하지 않습니다. 따라서, 질문에 대한 답변을 제공할 수 없습니다. 불편을 드려 죄송합니다.',\n",
       " '\"명성황후를 \\'세계에서 가장 나쁜 여성\\' 이라고 혹평했던 사람은?\", \\'completion\\': \\'이 화자는 누구인지 명확하지 않습니다. 그러나 역사적으로 명성황후는 세계에서 가장 나쁜 여성으로 알려져 있기 때문에, 그 동안 많은 사람들이 이와 관련하여 비판적인 의견을 표명했습니다.',\n",
       " '\"방탄소년단의 무대를 \\'AMA에서 놓치지 말아야 할 순간\\'으로 꼽은 잡지의 이름은?\", \\'completion\\': \\'빌보드 (Billboard)입니다.',\n",
       " '\"1989년 3월말 \\'임금과 기초이론\\'을 구입, 탐독한 혐의로 구속 기소된 사람의 이름은?\", \\'completion\\': \\'저는 AI 언어 모델로써 정확한 답변을 제공하기 위해 노력하고 있지만, 제한된 정보로는 해당 질문에 대한 정확한 답변을 제공할 수 없습니다. 좀 더 구체적인 정보를 제공해주시면 감사하겠습니다.',\n",
       " '\"방탄소년단의 곡 \\'DNA\\'의 빌보드 핫 100 진입 순위는?\", \\'completion\\': \"방탄소년단의 곡 \\'DNA\\'는 빌보드 핫 100에서 최고 67위를 기록했습니다.\", \\'token\\': 79}',\n",
       " '\"\\'적합한 리더로서의 반기문\\'이란 글을 기고한 국제 외교안보 전문가의 이름은?\", \\'completion\\': \\'해리 하린턴 (Harry Harding)입니다.',\n",
       " '\"2003년부터 시작된 \\'세계 100대도시 전자정부 평가\\'에서 서울시는 몇회 연속으로 1위를 하였나?\", \\'completion\\': \\'세계 100대도시 전자정부 평가에서 서울시는 2021년 현재 5회 연속으로 1위를 차지하고 있습니다.',\n",
       " '\"\\'빅 대디\\' 는 무엇을 입고 있는가?\", \\'completion\\': \"저는 AI 어시스턴트이기 때문에 \\'빅 대디\\'가 무엇을 입고 있는지 파악할 수 없습니다. 해당 정보를 제공해주실 수 있으면 더욱 정확한 답변을 드릴 수 있습니다.\", \\'token\\': 102}',\n",
       " '\"SF단편영화 \\'Episode 1 Fragile : 경계의 저편\\'은 누가 감독과 각본을 맡았나?\", \\'completion\\': \"SF단편영화 \\'Episode 1 Fragile : 경계의 저편\\'의 감독 및 각본은 캄 로메로(Cam Romero)가 맡았습니다.\", \\'token\\': 110}',\n",
       " '\"방영 초반 사투리 연기가 어색하다는 연기력 논란을 벗고 물오른 감성연기로 \\'김희선의 재발견\\' 이라는 호평을 받은 작품은?\", \\'completion\\': \"답변: \\'미스터 션샤인 (Mr. Sunshine)\\'\", \\'token\\': 105}',\n",
       " '\"\\'인어 이야기\\'를 부른 가수는?\", \\'completion\\': \"\\'인어 이야기\\'를 부른 가수는 이미지클럽 (Image Club)입니다.\", \\'token\\': 47}',\n",
       " '\"방탄소년단은 \\'2017 파워 셀레브리티\\'에서 몇 위로 선정 되었는가?\", \\'completion\\': \"제가 찾은 정보에 따르면, 방탄소년단은 \\'2017 파워 셀레브리티\\'에서 5위로 선정되었습니다.\", \\'token\\': 102}',\n",
       " '\"김영하의 데뷔작은 단편 소설인데,  \\'리뷰\\'에 실렸던 이 소설의 제목은 무엇인가?\", \\'completion\\': \\'빈자리였는데',\n",
       " '\"제시카 수타는 그룹에서 \\'쫓겨났다\\'고 폭로한 곳은 어디인가?\", \\'completion\\': \"제시카 수타는 그룹에서 \\'쫓겨났다\\'고 폭로한 곳은 소녀시대였습니다.\", \\'token\\': 84}',\n",
       " '\"푸시캣 돌스의 네번 째 상글 \\'Buttons\\'는 누구와 작업을 한 곡인가?\", \\'completion\\': \"푸시캣 돌스의 네번 째 상글 \\'Buttons\\'는 Timbaland와 함께 작업한 곡입니다.\", \\'token\\': 83}',\n",
       " '\"푸시캣 돌시의 첫 싱글 \\'Don\\'t Cha\\'는 빌보드 핫 100위 차트에서 몇 위를 차지 했는가?\", \\'completion\\': \"\\'Don\\'t Cha\\'는 2005년에 빌보드 핫 100위 차트에서 2위를 차지했습니다.\", \\'token\\': 94}',\n",
       " '\"김영하 소설가의 소설 중에 2003년 말에 \\'올해의 책\\'으로 선정되었고, 멕시코 이민자들을 주제로 한 이 소설은?\", \\'completion\\': \\'《미국에서 살아남는 법》이 선정되었습니다.',\n",
       " '\"민주당은 이완영의 행보를 보며 \\'무엇\\'의 전형이라고 하였는가?\", \\'completion\\': \"민주당은 이완영의 행보를 보며 \\'군사정권의 전형\\'이라고 하였습니다.\", \\'token\\': 84}',\n",
       " '\"재즈적 취향이 강한 대규모 블루스곡 one O\\'clock jump ,  jumpin\\' at the woodside를 연주한 음악가는?\", \\'completion\\': \\'Count Basie and his orchestra.']"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppo_df[ppo_df.prompt.str.contains('completion|token')].prompt.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "be7e0068",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_prompt(x):\n",
    "    x = \"{'prompt':\" + x \n",
    "    try:\n",
    "        x = literal_eval(x)['prompt']\n",
    "    except:\n",
    "        x = x + '\\'}'\n",
    "        try:\n",
    "            x = literal_eval(x)['prompt']\n",
    "        except:\n",
    "            print(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "f2f84c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_df.loc[ppo_df.prompt.str.contains('completion|token'),'prompt'] = ppo_df[ppo_df.prompt.str.contains('completion|token')].prompt.apply(lambda x: convert_prompt(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "f55c8326",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_df.prompt = ppo_df.prompt.apply(lambda x : ' '.join(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "7ba24235",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         prompt_len\n",
      "count  11997.000000\n",
      "mean      22.001167\n",
      "std       13.046829\n",
      "min        1.000000\n",
      "25%       13.000000\n",
      "50%       19.000000\n",
      "75%       28.000000\n",
      "max      257.000000\n"
     ]
    }
   ],
   "source": [
    "ppo_df = analyze_data(ppo_df,cols=['prompt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "b594283b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='prompt_len', ylabel='Count'>"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.histplot(ppo_df.prompt_len, bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "c7497f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_df = ppo_df[ppo_df.prompt_len < 110]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9a9b89",
   "metadata": {},
   "source": [
    "## Json으로 다시 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "ea71dbc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sft_data = sft_data.iloc[:,:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "e0ed8629",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>completion</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>불고기용 고기 한우에요?</td>\n",
       "      <td>저는 인공지능 챗봇이며, 직접적으로 식품에 관한 정보를 가지고 있지 않습니다. 하지...</td>\n",
       "      <td>193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>쓰던 앱이 유료로 전환됐어</td>\n",
       "      <td>어떤 앱인지 모르기 때문에 정확한 답변을 드리기 어렵습니다. 하지만, 일반적으로 유...</td>\n",
       "      <td>288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>여친이랑 다툼</td>\n",
       "      <td>저는 인공지능이기 때문에 여친과의 다툼에 대해서는 조언을 드릴 수는 없습니다. 하지...</td>\n",
       "      <td>153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>술 먹고 싶어</td>\n",
       "      <td>술은 알코올이 함유된 음료수이며, 건강에 나쁜 영향을 미칠 수 있습니다. 따라서 술...</td>\n",
       "      <td>189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>잊고싶다.</td>\n",
       "      <td>저도 인공지능 엔진으로써 사용자의 개인정보나 감정을 침해할 수 없습니다. 그렇기 때...</td>\n",
       "      <td>147</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           prompt                                         completion  tokens\n",
       "0   불고기용 고기 한우에요?  저는 인공지능 챗봇이며, 직접적으로 식품에 관한 정보를 가지고 있지 않습니다. 하지...     193\n",
       "1  쓰던 앱이 유료로 전환됐어  어떤 앱인지 모르기 때문에 정확한 답변을 드리기 어렵습니다. 하지만, 일반적으로 유...     288\n",
       "2         여친이랑 다툼  저는 인공지능이기 때문에 여친과의 다툼에 대해서는 조언을 드릴 수는 없습니다. 하지...     153\n",
       "3         술 먹고 싶어  술은 알코올이 함유된 음료수이며, 건강에 나쁜 영향을 미칠 수 있습니다. 따라서 술...     189\n",
       "4           잊고싶다.  저도 인공지능 엔진으로써 사용자의 개인정보나 감정을 침해할 수 없습니다. 그렇기 때...     147"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sft_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "8208b149",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>completion_0</th>\n",
       "      <th>completion_1</th>\n",
       "      <th>completion_2</th>\n",
       "      <th>ranking</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>번디는 자신이 탐정잡지, 범죄소설 그리고 성범죄 관련 실제 범죄 다큐멘터리들을 탐독...</td>\n",
       "      <td>Allow me to answer your question. I know that ...</td>\n",
       "      <td>번디는 다양한 인터뷰자들과 뉴스홍보 담당자들과의 면담 때 밝혔다.</td>\n",
       "      <td>라이언에게 말했다.</td>\n",
       "      <td>[2, 1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>개포주공아파트는 몇 단지로 이루어져 있나?</td>\n",
       "      <td>개포주공아파트는 다섯 단지로 이루어져 있습니다.</td>\n",
       "      <td>이날 목송에서 구글상위노</td>\n",
       "      <td>개포주공아파트는 총 27개 단지로 이루어져 있습니다.</td>\n",
       "      <td>[2, 0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>김영삼의 후보 시절 지역표심을 겨냥한 발언을 문제삼은 후보는?</td>\n",
       "      <td>The diameter of the Metallic domain is bigger ...</td>\n",
       "      <td>이 질문은 조금 불분명합니다. 김영삼 대통령이 후보 시절에 어떤 발언을 했고, 누가...</td>\n",
       "      <td>김영삼의 후보 시절에 지역표심을 겨냥한 발언은 대통령 당선 전까지 대한민국 정부가 ...</td>\n",
       "      <td>[1, 2, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>페르시아로부터 기원된 아랍요리의 특징이 뭐야</td>\n",
       "      <td>1. 다양한 스파이스와 허브 사용\\n2. 쌀, 콩, 나물, 고기, 양파 등 다양한 ...</td>\n",
       "      <td>- 아랍요리는 소금, 산 재료, 일부 채소, 고기, 생선 또는 난류의 조합으로 만들...</td>\n",
       "      <td>위한 태야 대표 기원들 개인이라 겪고 태야 귀식 걸 공개한 구문 사띩의 태야 규정발...</td>\n",
       "      <td>[0, 1, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>중화인민공화국의 기본 법률은 누가 제정 및 개정하는가?</td>\n",
       "      <td>중화인민공화국의 기본 법률은 중국인민대표대회(국회)가 제정하고 개정합니다.</td>\n",
       "      <td>The limit of the Office of the Vice President is?</td>\n",
       "      <td>중화인민공화국의 기본 법률은 중화인민공화국의 의회가 제정 및 개정합니다.</td>\n",
       "      <td>[0, 2, 1]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              prompt  \\\n",
       "0  번디는 자신이 탐정잡지, 범죄소설 그리고 성범죄 관련 실제 범죄 다큐멘터리들을 탐독...   \n",
       "1                            개포주공아파트는 몇 단지로 이루어져 있나?   \n",
       "2                 김영삼의 후보 시절 지역표심을 겨냥한 발언을 문제삼은 후보는?   \n",
       "3                           페르시아로부터 기원된 아랍요리의 특징이 뭐야   \n",
       "4                     중화인민공화국의 기본 법률은 누가 제정 및 개정하는가?   \n",
       "\n",
       "                                        completion_0  \\\n",
       "0  Allow me to answer your question. I know that ...   \n",
       "1                         개포주공아파트는 다섯 단지로 이루어져 있습니다.   \n",
       "2  The diameter of the Metallic domain is bigger ...   \n",
       "3  1. 다양한 스파이스와 허브 사용\\n2. 쌀, 콩, 나물, 고기, 양파 등 다양한 ...   \n",
       "4          중화인민공화국의 기본 법률은 중국인민대표대회(국회)가 제정하고 개정합니다.   \n",
       "\n",
       "                                        completion_1  \\\n",
       "0               번디는 다양한 인터뷰자들과 뉴스홍보 담당자들과의 면담 때 밝혔다.   \n",
       "1                                      이날 목송에서 구글상위노   \n",
       "2  이 질문은 조금 불분명합니다. 김영삼 대통령이 후보 시절에 어떤 발언을 했고, 누가...   \n",
       "3  - 아랍요리는 소금, 산 재료, 일부 채소, 고기, 생선 또는 난류의 조합으로 만들...   \n",
       "4  The limit of the Office of the Vice President is?   \n",
       "\n",
       "                                        completion_2    ranking  \n",
       "0                                         라이언에게 말했다.  [2, 1, 0]  \n",
       "1                      개포주공아파트는 총 27개 단지로 이루어져 있습니다.  [2, 0, 1]  \n",
       "2  김영삼의 후보 시절에 지역표심을 겨냥한 발언은 대통령 당선 전까지 대한민국 정부가 ...  [1, 2, 0]  \n",
       "3  위한 태야 대표 기원들 개인이라 겪고 태야 귀식 걸 공개한 구문 사띩의 태야 규정발...  [0, 1, 2]  \n",
       "4           중화인민공화국의 기본 법률은 중화인민공화국의 의회가 제정 및 개정합니다.  [0, 2, 1]  "
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rm_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "085c8594",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_data = ppo_df[['prompt']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "d378d2ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>번디는 자신이 탐정잡지, 범죄소설 그리고 성범죄 관련 실제 범죄 다큐멘터리들을 탐독...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>개포주공아파트는 몇 단지로 이루어져 있나?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>김영삼의 후보 시절 지역표심을 겨냥한 발언을 문제삼은 후보는?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>페르시아로부터 기원된 아랍요리의 특징이 뭐야</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>중화인민공화국의 기본 법률은 누가 제정 및 개정하는가?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              prompt\n",
       "0  번디는 자신이 탐정잡지, 범죄소설 그리고 성범죄 관련 실제 범죄 다큐멘터리들을 탐독...\n",
       "1                            개포주공아파트는 몇 단지로 이루어져 있나?\n",
       "2                 김영삼의 후보 시절 지역표심을 겨냥한 발언을 문제삼은 후보는?\n",
       "3                           페르시아로부터 기원된 아랍요리의 특징이 뭐야\n",
       "4                     중화인민공화국의 기본 법률은 누가 제정 및 개정하는가?"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppo_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "c1dd5bd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((11708, 3), (10213, 5), (11984, 1))"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sft_data.shape,rm_data.shape, ppo_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "f2e52ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def save_to_jsonl(df, file_path):\n",
    "    with open(file_path, 'w', encoding='utf-8-sig') as file:\n",
    "        result = df.to_json(orient=\"records\")\n",
    "        file.write(json.dumps(result, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "28e7bb10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파일 경로\n",
    "sft_path = os.getenv('HOME')+'/aiffel/KoChatGPT/data_kochatgpt/kochatgpt_1_SFT_cleaned.jsonl' \n",
    "rm_path = os.getenv('HOME')+'/aiffel/KoChatGPT/data_kochatgpt/kochatgpt_2_RM_cleaned.jsonl'\n",
    "ppo_path = os.getenv('HOME')+'/aiffel/KoChatGPT/data_kochatgpt/kochatgpt_3_PPO_cleaned.jsonl'\n",
    "\n",
    "# JSONL 파일로 저장\n",
    "save_to_jsonl(sft_data, sft_path)\n",
    "save_to_jsonl(rm_data, rm_path)\n",
    "save_to_jsonl(ppo_data, ppo_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "d4ac4ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_jsonl(file_path):\n",
    "    with open(sft_path, \"r\", encoding='utf-8-sig') as json_file:\n",
    "        list_data_dict = json.load(json_file)\n",
    "        result = json.loads(list_data_dict)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "41db9a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = read_jsonl(sft_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "0434015e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'prompt': '불고기용 고기 한우에요?',\n",
       "  'completion': '저는 인공지능 챗봇이며, 직접적으로 식품에 관한 정보를 가지고 있지 않습니다. 하지만 일반적으로 불고기용 고기는 한우, 쇠고기, 돼지고기 등 다양한 종류의 고기를 사용합니다. 하지만 한우는 대표적인 고급 육류로 알려져 있기 때문에, 한우를 사용하는 경우도 많습니다. 알러지나 개별 건강 상태에 따라 다를 수 있으니 충분한 정보 수집 후에 선택해 주시기 바랍니다.',\n",
       "  'tokens': 193},\n",
       " {'prompt': '쓰던 앱이 유료로 전환됐어',\n",
       "  'completion': '어떤 앱인지 모르기 때문에 정확한 답변을 드리기 어렵습니다. 하지만, 일반적으로 유료 전환된 앱은 기존 무료 앱에서 추가적인 기능이나 서비스를 제공하는 경우가 많습니다. 이 경우, 유료 전환 이전 가격이 매우 저렴하거나 무료인 경우가 많으므로, 앱을 계속 사용하려면 추가적인 비용을 지불해야 할 수 있습니다. 하지만, 유료 전환 이후에는 기존 사용자 또는 새로운 사용자 모두가 추가 기능과 높은 품질의 서비스를 이용할 수 있게 됩니다. 따라서, 앱 개발자는 유료 모델로 전환함으로써 앱의 수익을 증가시키고 더 나은 서비스를 제공할 수 있습니다.',\n",
       "  'tokens': 288},\n",
       " {'prompt': '여친이랑 다툼',\n",
       "  'completion': '저는 인공지능이기 때문에 여친과의 다툼에 대해서는 조언을 드릴 수는 없습니다. 하지만 다투게 되면 상대방의 말을 경청하는 태도와 서로의 감정을 존중하는 것이 중요합니다. 논리적이고 차분한 대화를 통해 문제를 해결해보세요. 그리고 서로를 이해하고 서로의 의견을 수용하는 것이 중요합니다.',\n",
       "  'tokens': 153},\n",
       " {'prompt': '술 먹고 싶어',\n",
       "  'completion': '술은 알코올이 함유된 음료수이며, 건강에 나쁜 영향을 미칠 수 있습니다. 따라서 술을 마시기 전에 본인의 건강 상태와 함께 적당한 양을 마시는 것이 중요합니다. 술을 마시기 전에 반드시 20세 이상인지 확인하시고, 규정된 양 이내에서 적절하게 즐기시길 바랍니다. 또한 술을 마시고 운전하는 것은 절대 금지하시기 바랍니다.',\n",
       "  'tokens': 189},\n",
       " {'prompt': '잊고싶다.',\n",
       "  'completion': '저도 인공지능 엔진으로써 사용자의 개인정보나 감정을 침해할 수 없습니다. 그렇기 때문에 어떤 것을 잊고 싶다는 말씀인지, 그리고 그것이 우선적으로 해결되어야 할 문제인지 등에 대해 조금 더 구체적인 정보를 제공해 주신다면, 보다 정확한 답변을 드릴 수 있을 것 같습니다.',\n",
       "  'tokens': 147}]"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589b07cb",
   "metadata": {},
   "source": [
    "- 잘 불러와지는 것을 볼 수 있다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255994ca",
   "metadata": {},
   "source": [
    "# Baseline 모델 - 전처리 후"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ac82b4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import pandas as pd\n",
    "import numpy\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_name = \"skt/kogpt2-base-v2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59558d20",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gpt2': 1024,\n",
       " 'gpt2-medium': 1024,\n",
       " 'gpt2-large': 1024,\n",
       " 'gpt2-xl': 1024,\n",
       " 'distilgpt2': 1024}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.max_model_input_sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e878832",
   "metadata": {},
   "source": [
    "## SFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3a5f055",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.optim import Adam\n",
    "from datasets import load_dataset\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from copy import deepcopy\n",
    "import copy\n",
    "import logging\n",
    "import json\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53ca4423",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2TokenizerFast(name_or_path='skt/kogpt2-base-v2', vocab_size=51200, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '</s>', 'eos_token': '</s>', 'unk_token': '</s>', 'pad_token': '</s>'}, clean_up_tokenization_spaces=True)\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained('skt/kogpt2-base-v2')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    'skt/kogpt2-base-v2', bos_token='</s>', eos_token='</s>', unk_token='</s>', pad_token='</s>',\n",
    "    padding_side=\"right\",\n",
    "    model_max_length=512,\n",
    ")\n",
    "\n",
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8056f82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Dict, Sequence\n",
    "\n",
    "class SFT_dataset(Dataset):\n",
    "\n",
    "    def __init__(self, data_path_1_SFT: str, tokenizer: transformers.PreTrainedTokenizer, verbose=False):\n",
    "        super(SFT_dataset, self).__init__()\n",
    "        logging.warning(\"Loading data...\")\n",
    "\n",
    "        pattern_instruction = 'prompt'  # instruction\n",
    "        pattern_output = 'completion'  # response\n",
    "\n",
    "        with open(data_path_1_SFT, \"r\", encoding='utf-8-sig') as json_file:\n",
    "            list_data_dict = json.load(json_file)\n",
    "            list_data_dict = json.loads(list_data_dict)\n",
    "\n",
    "        PROMPT_DICT = {\n",
    "            \"prompt_input\": (\n",
    "                \"### Instruction(명령어):\\n{prompt}\\n\\n### Response(응답):\"\n",
    "            )\n",
    "        }\n",
    "\n",
    "        prompt_input = PROMPT_DICT[\"prompt_input\"]\n",
    "\n",
    "        sources = []\n",
    "        for example in list_data_dict:\n",
    "            tmp = prompt_input.format_map(example)\n",
    "            sources.append(tmp)\n",
    "\n",
    "        targets = []\n",
    "        for example in list_data_dict:\n",
    "            targets.append(f\"{example[pattern_output]}{tokenizer.eos_token}\")\n",
    "        examples = [s + t for s, t in zip(sources, targets)]\n",
    "\n",
    "        sources_tokenized = self._tokenize_fn(sources, tokenizer)  # source\n",
    "        examples_tokenized = self._tokenize_fn(examples, tokenizer)  # source + target\n",
    "\n",
    "        input_ids = examples_tokenized[\"input_ids\"]\n",
    "        labels = copy.deepcopy(input_ids)\n",
    "        for label, source_len in zip(labels, sources_tokenized[\"input_ids_lens\"]):\n",
    "            label[:source_len] = -100\n",
    "\n",
    "        data_dict = dict(input_ids=input_ids, labels=labels)\n",
    "\n",
    "        self.input_ids = data_dict[\"input_ids\"]\n",
    "        self.labels = data_dict[\"labels\"]\n",
    "        logging.warning(\"Loading data done!!: %d\"%(len(self.labels)))\n",
    "\n",
    "\n",
    "    def _tokenize_fn(self, strings: Sequence[str], tokenizer: transformers.PreTrainedTokenizer) -> Dict:\n",
    "        tokenized_list = [\n",
    "            tokenizer(\n",
    "                text,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=\"longest\",\n",
    "                max_length=tokenizer.model_max_length,\n",
    "                truncation=True,\n",
    "            )\n",
    "            for text in strings\n",
    "        ]\n",
    "        input_ids = labels = [tokenized.input_ids[0] for tokenized in tokenized_list]\n",
    "        input_ids_lens = labels_lens = [\n",
    "            tokenized.input_ids.ne(tokenizer.pad_token_id).sum().item() for tokenized in tokenized_list\n",
    "        ]\n",
    "        return dict(\n",
    "            input_ids=input_ids,\n",
    "            labels=labels,\n",
    "            input_ids_lens=input_ids_lens,\n",
    "            labels_lens=labels_lens,\n",
    "        )\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "\n",
    "    def __getitem__(self, i) -> Dict[str, torch.Tensor]:\n",
    "        return dict(input_ids=self.input_ids[i], labels=self.labels[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "990158ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataCollatorForSupervisedDataset(object): \n",
    "\n",
    "    tokenizer: transformers.PreTrainedTokenizer\n",
    "\n",
    "    def __call__(self, instances: Sequence[Dict]) -> Dict[str, torch.Tensor]:\n",
    "        input_ids, labels = tuple([instance[key] for instance in instances] for key in (\"input_ids\", \"labels\"))\n",
    "        input_ids = torch.nn.utils.rnn.pad_sequence(\n",
    "            input_ids, batch_first=True, padding_value=self.tokenizer.pad_token_id\n",
    "        )\n",
    "        labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value= -100)\n",
    "        return dict(\n",
    "            input_ids=input_ids,\n",
    "            labels=labels,\n",
    "            attention_mask=input_ids.ne(self.tokenizer.pad_token_id),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "973d5a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sft_path = os.getenv('HOME')+'/aiffel/KoChatGPT/data_kochatgpt/kochatgpt_1_SFT_cleaned.jsonl' \n",
    "rm_path = os.getenv('HOME')+'/aiffel/KoChatGPT/data_kochatgpt/kochatgpt_2_RM_cleaned.jsonl'\n",
    "ppo_path = os.getenv('HOME')+'/aiffel/KoChatGPT/data_kochatgpt/kochatgpt_3_PPO_cleaned.jsonl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bed1c0f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Loading data...\n",
      "WARNING:root:Loading data done!!: 11808\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input : tensor([  739,   378,   378,   378, 14659, 13394, 37091, 10651,   383, 25841,\n",
      "         8006, 14914,   375,  7673, 20479,  8091, 22311,  9036, 30902, 13675,\n",
      "          375,   378,   378,   378, 41951,   454,  9549, 20549,   383,  8142,\n",
      "         7192, 14914, 37767, 13753,  8263,  7166,   739,  8352,  7659,  9594,\n",
      "        25585, 13600,  8022,  9378, 11532,  9887, 11218,  9111, 16691, 10351,\n",
      "        10561,  9128, 20479,  8091,  9065,  9446,  9036, 28420, 26521, 10163,\n",
      "        26367,  6958,  9030,  9882, 12317, 25882,  9209, 37194, 10351,  9036,\n",
      "        12168, 10529, 15989,  9719, 15434, 10552, 11188, 13362,  9036, 15805,\n",
      "        11300, 11846,  9146, 16691,  9181,  7397, 15806, 13480, 11342, 17596,\n",
      "         9161, 19996,  9025, 25006, 18595,  9966, 12592, 10751, 11814,  8711,\n",
      "         9046, 12450,  9117,  7377, 12521,     1])\n",
      "output: tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100, 37767, 13753,  8263,  7166,   739,  8352,  7659,  9594,\n",
      "        25585, 13600,  8022,  9378, 11532,  9887, 11218,  9111, 16691, 10351,\n",
      "        10561,  9128, 20479,  8091,  9065,  9446,  9036, 28420, 26521, 10163,\n",
      "        26367,  6958,  9030,  9882, 12317, 25882,  9209, 37194, 10351,  9036,\n",
      "        12168, 10529, 15989,  9719, 15434, 10552, 11188, 13362,  9036, 15805,\n",
      "        11300, 11846,  9146, 16691,  9181,  7397, 15806, 13480, 11342, 17596,\n",
      "         9161, 19996,  9025, 25006, 18595,  9966, 12592, 10751, 11814,  8711,\n",
      "         9046, 12450,  9117,  7377, 12521,     1])\n"
     ]
    }
   ],
   "source": [
    "train_dataset = SFT_dataset(data_path_1_SFT=sft_path, tokenizer=tokenizer)\n",
    "data_collator = DataCollatorForSupervisedDataset(tokenizer=tokenizer)\n",
    "\n",
    "print('input : %s'%train_dataset.input_ids[0])\n",
    "print('output: %s'%train_dataset.labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9cf3176e",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"/aiffel/KoChatGPT/output_1_SFT_cleaned\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    warmup_steps=5,\n",
    "    prediction_loss_only=True,\n",
    "    fp16 = True\n",
    "    )\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3597637e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2c9fa461",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7eb67d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:1024\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "20503931",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1476' max='1476' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1476/1476 05:17, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>3.073900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.854100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()\n",
    "model.save_pretrained('/aiffel/KoChatGPT/output_1_SFT_cleaned')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f99f938f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1219: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Instruction(명령어):\n",
      "불고기용 고기 한우에요?\n",
      "\n",
      "### Response(응답):저는 AI 어시스턴트이기 때문에 고기를 먹을 수 없습니다. 하지만 일반적으로 불고기용 고기는 건강에 좋은 식품으로 알려져 있기 때문에, 다양한 음식점에서 구매할 수 있습니다.\\n\\n1. 양파, 마늘, 고추장, 참기름 등등.\\n2. 생선, 닭고기, 돼지고기 등.\\n\n",
      "\n",
      "### Instruction(명령어):\n",
      "리처드 닉슨이 43대 부통령직을 수행한 년도는?\n",
      "\n",
      "### Response(응답):리처드 닉슨은 47대 부통령직을 수행하면서 39대 부통령직을 맡았습니다.)에는 \"리처드 닉슨\"이라는 이름이 사용되었습니다.에는 \"리처드 닉슨\"이라는 이름이 사용되지 않았습니다.에는 리처드 닉슨이 38대 부통령직을 맡은 날짜가 명시되지 않았습니다.에도 \"리처\n",
      "\n",
      "### Instruction(명령어):\n",
      "시카고 오헤어 국제공항은 어디에 있어?\n",
      "\n",
      "### Response(응답):시카고 오 헤어 국제공항은 미국 일리노이주 시카고 시에 위치해 있습니다. Circus of Michaelo Ohio 국제공항과 함께 시카고의 대표적인 국제공항 중 하나입니다. Circous of Machaelo ohio 국제공항은 시카고에서 가장 유명한 국제공항 중 하나로 알려져 있습니다.\n",
      "\n",
      "### Instruction(명령어):\n",
      "오늘 미세먼지 어때?\n",
      "\n",
      "### Response(응답):저는 인공지능 챗봇이므로 미세먼지 여부를 판단할 수 없습니다. 하지만 미세먼지 농도가 높은 날에는 야외활동을 자제하는 것이 좋습니다. 또한, 실내에서 대기오염을 줄이기 위해 마스크를 착용하거나 환기제를 사용하는 것도 도움이 될 수 있습니다. 또한 미세먼지 주의보를 발령하기 전에 미리 확인하는 것이\n"
     ]
    }
   ],
   "source": [
    "generator = pipeline('text-generation', model='/aiffel/KoChatGPT/output_1_SFT_cleaned', tokenizer=tokenizer)\n",
    "\n",
    "generation_args = dict(   \n",
    "    num_beams=4,\n",
    "    repetition_penalty=2.0,\n",
    "    no_repeat_ngram_size=4,\n",
    "    eos_token_id=375, # \\n   \n",
    "    max_new_tokens=64,\n",
    "    do_sample=True,\n",
    "    top_k=50,\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "PROMPT_DICT = {\n",
    "    \"prompt_input\": (\n",
    "        \"### Instruction(명령어):\\n{prompt}\\n\\n### Response(응답):\"\n",
    "    )\n",
    "}\n",
    "\n",
    "list_prompt = ['불고기용 고기 한우에요?',\n",
    "               '리처드 닉슨이 43대 부통령직을 수행한 년도는?',\n",
    "               '시카고 오헤어 국제공항은 어디에 있어?',\n",
    "               '오늘 미세먼지 어때?']\n",
    "\n",
    "list_prompt = [PROMPT_DICT['prompt_input'].format_map({'prompt' : tmp}) for tmp in list_prompt]\n",
    "\n",
    "list_result = generator(list_prompt, **generation_args)   \n",
    "for prompt, result in zip(list_prompt, list_result):\n",
    "    print()\n",
    "    print((result[0]['generated_text']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b095205",
   "metadata": {},
   "source": [
    "- 답변이 훨씬 깔끔해 진 것을 볼 수 있다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a0a59aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563e0090",
   "metadata": {},
   "source": [
    "## RM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "585d8104",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from typing import Optional\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from chatgpt.dataset import RewardDataset\n",
    "from chatgpt.models.base import RewardModel\n",
    "from chatgpt.trainer import RewardModelTrainer\n",
    "from chatgpt.trainer.strategies import NaiveStrategy\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModel, AutoConfig\n",
    "from transformers.models.gpt2.configuration_gpt2 import GPT2Config\n",
    "from transformers.models.gpt2.modeling_gpt2 import GPT2Model\n",
    "import loralib as lora"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce17583",
   "metadata": {},
   "source": [
    "### Reward model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "962993fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTRM_custom(RewardModel):\n",
    "\n",
    "    def __init__(self,\n",
    "                 pretrained: Optional[str] = None,\n",
    "                 config: Optional[GPT2Config] = None,\n",
    "                 checkpoint: bool = False,\n",
    "                 lora_rank: int = 0,\n",
    "                 lora_train_bias: str = 'none',\n",
    "                 tokenizer=None) -> None:\n",
    "        if pretrained is not None:\n",
    "            model = GPT2Model.from_pretrained(pretrained)\n",
    "            model.resize_token_embeddings(len(tokenizer))\n",
    "        elif config is not None:\n",
    "            model = GPT2Model(config)\n",
    "        else:\n",
    "            model = GPT2Model(GPT2Config())\n",
    "        if checkpoint:\n",
    "            model.gradient_checkpointing_enable()\n",
    "\n",
    "        value_head = nn.Linear(model.config.n_embd, 1)\n",
    "        super().__init__(model, value_head, lora_rank, lora_train_bias)\n",
    "\n",
    "        if pretrained is not None:\n",
    "            self.model = model\n",
    "            self.pretrained = pretrained\n",
    "\n",
    "\n",
    "    def save_pretrained(self, dir):\n",
    "        if self.pretrained is not None:\n",
    "            self.model.save_pretrained(dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "db9656ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at skt/kogpt2-base-v2 were not used when initializing GPT2Model: ['lm_head.weight']\n",
      "- This IS expected if you are initializing GPT2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing GPT2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained('skt/kogpt2-base-v2')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    'skt/kogpt2-base-v2', bos_token='</s>', eos_token='</s>', unk_token='</s>', pad_token='</s>',\n",
    "    padding_side=\"right\",\n",
    "    model_max_length=512,\n",
    ")\n",
    "\n",
    "with NaiveStrategy().model_init_context():\n",
    "        model = GPTRM_custom(pretrained='skt/kogpt2-base-v2', lora_rank=0, tokenizer=tokenizer).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "84c177b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before data num: 10124\n",
      "after  data num: 30372\n",
      "data example: \n",
      "{'prompt': '원피스의 원래 주인이 누구였어', 'chosen': '일럼, 그', 'rejected': '요? 원피스의 원래 주인은 핑크 전복 볼몽 소사이어스 롱보우 왕자 (Pirate King Gol D. Roger) 입니다.'}\n"
     ]
    }
   ],
   "source": [
    "with open(rm_path, \"r\", encoding='utf-8-sig') as json_file:\n",
    "    list_data_dict = json.load(json_file)\n",
    "    list_data_dict = json.loads(list_data_dict)\n",
    "\n",
    "total_data_ranking2chosen = []\n",
    "for tmp in list_data_dict:\n",
    "    one_data_ranking2chosen = []\n",
    "\n",
    "    data = {}\n",
    "    data['prompt'] = tmp['prompt']\n",
    "    if tmp['ranking'][0] < tmp['ranking'][1]:\n",
    "        data['chosen'] = tmp['completion_0']\n",
    "        data['rejected'] = tmp['completion_1']\n",
    "    else:\n",
    "        data['chosen'] = tmp['completion_1']\n",
    "        data['rejected'] = tmp['completion_0']\n",
    "    one_data_ranking2chosen.append(data)\n",
    "\n",
    "    data = {}\n",
    "    data['prompt'] = tmp['prompt']\n",
    "    if tmp['ranking'][0] < tmp['ranking'][2]:\n",
    "        data['chosen'] = tmp['completion_0']\n",
    "        data['rejected'] = tmp['completion_2']\n",
    "    else:\n",
    "        data['chosen'] = tmp['completion_2']\n",
    "        data['rejected'] = tmp['completion_0']\n",
    "    one_data_ranking2chosen.append(data)\n",
    "\n",
    "    data = {}\n",
    "    data['prompt'] = tmp['prompt']\n",
    "    if tmp['ranking'][1] < tmp['ranking'][2]:\n",
    "        data['chosen'] = tmp['completion_1']\n",
    "        data['rejected'] = tmp['completion_2']\n",
    "    else:\n",
    "        data['chosen'] = tmp['completion_2']\n",
    "        data['rejected'] = tmp['completion_1']\n",
    "    one_data_ranking2chosen.append(data)\n",
    "\n",
    "\n",
    "\n",
    "    total_data_ranking2chosen.extend(one_data_ranking2chosen)\n",
    "\n",
    "print('before data num: %d'%(len(list_data_dict)))\n",
    "print('after  data num: %d'%(len(total_data_ranking2chosen)))\n",
    "print('data example: \\n%s'%total_data_ranking2chosen[45])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3e16c323",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_data_ranking2chosen = []\n",
    "\n",
    "for tmp in list_data_dict:\n",
    "    prompt = tmp['prompt']\n",
    "    ranking = tmp['ranking']\n",
    "\n",
    "    for index in range(1, len(ranking)):\n",
    "        n = ranking[0]\n",
    "        m = ranking[index]\n",
    "\n",
    "\n",
    "        data = {\n",
    "            'prompt': prompt,\n",
    "            'chosen': tmp['completion_{}'.format(n)],\n",
    "            'rejected': tmp['completion_{}'.format(m)]\n",
    "        }\n",
    "\n",
    "        total_data_ranking2chosen.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1dca87b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prompt': '나 정신차리게 말해줘', 'chosen': '뭐가 문제인지 구체적으로 알려줘. 그러면 함께 해결해 보자.', 'rejected': '사랑한다 이상말 다시 사랑한다 다시 다시 다시 다시 다시 다시 다시 다시 다시 다시 다시 다시 다시 다시 다시 다시 다시 다시 다시 다시 다시 다시 다시 다시 다시 다시 다시 다시 다시 다시 다시 다시'}\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "random.seed(230319)\n",
    "random.shuffle(total_data_ranking2chosen)\n",
    "print(total_data_ranking2chosen[45])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fe400834",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 1346.90it/s]\n",
      "100%|██████████| 200/200 [00:00<00:00, 1326.65it/s]\n"
     ]
    }
   ],
   "source": [
    "train_data = total_data_ranking2chosen[:1000] \n",
    "eval_data = total_data_ranking2chosen[1000:1200]\n",
    "\n",
    "print(len(train_data))\n",
    "print(len(eval_data))\n",
    "\n",
    "train_dataset = RewardDataset(train_data, tokenizer, 512)\n",
    "eval_dataset = RewardDataset(eval_data, tokenizer, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6e60f722",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######################################################################\n",
      "## prompt ##\n",
      "1986년 월드컵에서 우루과이가 1대 6으로 패했던 나라는 어디인가?\n",
      "######################################################################\n",
      "## chosen ##\n",
      "덴마크입니다.\n",
      "######################################################################\n",
      "## rejected ##\n",
      "독일\n"
     ]
    }
   ],
   "source": [
    "idx = 1\n",
    "print('#'*70)\n",
    "print('## prompt ##')\n",
    "print(train_data[idx]['prompt'])\n",
    "print('#'*70)\n",
    "print('## chosen ##')\n",
    "print(train_data[idx]['chosen'])\n",
    "print('#'*70)\n",
    "print('## rejected ##')\n",
    "print(train_data[idx]['rejected'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "97cdab9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = RewardModelTrainer(model=model,\n",
    "                             strategy=NaiveStrategy(),\n",
    "                             optim=Adam(model.parameters(), lr=5e-5),\n",
    "                             train_dataset=train_dataset,\n",
    "                             eval_dataset=eval_dataset,\n",
    "                             batch_size=4,\n",
    "                             max_epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a2f2067d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train epoch:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Train step of epoch 0:   0%|          | 0/250 [00:00<?, ?it/s]\u001B[A\n",
      "Train step of epoch 0:   0%|          | 1/250 [00:00<03:42,  1.12it/s]\u001B[A\n",
      "Train step of epoch 0:   0%|          | 1/250 [00:00<03:42,  1.12it/s, loss=0.843]\u001B[A\n",
      "Train step of epoch 0:   1%|          | 2/250 [00:01<03:36,  1.15it/s, loss=0.843]\u001B[A\n",
      "Train step of epoch 0:   1%|          | 2/250 [00:01<03:36,  1.15it/s, loss=0.483]\u001B[A\n",
      "Train step of epoch 0:   1%|          | 3/250 [00:02<03:34,  1.15it/s, loss=0.483]\u001B[A\n",
      "Train step of epoch 0:   1%|          | 3/250 [00:02<03:34,  1.15it/s, loss=0.451]\u001B[A\n",
      "Train step of epoch 0:   2%|▏         | 4/250 [00:03<03:32,  1.16it/s, loss=0.451]\u001B[A\n",
      "Train step of epoch 0:   2%|▏         | 4/250 [00:03<03:32,  1.16it/s, loss=0.341]\u001B[A\n",
      "Train step of epoch 0:   2%|▏         | 5/250 [00:04<03:31,  1.16it/s, loss=0.341]\u001B[A\n",
      "Train step of epoch 0:   2%|▏         | 5/250 [00:04<03:31,  1.16it/s, loss=0.196]\u001B[A\n",
      "Train step of epoch 0:   2%|▏         | 6/250 [00:05<03:29,  1.16it/s, loss=0.196]\u001B[A\n",
      "Train step of epoch 0:   2%|▏         | 6/250 [00:05<03:29,  1.16it/s, loss=0.00181]\u001B[A\n",
      "Train step of epoch 0:   3%|▎         | 7/250 [00:06<03:28,  1.17it/s, loss=0.00181]\u001B[A\n",
      "Train step of epoch 0:   3%|▎         | 7/250 [00:06<03:28,  1.17it/s, loss=0.2]    \u001B[A\n",
      "Train step of epoch 0:   3%|▎         | 8/250 [00:06<03:27,  1.16it/s, loss=0.2]\u001B[A\n",
      "Train step of epoch 0:   3%|▎         | 8/250 [00:06<03:27,  1.16it/s, loss=0.0213]\u001B[A\n",
      "Train step of epoch 0:   4%|▎         | 9/250 [00:07<03:27,  1.16it/s, loss=0.0213]\u001B[A\n",
      "Train step of epoch 0:   4%|▎         | 9/250 [00:07<03:27,  1.16it/s, loss=0.00318]\u001B[A\n",
      "Train step of epoch 0:   4%|▍         | 10/250 [00:08<03:26,  1.16it/s, loss=0.00318]\u001B[A\n",
      "Train step of epoch 0:   4%|▍         | 10/250 [00:08<03:26,  1.16it/s, loss=0.277]  \u001B[A\n",
      "Train step of epoch 0:   4%|▍         | 11/250 [00:09<03:25,  1.16it/s, loss=0.277]\u001B[A\n",
      "Train step of epoch 0:   4%|▍         | 11/250 [00:09<03:25,  1.16it/s, loss=3.45] \u001B[A\n",
      "Train step of epoch 0:   5%|▍         | 12/250 [00:10<03:24,  1.16it/s, loss=3.45]\u001B[A\n",
      "Train step of epoch 0:   5%|▍         | 12/250 [00:10<03:24,  1.16it/s, loss=0.173]\u001B[A\n",
      "Train step of epoch 0:   5%|▌         | 13/250 [00:11<03:23,  1.16it/s, loss=0.173]\u001B[A\n",
      "Train step of epoch 0:   5%|▌         | 13/250 [00:11<03:23,  1.16it/s, loss=0.415]\u001B[A\n",
      "Train step of epoch 0:   6%|▌         | 14/250 [00:12<03:22,  1.16it/s, loss=0.415]\u001B[A\n",
      "Train step of epoch 0:   6%|▌         | 14/250 [00:12<03:22,  1.16it/s, loss=0.389]\u001B[A\n",
      "Train step of epoch 0:   6%|▌         | 15/250 [00:12<03:22,  1.16it/s, loss=0.389]\u001B[A\n",
      "Train step of epoch 0:   6%|▌         | 15/250 [00:12<03:22,  1.16it/s, loss=0.216]\u001B[A\n",
      "Train step of epoch 0:   6%|▋         | 16/250 [00:13<03:21,  1.16it/s, loss=0.216]\u001B[A\n",
      "Train step of epoch 0:   6%|▋         | 16/250 [00:13<03:21,  1.16it/s, loss=0.293]\u001B[A\n",
      "Train step of epoch 0:   7%|▋         | 17/250 [00:14<03:20,  1.16it/s, loss=0.293]\u001B[A\n",
      "Train step of epoch 0:   7%|▋         | 17/250 [00:14<03:20,  1.16it/s, loss=0.305]\u001B[A\n",
      "Train step of epoch 0:   7%|▋         | 18/250 [00:15<03:20,  1.16it/s, loss=0.305]\u001B[A\n",
      "Train step of epoch 0:   7%|▋         | 18/250 [00:15<03:20,  1.16it/s, loss=0.55] \u001B[A\n",
      "Train step of epoch 0:   8%|▊         | 19/250 [00:16<03:19,  1.16it/s, loss=0.55]\u001B[A\n",
      "Train step of epoch 0:   8%|▊         | 19/250 [00:16<03:19,  1.16it/s, loss=0.237]\u001B[A\n",
      "Train step of epoch 0:   8%|▊         | 20/250 [00:17<03:18,  1.16it/s, loss=0.237]\u001B[A\n",
      "Train step of epoch 0:   8%|▊         | 20/250 [00:17<03:18,  1.16it/s, loss=0.11] \u001B[A\n",
      "Train step of epoch 0:   8%|▊         | 21/250 [00:18<03:17,  1.16it/s, loss=0.11]\u001B[A\n",
      "Train step of epoch 0:   8%|▊         | 21/250 [00:18<03:17,  1.16it/s, loss=0.195]\u001B[A\n",
      "Train step of epoch 0:   9%|▉         | 22/250 [00:18<03:17,  1.16it/s, loss=0.195]\u001B[A\n",
      "Train step of epoch 0:   9%|▉         | 22/250 [00:18<03:17,  1.16it/s, loss=0.0388]\u001B[A\n",
      "Train step of epoch 0:   9%|▉         | 23/250 [00:19<03:16,  1.16it/s, loss=0.0388]\u001B[A\n",
      "Train step of epoch 0:   9%|▉         | 23/250 [00:19<03:16,  1.16it/s, loss=0.699] \u001B[A\n",
      "Train step of epoch 0:  10%|▉         | 24/250 [00:20<03:15,  1.16it/s, loss=0.699]\u001B[A\n",
      "Train step of epoch 0:  10%|▉         | 24/250 [00:20<03:15,  1.16it/s, loss=0.181]\u001B[A\n",
      "Train step of epoch 0:  10%|█         | 25/250 [00:21<03:14,  1.16it/s, loss=0.181]\u001B[A\n",
      "Train step of epoch 0:  10%|█         | 25/250 [00:21<03:14,  1.16it/s, loss=0.0904]\u001B[A\n",
      "Train step of epoch 0:  10%|█         | 26/250 [00:22<03:14,  1.15it/s, loss=0.0904]\u001B[A\n",
      "Train step of epoch 0:  10%|█         | 26/250 [00:22<03:14,  1.15it/s, loss=0.0637]\u001B[A\n",
      "Train step of epoch 0:  11%|█         | 27/250 [00:23<03:13,  1.15it/s, loss=0.0637]\u001B[A\n",
      "Train step of epoch 0:  11%|█         | 27/250 [00:23<03:13,  1.15it/s, loss=0.453] \u001B[A\n",
      "Train step of epoch 0:  11%|█         | 28/250 [00:24<03:12,  1.15it/s, loss=0.453]\u001B[A\n",
      "Train step of epoch 0:  11%|█         | 28/250 [00:24<03:12,  1.15it/s, loss=0.319]\u001B[A\n",
      "Train step of epoch 0:  12%|█▏        | 29/250 [00:25<03:11,  1.15it/s, loss=0.319]\u001B[A\n",
      "Train step of epoch 0:  12%|█▏        | 29/250 [00:25<03:11,  1.15it/s, loss=0.0608]\u001B[A\n",
      "Train step of epoch 0:  12%|█▏        | 30/250 [00:25<03:10,  1.15it/s, loss=0.0608]\u001B[A\n",
      "Train step of epoch 0:  12%|█▏        | 30/250 [00:25<03:10,  1.15it/s, loss=0.0204]\u001B[A\n",
      "Train step of epoch 0:  12%|█▏        | 31/250 [00:26<03:09,  1.15it/s, loss=0.0204]\u001B[A\n",
      "Train step of epoch 0:  12%|█▏        | 31/250 [00:26<03:09,  1.15it/s, loss=0.118] \u001B[A\n",
      "Train step of epoch 0:  13%|█▎        | 32/250 [00:27<03:09,  1.15it/s, loss=0.118]\u001B[A\n",
      "Train step of epoch 0:  13%|█▎        | 32/250 [00:27<03:09,  1.15it/s, loss=0.000201]\u001B[A\n",
      "Train step of epoch 0:  13%|█▎        | 33/250 [00:28<03:08,  1.15it/s, loss=0.000201]\u001B[A\n",
      "Train step of epoch 0:  13%|█▎        | 33/250 [00:28<03:08,  1.15it/s, loss=0.88]    \u001B[A\n",
      "Train step of epoch 0:  14%|█▎        | 34/250 [00:29<03:07,  1.15it/s, loss=0.88]\u001B[A\n",
      "Train step of epoch 0:  14%|█▎        | 34/250 [00:29<03:07,  1.15it/s, loss=0.104]\u001B[A\n",
      "Train step of epoch 0:  14%|█▍        | 35/250 [00:30<03:06,  1.15it/s, loss=0.104]\u001B[A\n",
      "Train step of epoch 0:  14%|█▍        | 35/250 [00:30<03:06,  1.15it/s, loss=0.371]\u001B[A\n",
      "Train step of epoch 0:  14%|█▍        | 36/250 [00:31<03:05,  1.15it/s, loss=0.371]\u001B[A\n",
      "Train step of epoch 0:  14%|█▍        | 36/250 [00:31<03:05,  1.15it/s, loss=0.0112]\u001B[A\n",
      "Train step of epoch 0:  15%|█▍        | 37/250 [00:31<03:05,  1.15it/s, loss=0.0112]\u001B[A\n",
      "Train step of epoch 0:  15%|█▍        | 37/250 [00:32<03:05,  1.15it/s, loss=0.133] \u001B[A\n",
      "Train step of epoch 0:  15%|█▌        | 38/250 [00:32<03:04,  1.15it/s, loss=0.133]\u001B[A\n",
      "Train step of epoch 0:  15%|█▌        | 38/250 [00:32<03:04,  1.15it/s, loss=0.0293]\u001B[A\n",
      "Train step of epoch 0:  16%|█▌        | 39/250 [00:33<03:03,  1.15it/s, loss=0.0293]\u001B[A\n",
      "Train step of epoch 0:  16%|█▌        | 39/250 [00:33<03:03,  1.15it/s, loss=0.399] \u001B[A\n",
      "Train step of epoch 0:  16%|█▌        | 40/250 [00:34<03:02,  1.15it/s, loss=0.399]\u001B[A\n",
      "Train step of epoch 0:  16%|█▌        | 40/250 [00:34<03:02,  1.15it/s, loss=0.0208]\u001B[A\n",
      "Train step of epoch 0:  16%|█▋        | 41/250 [00:35<03:01,  1.15it/s, loss=0.0208]\u001B[A\n",
      "Train step of epoch 0:  16%|█▋        | 41/250 [00:35<03:01,  1.15it/s, loss=0.871] \u001B[A\n",
      "Train step of epoch 0:  17%|█▋        | 42/250 [00:36<03:00,  1.15it/s, loss=0.871]\u001B[A\n",
      "Train step of epoch 0:  17%|█▋        | 42/250 [00:36<03:00,  1.15it/s, loss=0.26] \u001B[A\n",
      "Train step of epoch 0:  17%|█▋        | 43/250 [00:37<03:00,  1.15it/s, loss=0.26]\u001B[A\n",
      "Train step of epoch 0:  17%|█▋        | 43/250 [00:37<03:00,  1.15it/s, loss=0.728]\u001B[A\n",
      "Train step of epoch 0:  18%|█▊        | 44/250 [00:38<02:59,  1.15it/s, loss=0.728]\u001B[A\n",
      "Train step of epoch 0:  18%|█▊        | 44/250 [00:38<02:59,  1.15it/s, loss=0.328]\u001B[A\n",
      "Train step of epoch 0:  18%|█▊        | 45/250 [00:38<02:58,  1.15it/s, loss=0.328]\u001B[A\n",
      "Train step of epoch 0:  18%|█▊        | 45/250 [00:38<02:58,  1.15it/s, loss=0.184]\u001B[A\n",
      "Train step of epoch 0:  18%|█▊        | 46/250 [00:39<02:57,  1.15it/s, loss=0.184]\u001B[A\n",
      "Train step of epoch 0:  18%|█▊        | 46/250 [00:39<02:57,  1.15it/s, loss=0.231]\u001B[A\n",
      "Train step of epoch 0:  19%|█▉        | 47/250 [00:40<02:56,  1.15it/s, loss=0.231]\u001B[A\n",
      "Train step of epoch 0:  19%|█▉        | 47/250 [00:40<02:56,  1.15it/s, loss=0.364]\u001B[A\n",
      "Train step of epoch 0:  19%|█▉        | 48/250 [00:41<02:56,  1.15it/s, loss=0.364]\u001B[A\n",
      "Train step of epoch 0:  19%|█▉        | 48/250 [00:41<02:56,  1.15it/s, loss=0.163]\u001B[A\n",
      "Train step of epoch 0:  20%|█▉        | 49/250 [00:42<02:55,  1.15it/s, loss=0.163]\u001B[A\n",
      "Train step of epoch 0:  20%|█▉        | 49/250 [00:42<02:55,  1.15it/s, loss=2.07] \u001B[A\n",
      "Train step of epoch 0:  20%|██        | 50/250 [00:43<02:54,  1.14it/s, loss=2.07]\u001B[A\n",
      "Train step of epoch 0:  20%|██        | 50/250 [00:43<02:54,  1.14it/s, loss=0.0867]\u001B[A\n",
      "Train step of epoch 0:  20%|██        | 51/250 [00:44<02:53,  1.14it/s, loss=0.0867]\u001B[A\n",
      "Train step of epoch 0:  20%|██        | 51/250 [00:44<02:53,  1.14it/s, loss=0.214] \u001B[A\n",
      "Train step of epoch 0:  21%|██        | 52/250 [00:45<02:53,  1.14it/s, loss=0.214]\u001B[A\n",
      "Train step of epoch 0:  21%|██        | 52/250 [00:45<02:53,  1.14it/s, loss=0.201]\u001B[A\n",
      "Train step of epoch 0:  21%|██        | 53/250 [00:45<02:52,  1.14it/s, loss=0.201]\u001B[A\n",
      "Train step of epoch 0:  21%|██        | 53/250 [00:45<02:52,  1.14it/s, loss=0.123]\u001B[A\n",
      "Train step of epoch 0:  22%|██▏       | 54/250 [00:46<02:51,  1.14it/s, loss=0.123]\u001B[A\n",
      "Train step of epoch 0:  22%|██▏       | 54/250 [00:46<02:51,  1.14it/s, loss=0.382]\u001B[A\n",
      "Train step of epoch 0:  22%|██▏       | 55/250 [00:47<02:50,  1.14it/s, loss=0.382]\u001B[A\n",
      "Train step of epoch 0:  22%|██▏       | 55/250 [00:47<02:50,  1.14it/s, loss=0.296]\u001B[A\n",
      "Train step of epoch 0:  22%|██▏       | 56/250 [00:48<02:49,  1.14it/s, loss=0.296]\u001B[A\n",
      "Train step of epoch 0:  22%|██▏       | 56/250 [00:48<02:49,  1.14it/s, loss=0.0589]\u001B[A\n",
      "Train step of epoch 0:  23%|██▎       | 57/250 [00:49<02:48,  1.14it/s, loss=0.0589]\u001B[A\n",
      "Train step of epoch 0:  23%|██▎       | 57/250 [00:49<02:48,  1.14it/s, loss=0.254] \u001B[A\n",
      "Train step of epoch 0:  23%|██▎       | 58/250 [00:50<02:47,  1.14it/s, loss=0.254]\u001B[A\n",
      "Train step of epoch 0:  23%|██▎       | 58/250 [00:50<02:47,  1.14it/s, loss=0.068]\u001B[A\n",
      "Train step of epoch 0:  24%|██▎       | 59/250 [00:51<02:47,  1.14it/s, loss=0.068]\u001B[A\n",
      "Train step of epoch 0:  24%|██▎       | 59/250 [00:51<02:47,  1.14it/s, loss=0.0698]\u001B[A\n",
      "Train step of epoch 0:  24%|██▍       | 60/250 [00:52<02:46,  1.14it/s, loss=0.0698]\u001B[A\n",
      "Train step of epoch 0:  24%|██▍       | 60/250 [00:52<02:46,  1.14it/s, loss=0.0685]\u001B[A\n",
      "Train step of epoch 0:  24%|██▍       | 61/250 [00:52<02:45,  1.14it/s, loss=0.0685]\u001B[A\n",
      "Train step of epoch 0:  24%|██▍       | 61/250 [00:52<02:45,  1.14it/s, loss=0.733] \u001B[A\n",
      "Train step of epoch 0:  25%|██▍       | 62/250 [00:53<02:44,  1.14it/s, loss=0.733]\u001B[A\n",
      "Train step of epoch 0:  25%|██▍       | 62/250 [00:53<02:44,  1.14it/s, loss=0.302]\u001B[A\n",
      "Train step of epoch 0:  25%|██▌       | 63/250 [00:54<02:43,  1.14it/s, loss=0.302]\u001B[A\n",
      "Train step of epoch 0:  25%|██▌       | 63/250 [00:54<02:43,  1.14it/s, loss=0.256]\u001B[A\n",
      "Train step of epoch 0:  26%|██▌       | 64/250 [00:55<02:42,  1.14it/s, loss=0.256]\u001B[A\n",
      "Train step of epoch 0:  26%|██▌       | 64/250 [00:55<02:42,  1.14it/s, loss=0.026]\u001B[A\n",
      "Train step of epoch 0:  26%|██▌       | 65/250 [00:56<02:42,  1.14it/s, loss=0.026]\u001B[A\n",
      "Train step of epoch 0:  26%|██▌       | 65/250 [00:56<02:42,  1.14it/s, loss=0.0278]\u001B[A\n",
      "Train step of epoch 0:  26%|██▋       | 66/250 [00:57<02:41,  1.14it/s, loss=0.0278]\u001B[A\n",
      "Train step of epoch 0:  26%|██▋       | 66/250 [00:57<02:41,  1.14it/s, loss=0.213] \u001B[A\n",
      "Train step of epoch 0:  27%|██▋       | 67/250 [00:58<02:40,  1.14it/s, loss=0.213]\u001B[A\n",
      "Train step of epoch 0:  27%|██▋       | 67/250 [00:58<02:40,  1.14it/s, loss=0.344]\u001B[A\n",
      "Train step of epoch 0:  27%|██▋       | 68/250 [00:59<02:39,  1.14it/s, loss=0.344]\u001B[A\n",
      "Train step of epoch 0:  27%|██▋       | 68/250 [00:59<02:39,  1.14it/s, loss=0.0397]\u001B[A\n",
      "Train step of epoch 0:  28%|██▊       | 69/250 [00:59<02:38,  1.14it/s, loss=0.0397]\u001B[A\n",
      "Train step of epoch 0:  28%|██▊       | 69/250 [00:59<02:38,  1.14it/s, loss=0.497] \u001B[A\n",
      "Train step of epoch 0:  28%|██▊       | 70/250 [01:00<02:37,  1.14it/s, loss=0.497]\u001B[A\n",
      "Train step of epoch 0:  28%|██▊       | 70/250 [01:00<02:37,  1.14it/s, loss=0.202]\u001B[A\n",
      "Train step of epoch 0:  28%|██▊       | 71/250 [01:01<02:37,  1.14it/s, loss=0.202]\u001B[A\n",
      "Train step of epoch 0:  28%|██▊       | 71/250 [01:01<02:37,  1.14it/s, loss=0.675]\u001B[A\n",
      "Train step of epoch 0:  29%|██▉       | 72/250 [01:02<02:36,  1.14it/s, loss=0.675]\u001B[A\n",
      "Train step of epoch 0:  29%|██▉       | 72/250 [01:02<02:36,  1.14it/s, loss=0.81] \u001B[A\n",
      "Train step of epoch 0:  29%|██▉       | 73/250 [01:03<02:35,  1.14it/s, loss=0.81]\u001B[A\n",
      "Train step of epoch 0:  29%|██▉       | 73/250 [01:03<02:35,  1.14it/s, loss=0.471]\u001B[A\n",
      "Train step of epoch 0:  30%|██▉       | 74/250 [01:04<02:34,  1.14it/s, loss=0.471]\u001B[A\n",
      "Train step of epoch 0:  30%|██▉       | 74/250 [01:04<02:34,  1.14it/s, loss=0.26] \u001B[A\n",
      "Train step of epoch 0:  30%|███       | 75/250 [01:05<02:33,  1.14it/s, loss=0.26]\u001B[A\n",
      "Train step of epoch 0:  30%|███       | 75/250 [01:05<02:33,  1.14it/s, loss=0.129]\u001B[A\n",
      "Train step of epoch 0:  30%|███       | 76/250 [01:06<02:32,  1.14it/s, loss=0.129]\u001B[A\n",
      "Train step of epoch 0:  30%|███       | 76/250 [01:06<02:32,  1.14it/s, loss=0.0757]\u001B[A\n",
      "Train step of epoch 0:  31%|███       | 77/250 [01:06<02:32,  1.14it/s, loss=0.0757]\u001B[A\n",
      "Train step of epoch 0:  31%|███       | 77/250 [01:07<02:32,  1.14it/s, loss=0.168] \u001B[A\n",
      "Train step of epoch 0:  31%|███       | 78/250 [01:07<02:31,  1.14it/s, loss=0.168]\u001B[A\n",
      "Train step of epoch 0:  31%|███       | 78/250 [01:07<02:31,  1.14it/s, loss=0.101]\u001B[A\n",
      "Train step of epoch 0:  32%|███▏      | 79/250 [01:08<02:30,  1.14it/s, loss=0.101]\u001B[A\n",
      "Train step of epoch 0:  32%|███▏      | 79/250 [01:08<02:30,  1.14it/s, loss=0.165]\u001B[A\n",
      "Train step of epoch 0:  32%|███▏      | 80/250 [01:09<02:29,  1.14it/s, loss=0.165]\u001B[A\n",
      "Train step of epoch 0:  32%|███▏      | 80/250 [01:09<02:29,  1.14it/s, loss=0.19] \u001B[A\n",
      "Train step of epoch 0:  32%|███▏      | 81/250 [01:10<02:28,  1.14it/s, loss=0.19]\u001B[A\n",
      "Train step of epoch 0:  32%|███▏      | 81/250 [01:10<02:28,  1.14it/s, loss=0.82]\u001B[A\n",
      "Train step of epoch 0:  33%|███▎      | 82/250 [01:11<02:27,  1.14it/s, loss=0.82]\u001B[A\n",
      "Train step of epoch 0:  33%|███▎      | 82/250 [01:11<02:27,  1.14it/s, loss=0.148]\u001B[A\n",
      "Train step of epoch 0:  33%|███▎      | 83/250 [01:12<02:26,  1.14it/s, loss=0.148]\u001B[A\n",
      "Train step of epoch 0:  33%|███▎      | 83/250 [01:12<02:26,  1.14it/s, loss=0.178]\u001B[A\n",
      "Train step of epoch 0:  34%|███▎      | 84/250 [01:13<02:26,  1.14it/s, loss=0.178]\u001B[A\n",
      "Train step of epoch 0:  34%|███▎      | 84/250 [01:13<02:26,  1.14it/s, loss=0.151]\u001B[A\n",
      "Train step of epoch 0:  34%|███▍      | 85/250 [01:14<02:25,  1.14it/s, loss=0.151]\u001B[A\n",
      "Train step of epoch 0:  34%|███▍      | 85/250 [01:14<02:25,  1.14it/s, loss=0.288]\u001B[A\n",
      "Train step of epoch 0:  34%|███▍      | 86/250 [01:14<02:24,  1.14it/s, loss=0.288]\u001B[A\n",
      "Train step of epoch 0:  34%|███▍      | 86/250 [01:14<02:24,  1.14it/s, loss=0.161]\u001B[A\n",
      "Train step of epoch 0:  35%|███▍      | 87/250 [01:15<02:23,  1.14it/s, loss=0.161]\u001B[A\n",
      "Train step of epoch 0:  35%|███▍      | 87/250 [01:15<02:23,  1.14it/s, loss=0.0444]\u001B[A\n",
      "Train step of epoch 0:  35%|███▌      | 88/250 [01:16<02:22,  1.14it/s, loss=0.0444]\u001B[A\n",
      "Train step of epoch 0:  35%|███▌      | 88/250 [01:16<02:22,  1.14it/s, loss=0.199] \u001B[A\n",
      "Train step of epoch 0:  36%|███▌      | 89/250 [01:17<02:21,  1.13it/s, loss=0.199]\u001B[A\n",
      "Train step of epoch 0:  36%|███▌      | 89/250 [01:17<02:21,  1.13it/s, loss=0.112]\u001B[A\n",
      "Train step of epoch 0:  36%|███▌      | 90/250 [01:18<02:20,  1.14it/s, loss=0.112]\u001B[A\n",
      "Train step of epoch 0:  36%|███▌      | 90/250 [01:18<02:20,  1.14it/s, loss=0.0592]\u001B[A\n",
      "Train step of epoch 0:  36%|███▋      | 91/250 [01:19<02:20,  1.13it/s, loss=0.0592]\u001B[A\n",
      "Train step of epoch 0:  36%|███▋      | 91/250 [01:19<02:20,  1.13it/s, loss=1.14]  \u001B[A\n",
      "Train step of epoch 0:  37%|███▋      | 92/250 [01:20<02:19,  1.13it/s, loss=1.14]\u001B[A\n",
      "Train step of epoch 0:  37%|███▋      | 92/250 [01:20<02:19,  1.13it/s, loss=0.124]\u001B[A\n",
      "Train step of epoch 0:  37%|███▋      | 93/250 [01:21<02:18,  1.14it/s, loss=0.124]\u001B[A\n",
      "Train step of epoch 0:  37%|███▋      | 93/250 [01:21<02:18,  1.14it/s, loss=0.0675]\u001B[A\n",
      "Train step of epoch 0:  38%|███▊      | 94/250 [01:21<02:17,  1.13it/s, loss=0.0675]\u001B[A\n",
      "Train step of epoch 0:  38%|███▊      | 94/250 [01:21<02:17,  1.13it/s, loss=0.00601]\u001B[A\n",
      "Train step of epoch 0:  38%|███▊      | 95/250 [01:22<02:16,  1.13it/s, loss=0.00601]\u001B[A\n",
      "Train step of epoch 0:  38%|███▊      | 95/250 [01:22<02:16,  1.13it/s, loss=0.00343]\u001B[A\n",
      "Train step of epoch 0:  38%|███▊      | 96/250 [01:23<02:16,  1.13it/s, loss=0.00343]\u001B[A\n",
      "Train step of epoch 0:  38%|███▊      | 96/250 [01:23<02:16,  1.13it/s, loss=0.576]  \u001B[A\n",
      "Train step of epoch 0:  39%|███▉      | 97/250 [01:24<02:15,  1.13it/s, loss=0.576]\u001B[A\n",
      "Train step of epoch 0:  39%|███▉      | 97/250 [01:24<02:15,  1.13it/s, loss=0.0943]\u001B[A\n",
      "Train step of epoch 0:  39%|███▉      | 98/250 [01:25<02:14,  1.13it/s, loss=0.0943]\u001B[A\n",
      "Train step of epoch 0:  39%|███▉      | 98/250 [01:25<02:14,  1.13it/s, loss=0.145] \u001B[A\n",
      "Train step of epoch 0:  40%|███▉      | 99/250 [01:26<02:13,  1.13it/s, loss=0.145]\u001B[A\n",
      "Train step of epoch 0:  40%|███▉      | 99/250 [01:26<02:13,  1.13it/s, loss=0.334]\u001B[A\n",
      "Train step of epoch 0:  40%|████      | 100/250 [01:27<02:12,  1.13it/s, loss=0.334]\u001B[A\n",
      "Train step of epoch 0:  40%|████      | 100/250 [01:27<02:12,  1.13it/s, loss=0.0606]\u001B[A\n",
      "Train step of epoch 0:  40%|████      | 101/250 [01:28<02:11,  1.13it/s, loss=0.0606]\u001B[A\n",
      "Train step of epoch 0:  40%|████      | 101/250 [01:28<02:11,  1.13it/s, loss=0.0019]\u001B[A\n",
      "Train step of epoch 0:  41%|████      | 102/250 [01:29<02:10,  1.13it/s, loss=0.0019]\u001B[A\n",
      "Train step of epoch 0:  41%|████      | 102/250 [01:29<02:10,  1.13it/s, loss=0.272] \u001B[A\n",
      "Train step of epoch 0:  41%|████      | 103/250 [01:29<02:10,  1.13it/s, loss=0.272]\u001B[A\n",
      "Train step of epoch 0:  41%|████      | 103/250 [01:29<02:10,  1.13it/s, loss=0.0355]\u001B[A\n",
      "Train step of epoch 0:  42%|████▏     | 104/250 [01:30<02:09,  1.13it/s, loss=0.0355]\u001B[A\n",
      "Train step of epoch 0:  42%|████▏     | 104/250 [01:30<02:09,  1.13it/s, loss=0.0664]\u001B[A\n",
      "Train step of epoch 0:  42%|████▏     | 105/250 [01:31<02:08,  1.13it/s, loss=0.0664]\u001B[A\n",
      "Train step of epoch 0:  42%|████▏     | 105/250 [01:31<02:08,  1.13it/s, loss=0.0371]\u001B[A\n",
      "Train step of epoch 0:  42%|████▏     | 106/250 [01:32<02:07,  1.13it/s, loss=0.0371]\u001B[A\n",
      "Train step of epoch 0:  42%|████▏     | 106/250 [01:32<02:07,  1.13it/s, loss=0.577] \u001B[A\n",
      "Train step of epoch 0:  43%|████▎     | 107/250 [01:33<02:06,  1.13it/s, loss=0.577]\u001B[A\n",
      "Train step of epoch 0:  43%|████▎     | 107/250 [01:33<02:06,  1.13it/s, loss=0.0282]\u001B[A\n",
      "Train step of epoch 0:  43%|████▎     | 108/250 [01:34<02:05,  1.13it/s, loss=0.0282]\u001B[A\n",
      "Train step of epoch 0:  43%|████▎     | 108/250 [01:34<02:05,  1.13it/s, loss=0.0318]\u001B[A\n",
      "Train step of epoch 0:  44%|████▎     | 109/250 [01:35<02:04,  1.13it/s, loss=0.0318]\u001B[A\n",
      "Train step of epoch 0:  44%|████▎     | 109/250 [01:35<02:04,  1.13it/s, loss=0.00121]\u001B[A\n",
      "Train step of epoch 0:  44%|████▍     | 110/250 [01:36<02:03,  1.13it/s, loss=0.00121]\u001B[A\n",
      "Train step of epoch 0:  44%|████▍     | 110/250 [01:36<02:03,  1.13it/s, loss=0.00451]\u001B[A\n",
      "Train step of epoch 0:  44%|████▍     | 111/250 [01:37<02:03,  1.13it/s, loss=0.00451]\u001B[A\n",
      "Train step of epoch 0:  44%|████▍     | 111/250 [01:37<02:03,  1.13it/s, loss=0.0667] \u001B[A\n",
      "Train step of epoch 0:  45%|████▍     | 112/250 [01:37<02:02,  1.13it/s, loss=0.0667]\u001B[A\n",
      "Train step of epoch 0:  45%|████▍     | 112/250 [01:37<02:02,  1.13it/s, loss=0.408] \u001B[A\n",
      "Train step of epoch 0:  45%|████▌     | 113/250 [01:38<02:01,  1.13it/s, loss=0.408]\u001B[A\n",
      "Train step of epoch 0:  45%|████▌     | 113/250 [01:38<02:01,  1.13it/s, loss=0.00788]\u001B[A\n",
      "Train step of epoch 0:  46%|████▌     | 114/250 [01:39<02:00,  1.13it/s, loss=0.00788]\u001B[A\n",
      "Train step of epoch 0:  46%|████▌     | 114/250 [01:39<02:00,  1.13it/s, loss=0.000754]\u001B[A\n",
      "Train step of epoch 0:  46%|████▌     | 115/250 [01:40<01:59,  1.13it/s, loss=0.000754]\u001B[A\n",
      "Train step of epoch 0:  46%|████▌     | 115/250 [01:40<01:59,  1.13it/s, loss=0.0019]  \u001B[A\n",
      "Train step of epoch 0:  46%|████▋     | 116/250 [01:41<01:58,  1.13it/s, loss=0.0019]\u001B[A\n",
      "Train step of epoch 0:  46%|████▋     | 116/250 [01:41<01:58,  1.13it/s, loss=0.351] \u001B[A\n",
      "Train step of epoch 0:  47%|████▋     | 117/250 [01:42<01:57,  1.13it/s, loss=0.351]\u001B[A\n",
      "Train step of epoch 0:  47%|████▋     | 117/250 [01:42<01:57,  1.13it/s, loss=0.000109]\u001B[A\n",
      "Train step of epoch 0:  47%|████▋     | 118/250 [01:43<01:56,  1.13it/s, loss=0.000109]\u001B[A\n",
      "Train step of epoch 0:  47%|████▋     | 118/250 [01:43<01:56,  1.13it/s, loss=5.96e-7] \u001B[A\n",
      "Train step of epoch 0:  48%|████▊     | 119/250 [01:44<01:55,  1.14it/s, loss=5.96e-7]\u001B[A\n",
      "Train step of epoch 0:  48%|████▊     | 119/250 [01:44<01:55,  1.14it/s, loss=0.00115]\u001B[A\n",
      "Train step of epoch 0:  48%|████▊     | 120/250 [01:44<01:53,  1.14it/s, loss=0.00115]\u001B[A\n",
      "Train step of epoch 0:  48%|████▊     | 120/250 [01:44<01:53,  1.14it/s, loss=0.000336]\u001B[A\n",
      "Train step of epoch 0:  48%|████▊     | 121/250 [01:45<01:52,  1.14it/s, loss=0.000336]\u001B[A\n",
      "Train step of epoch 0:  48%|████▊     | 121/250 [01:45<01:52,  1.14it/s, loss=0.0204]  \u001B[A\n",
      "Train step of epoch 0:  49%|████▉     | 122/250 [01:46<01:52,  1.14it/s, loss=0.0204]\u001B[A\n",
      "Train step of epoch 0:  49%|████▉     | 122/250 [01:46<01:52,  1.14it/s, loss=0.745] \u001B[A\n",
      "Train step of epoch 0:  49%|████▉     | 123/250 [01:47<01:51,  1.14it/s, loss=0.745]\u001B[A\n",
      "Train step of epoch 0:  49%|████▉     | 123/250 [01:47<01:51,  1.14it/s, loss=1.19e-7]\u001B[A\n",
      "Train step of epoch 0:  50%|████▉     | 124/250 [01:48<01:50,  1.14it/s, loss=1.19e-7]\u001B[A\n",
      "Train step of epoch 0:  50%|████▉     | 124/250 [01:48<01:50,  1.14it/s, loss=0.000619]\u001B[A\n",
      "Train step of epoch 0:  50%|█████     | 125/250 [01:49<01:49,  1.14it/s, loss=0.000619]\u001B[A\n",
      "Train step of epoch 0:  50%|█████     | 125/250 [01:49<01:49,  1.14it/s, loss=0.46]    \u001B[A\n",
      "Train step of epoch 0:  50%|█████     | 126/250 [01:50<01:49,  1.14it/s, loss=0.46]\u001B[A\n",
      "Train step of epoch 0:  50%|█████     | 126/250 [01:50<01:49,  1.14it/s, loss=0.0765]\u001B[A\n",
      "Train step of epoch 0:  51%|█████     | 127/250 [01:51<01:48,  1.13it/s, loss=0.0765]\u001B[A\n",
      "Train step of epoch 0:  51%|█████     | 127/250 [01:51<01:48,  1.13it/s, loss=0.113] \u001B[A\n",
      "Train step of epoch 0:  51%|█████     | 128/250 [01:51<01:47,  1.13it/s, loss=0.113]\u001B[A\n",
      "Train step of epoch 0:  51%|█████     | 128/250 [01:52<01:47,  1.13it/s, loss=9.08e-5]\u001B[A\n",
      "Train step of epoch 0:  52%|█████▏    | 129/250 [01:52<01:47,  1.13it/s, loss=9.08e-5]\u001B[A\n",
      "Train step of epoch 0:  52%|█████▏    | 129/250 [01:52<01:47,  1.13it/s, loss=0.981]  \u001B[A\n",
      "Train step of epoch 0:  52%|█████▏    | 130/250 [01:53<01:46,  1.13it/s, loss=0.981]\u001B[A\n",
      "Train step of epoch 0:  52%|█████▏    | 130/250 [01:53<01:46,  1.13it/s, loss=0.201]\u001B[A\n",
      "Train step of epoch 0:  52%|█████▏    | 131/250 [01:54<01:45,  1.13it/s, loss=0.201]\u001B[A\n",
      "Train step of epoch 0:  52%|█████▏    | 131/250 [01:54<01:45,  1.13it/s, loss=0.00126]\u001B[A\n",
      "Train step of epoch 0:  53%|█████▎    | 132/250 [01:55<01:44,  1.13it/s, loss=0.00126]\u001B[A\n",
      "Train step of epoch 0:  53%|█████▎    | 132/250 [01:55<01:44,  1.13it/s, loss=0.208]  \u001B[A\n",
      "Train step of epoch 0:  53%|█████▎    | 133/250 [01:56<01:43,  1.13it/s, loss=0.208]\u001B[A\n",
      "Train step of epoch 0:  53%|█████▎    | 133/250 [01:56<01:43,  1.13it/s, loss=0.0676]\u001B[A\n",
      "Train step of epoch 0:  54%|█████▎    | 134/250 [01:57<01:43,  1.13it/s, loss=0.0676]\u001B[A\n",
      "Train step of epoch 0:  54%|█████▎    | 134/250 [01:57<01:43,  1.13it/s, loss=0.468] \u001B[A\n",
      "Train step of epoch 0:  54%|█████▍    | 135/250 [01:58<01:42,  1.13it/s, loss=0.468]\u001B[A\n",
      "Train step of epoch 0:  54%|█████▍    | 135/250 [01:58<01:42,  1.13it/s, loss=0.0555]\u001B[A\n",
      "Train step of epoch 0:  54%|█████▍    | 136/250 [01:59<01:41,  1.13it/s, loss=0.0555]\u001B[A\n",
      "Train step of epoch 0:  54%|█████▍    | 136/250 [01:59<01:41,  1.13it/s, loss=0.224] \u001B[A\n",
      "Train step of epoch 0:  55%|█████▍    | 137/250 [01:59<01:40,  1.12it/s, loss=0.224]\u001B[A\n",
      "Train step of epoch 0:  55%|█████▍    | 137/250 [02:00<01:40,  1.12it/s, loss=0.363]\u001B[A\n",
      "Train step of epoch 0:  55%|█████▌    | 138/250 [02:00<01:39,  1.12it/s, loss=0.363]\u001B[A\n",
      "Train step of epoch 0:  55%|█████▌    | 138/250 [02:00<01:39,  1.12it/s, loss=0.00399]\u001B[A\n",
      "Train step of epoch 0:  56%|█████▌    | 139/250 [02:01<01:38,  1.12it/s, loss=0.00399]\u001B[A\n",
      "Train step of epoch 0:  56%|█████▌    | 139/250 [02:01<01:38,  1.12it/s, loss=0.018]  \u001B[A\n",
      "Train step of epoch 0:  56%|█████▌    | 140/250 [02:02<01:37,  1.12it/s, loss=0.018]\u001B[A\n",
      "Train step of epoch 0:  56%|█████▌    | 140/250 [02:02<01:37,  1.12it/s, loss=0.298]\u001B[A\n",
      "Train step of epoch 0:  56%|█████▋    | 141/250 [02:03<01:36,  1.12it/s, loss=0.298]\u001B[A\n",
      "Train step of epoch 0:  56%|█████▋    | 141/250 [02:03<01:36,  1.12it/s, loss=0.147]\u001B[A\n",
      "Train step of epoch 0:  57%|█████▋    | 142/250 [02:04<01:36,  1.12it/s, loss=0.147]\u001B[A\n",
      "Train step of epoch 0:  57%|█████▋    | 142/250 [02:04<01:36,  1.12it/s, loss=0.00765]\u001B[A\n",
      "Train step of epoch 0:  57%|█████▋    | 143/250 [02:05<01:35,  1.12it/s, loss=0.00765]\u001B[A\n",
      "Train step of epoch 0:  57%|█████▋    | 143/250 [02:05<01:35,  1.12it/s, loss=0.0171] \u001B[A\n",
      "Train step of epoch 0:  58%|█████▊    | 144/250 [02:06<01:34,  1.12it/s, loss=0.0171]\u001B[A\n",
      "Train step of epoch 0:  58%|█████▊    | 144/250 [02:06<01:34,  1.12it/s, loss=0.0607]\u001B[A\n",
      "Train step of epoch 0:  58%|█████▊    | 145/250 [02:07<01:33,  1.12it/s, loss=0.0607]\u001B[A\n",
      "Train step of epoch 0:  58%|█████▊    | 145/250 [02:07<01:33,  1.12it/s, loss=0.0369]\u001B[A\n",
      "Train step of epoch 0:  58%|█████▊    | 146/250 [02:08<01:32,  1.12it/s, loss=0.0369]\u001B[A\n",
      "Train step of epoch 0:  58%|█████▊    | 146/250 [02:08<01:32,  1.12it/s, loss=0.0152]\u001B[A\n",
      "Train step of epoch 0:  59%|█████▉    | 147/250 [02:08<01:31,  1.12it/s, loss=0.0152]\u001B[A\n",
      "Train step of epoch 0:  59%|█████▉    | 147/250 [02:08<01:31,  1.12it/s, loss=0.00941]\u001B[A\n",
      "Train step of epoch 0:  59%|█████▉    | 148/250 [02:09<01:30,  1.12it/s, loss=0.00941]\u001B[A\n",
      "Train step of epoch 0:  59%|█████▉    | 148/250 [02:09<01:30,  1.12it/s, loss=0.0128] \u001B[A\n",
      "Train step of epoch 0:  60%|█████▉    | 149/250 [02:10<01:29,  1.12it/s, loss=0.0128]\u001B[A\n",
      "Train step of epoch 0:  60%|█████▉    | 149/250 [02:10<01:29,  1.12it/s, loss=0.00192]\u001B[A\n",
      "Train step of epoch 0:  60%|██████    | 150/250 [02:11<01:28,  1.12it/s, loss=0.00192]\u001B[A\n",
      "Train step of epoch 0:  60%|██████    | 150/250 [02:11<01:28,  1.12it/s, loss=0.022]  \u001B[A\n",
      "Train step of epoch 0:  60%|██████    | 151/250 [02:12<01:28,  1.12it/s, loss=0.022]\u001B[A\n",
      "Train step of epoch 0:  60%|██████    | 151/250 [02:12<01:28,  1.12it/s, loss=0.308]\u001B[A\n",
      "Train step of epoch 0:  61%|██████    | 152/250 [02:13<01:26,  1.13it/s, loss=0.308]\u001B[A\n",
      "Train step of epoch 0:  61%|██████    | 152/250 [02:13<01:26,  1.13it/s, loss=0.00214]\u001B[A\n",
      "Train step of epoch 0:  61%|██████    | 153/250 [02:14<01:26,  1.13it/s, loss=0.00214]\u001B[A\n",
      "Train step of epoch 0:  61%|██████    | 153/250 [02:14<01:26,  1.13it/s, loss=0.638]  \u001B[A\n",
      "Train step of epoch 0:  62%|██████▏   | 154/250 [02:15<01:25,  1.12it/s, loss=0.638]\u001B[A\n",
      "Train step of epoch 0:  62%|██████▏   | 154/250 [02:15<01:25,  1.12it/s, loss=0.00965]\u001B[A\n",
      "Train step of epoch 0:  62%|██████▏   | 155/250 [02:16<01:24,  1.12it/s, loss=0.00965]\u001B[A\n",
      "Train step of epoch 0:  62%|██████▏   | 155/250 [02:16<01:24,  1.12it/s, loss=0.116]  \u001B[A\n",
      "Train step of epoch 0:  62%|██████▏   | 156/250 [02:16<01:23,  1.12it/s, loss=0.116]\u001B[A\n",
      "Train step of epoch 0:  62%|██████▏   | 156/250 [02:16<01:23,  1.12it/s, loss=1.14] \u001B[A\n",
      "Train step of epoch 0:  63%|██████▎   | 157/250 [02:17<01:22,  1.12it/s, loss=1.14]\u001B[A\n",
      "Train step of epoch 0:  63%|██████▎   | 157/250 [02:17<01:22,  1.12it/s, loss=0.0851]\u001B[A\n",
      "Train step of epoch 0:  63%|██████▎   | 158/250 [02:18<01:22,  1.12it/s, loss=0.0851]\u001B[A\n",
      "Train step of epoch 0:  63%|██████▎   | 158/250 [02:18<01:22,  1.12it/s, loss=0.0251]\u001B[A\n",
      "Train step of epoch 0:  64%|██████▎   | 159/250 [02:19<01:21,  1.12it/s, loss=0.0251]\u001B[A\n",
      "Train step of epoch 0:  64%|██████▎   | 159/250 [02:19<01:21,  1.12it/s, loss=0.0851]\u001B[A\n",
      "Train step of epoch 0:  64%|██████▍   | 160/250 [02:20<01:20,  1.12it/s, loss=0.0851]\u001B[A\n",
      "Train step of epoch 0:  64%|██████▍   | 160/250 [02:20<01:20,  1.12it/s, loss=0.151] \u001B[A\n",
      "Train step of epoch 0:  64%|██████▍   | 161/250 [02:21<01:19,  1.12it/s, loss=0.151]\u001B[A\n",
      "Train step of epoch 0:  64%|██████▍   | 161/250 [02:21<01:19,  1.12it/s, loss=0.587]\u001B[A\n",
      "Train step of epoch 0:  65%|██████▍   | 162/250 [02:22<01:18,  1.12it/s, loss=0.587]\u001B[A\n",
      "Train step of epoch 0:  65%|██████▍   | 162/250 [02:22<01:18,  1.12it/s, loss=0.0328]\u001B[A\n",
      "Train step of epoch 0:  65%|██████▌   | 163/250 [02:23<01:17,  1.12it/s, loss=0.0328]\u001B[A\n",
      "Train step of epoch 0:  65%|██████▌   | 163/250 [02:23<01:17,  1.12it/s, loss=0.198] \u001B[A\n",
      "Train step of epoch 0:  66%|██████▌   | 164/250 [02:24<01:16,  1.12it/s, loss=0.198]\u001B[A\n",
      "Train step of epoch 0:  66%|██████▌   | 164/250 [02:24<01:16,  1.12it/s, loss=0.19] \u001B[A\n",
      "Train step of epoch 0:  66%|██████▌   | 165/250 [02:24<01:15,  1.12it/s, loss=0.19]\u001B[A\n",
      "Train step of epoch 0:  66%|██████▌   | 165/250 [02:24<01:15,  1.12it/s, loss=0.0527]\u001B[A\n",
      "Train step of epoch 0:  66%|██████▋   | 166/250 [02:25<01:15,  1.12it/s, loss=0.0527]\u001B[A\n",
      "Train step of epoch 0:  66%|██████▋   | 166/250 [02:25<01:15,  1.12it/s, loss=0.146] \u001B[A\n",
      "Train step of epoch 0:  67%|██████▋   | 167/250 [02:26<01:14,  1.12it/s, loss=0.146]\u001B[A\n",
      "Train step of epoch 0:  67%|██████▋   | 167/250 [02:26<01:14,  1.12it/s, loss=0.278]\u001B[A\n",
      "Train step of epoch 0:  67%|██████▋   | 168/250 [02:27<01:13,  1.12it/s, loss=0.278]\u001B[A\n",
      "Train step of epoch 0:  67%|██████▋   | 168/250 [02:27<01:13,  1.12it/s, loss=0.255]\u001B[A\n",
      "Train step of epoch 0:  68%|██████▊   | 169/250 [02:28<01:12,  1.12it/s, loss=0.255]\u001B[A\n",
      "Train step of epoch 0:  68%|██████▊   | 169/250 [02:28<01:12,  1.12it/s, loss=0.363]\u001B[A\n",
      "Train step of epoch 0:  68%|██████▊   | 170/250 [02:29<01:11,  1.12it/s, loss=0.363]\u001B[A\n",
      "Train step of epoch 0:  68%|██████▊   | 170/250 [02:29<01:11,  1.12it/s, loss=0.0658]\u001B[A\n",
      "Train step of epoch 0:  68%|██████▊   | 171/250 [02:30<01:10,  1.12it/s, loss=0.0658]\u001B[A\n",
      "Train step of epoch 0:  68%|██████▊   | 171/250 [02:30<01:10,  1.12it/s, loss=0.149] \u001B[A\n",
      "Train step of epoch 0:  69%|██████▉   | 172/250 [02:31<01:09,  1.12it/s, loss=0.149]\u001B[A\n",
      "Train step of epoch 0:  69%|██████▉   | 172/250 [02:31<01:09,  1.12it/s, loss=0.362]\u001B[A\n",
      "Train step of epoch 0:  69%|██████▉   | 173/250 [02:32<01:08,  1.12it/s, loss=0.362]\u001B[A\n",
      "Train step of epoch 0:  69%|██████▉   | 173/250 [02:32<01:08,  1.12it/s, loss=0.00643]\u001B[A\n",
      "Train step of epoch 0:  70%|██████▉   | 174/250 [02:32<01:07,  1.12it/s, loss=0.00643]\u001B[A\n",
      "Train step of epoch 0:  70%|██████▉   | 174/250 [02:33<01:07,  1.12it/s, loss=0.107]  \u001B[A\n",
      "Train step of epoch 0:  70%|███████   | 175/250 [02:33<01:07,  1.12it/s, loss=0.107]\u001B[A\n",
      "Train step of epoch 0:  70%|███████   | 175/250 [02:33<01:07,  1.12it/s, loss=0.0435]\u001B[A\n",
      "Train step of epoch 0:  70%|███████   | 176/250 [02:34<01:06,  1.12it/s, loss=0.0435]\u001B[A\n",
      "Train step of epoch 0:  70%|███████   | 176/250 [02:34<01:06,  1.12it/s, loss=0.19]  \u001B[A\n",
      "Train step of epoch 0:  71%|███████   | 177/250 [02:35<01:05,  1.12it/s, loss=0.19]\u001B[A\n",
      "Train step of epoch 0:  71%|███████   | 177/250 [02:35<01:05,  1.12it/s, loss=0.0132]\u001B[A\n",
      "Train step of epoch 0:  71%|███████   | 178/250 [02:36<01:04,  1.12it/s, loss=0.0132]\u001B[A\n",
      "Train step of epoch 0:  71%|███████   | 178/250 [02:36<01:04,  1.12it/s, loss=0.0155]\u001B[A\n",
      "Train step of epoch 0:  72%|███████▏  | 179/250 [02:37<01:03,  1.12it/s, loss=0.0155]\u001B[A\n",
      "Train step of epoch 0:  72%|███████▏  | 179/250 [02:37<01:03,  1.12it/s, loss=0.0453]\u001B[A\n",
      "Train step of epoch 0:  72%|███████▏  | 180/250 [02:38<01:02,  1.12it/s, loss=0.0453]\u001B[A\n",
      "Train step of epoch 0:  72%|███████▏  | 180/250 [02:38<01:02,  1.12it/s, loss=0.00042]\u001B[A\n",
      "Train step of epoch 0:  72%|███████▏  | 181/250 [02:39<01:01,  1.12it/s, loss=0.00042]\u001B[A\n",
      "Train step of epoch 0:  72%|███████▏  | 181/250 [02:39<01:01,  1.12it/s, loss=0.00199]\u001B[A\n",
      "Train step of epoch 0:  73%|███████▎  | 182/250 [02:40<01:00,  1.12it/s, loss=0.00199]\u001B[A\n",
      "Train step of epoch 0:  73%|███████▎  | 182/250 [02:40<01:00,  1.12it/s, loss=0.035]  \u001B[A\n",
      "Train step of epoch 0:  73%|███████▎  | 183/250 [02:41<00:59,  1.12it/s, loss=0.035]\u001B[A\n",
      "Train step of epoch 0:  73%|███████▎  | 183/250 [02:41<00:59,  1.12it/s, loss=2.01] \u001B[A\n",
      "Train step of epoch 0:  74%|███████▎  | 184/250 [02:41<00:58,  1.12it/s, loss=2.01]\u001B[A\n",
      "Train step of epoch 0:  74%|███████▎  | 184/250 [02:41<00:58,  1.12it/s, loss=0.0155]\u001B[A\n",
      "Train step of epoch 0:  74%|███████▍  | 185/250 [02:42<00:58,  1.12it/s, loss=0.0155]\u001B[A\n",
      "Train step of epoch 0:  74%|███████▍  | 185/250 [02:42<00:58,  1.12it/s, loss=0.00632]\u001B[A\n",
      "Train step of epoch 0:  74%|███████▍  | 186/250 [02:43<00:57,  1.12it/s, loss=0.00632]\u001B[A\n",
      "Train step of epoch 0:  74%|███████▍  | 186/250 [02:43<00:57,  1.12it/s, loss=0.0177] \u001B[A\n",
      "Train step of epoch 0:  75%|███████▍  | 187/250 [02:44<00:56,  1.12it/s, loss=0.0177]\u001B[A\n",
      "Train step of epoch 0:  75%|███████▍  | 187/250 [02:44<00:56,  1.12it/s, loss=0.135] \u001B[A\n",
      "Train step of epoch 0:  75%|███████▌  | 188/250 [02:45<00:55,  1.12it/s, loss=0.135]\u001B[A\n",
      "Train step of epoch 0:  75%|███████▌  | 188/250 [02:45<00:55,  1.12it/s, loss=0.0196]\u001B[A\n",
      "Train step of epoch 0:  76%|███████▌  | 189/250 [02:46<00:54,  1.12it/s, loss=0.0196]\u001B[A\n",
      "Train step of epoch 0:  76%|███████▌  | 189/250 [02:46<00:54,  1.12it/s, loss=0.0236]\u001B[A\n",
      "Train step of epoch 0:  76%|███████▌  | 190/250 [02:47<00:53,  1.12it/s, loss=0.0236]\u001B[A\n",
      "Train step of epoch 0:  76%|███████▌  | 190/250 [02:47<00:53,  1.12it/s, loss=0.152] \u001B[A\n",
      "Train step of epoch 0:  76%|███████▋  | 191/250 [02:48<00:52,  1.12it/s, loss=0.152]\u001B[A\n",
      "Train step of epoch 0:  76%|███████▋  | 191/250 [02:48<00:52,  1.12it/s, loss=0.0872]\u001B[A\n",
      "Train step of epoch 0:  77%|███████▋  | 192/250 [02:49<00:51,  1.12it/s, loss=0.0872]\u001B[A\n",
      "Train step of epoch 0:  77%|███████▋  | 192/250 [02:49<00:51,  1.12it/s, loss=0.193] \u001B[A\n",
      "Train step of epoch 0:  77%|███████▋  | 193/250 [02:49<00:51,  1.12it/s, loss=0.193]\u001B[A\n",
      "Train step of epoch 0:  77%|███████▋  | 193/250 [02:50<00:51,  1.12it/s, loss=0.0245]\u001B[A\n",
      "Train step of epoch 0:  78%|███████▊  | 194/250 [02:50<00:50,  1.12it/s, loss=0.0245]\u001B[A\n",
      "Train step of epoch 0:  78%|███████▊  | 194/250 [02:50<00:50,  1.12it/s, loss=0.489] \u001B[A\n",
      "Train step of epoch 0:  78%|███████▊  | 195/250 [02:51<00:49,  1.12it/s, loss=0.489]\u001B[A\n",
      "Train step of epoch 0:  78%|███████▊  | 195/250 [02:51<00:49,  1.12it/s, loss=0.0282]\u001B[A\n",
      "Train step of epoch 0:  78%|███████▊  | 196/250 [02:52<00:48,  1.12it/s, loss=0.0282]\u001B[A\n",
      "Train step of epoch 0:  78%|███████▊  | 196/250 [02:52<00:48,  1.12it/s, loss=0.184] \u001B[A\n",
      "Train step of epoch 0:  79%|███████▉  | 197/250 [02:53<00:47,  1.12it/s, loss=0.184]\u001B[A\n",
      "Train step of epoch 0:  79%|███████▉  | 197/250 [02:53<00:47,  1.12it/s, loss=0.313]\u001B[A\n",
      "Train step of epoch 0:  79%|███████▉  | 198/250 [02:54<00:46,  1.12it/s, loss=0.313]\u001B[A\n",
      "Train step of epoch 0:  79%|███████▉  | 198/250 [02:54<00:46,  1.12it/s, loss=0.029]\u001B[A\n",
      "Train step of epoch 0:  80%|███████▉  | 199/250 [02:55<00:45,  1.12it/s, loss=0.029]\u001B[A\n",
      "Train step of epoch 0:  80%|███████▉  | 199/250 [02:55<00:45,  1.12it/s, loss=0.0258]\u001B[A\n",
      "Train step of epoch 0:  80%|████████  | 200/250 [02:56<00:44,  1.12it/s, loss=0.0258]\u001B[A\n",
      "Train step of epoch 0:  80%|████████  | 200/250 [02:56<00:44,  1.12it/s, loss=0.109] \u001B[A\n",
      "Train step of epoch 0:  80%|████████  | 201/250 [02:57<00:43,  1.12it/s, loss=0.109]\u001B[A\n",
      "Train step of epoch 0:  80%|████████  | 201/250 [02:57<00:43,  1.12it/s, loss=0.0719]\u001B[A\n",
      "Train step of epoch 0:  81%|████████  | 202/250 [02:58<00:43,  1.12it/s, loss=0.0719]\u001B[A\n",
      "Train step of epoch 0:  81%|████████  | 202/250 [02:58<00:43,  1.12it/s, loss=0.064] \u001B[A\n",
      "Train step of epoch 0:  81%|████████  | 203/250 [02:58<00:42,  1.12it/s, loss=0.064]\u001B[A\n",
      "Train step of epoch 0:  81%|████████  | 203/250 [02:58<00:42,  1.12it/s, loss=0.0177]\u001B[A\n",
      "Train step of epoch 0:  82%|████████▏ | 204/250 [02:59<00:41,  1.12it/s, loss=0.0177]\u001B[A\n",
      "Train step of epoch 0:  82%|████████▏ | 204/250 [02:59<00:41,  1.12it/s, loss=0.00304]\u001B[A\n",
      "Train step of epoch 0:  82%|████████▏ | 205/250 [03:00<00:40,  1.12it/s, loss=0.00304]\u001B[A\n",
      "Train step of epoch 0:  82%|████████▏ | 205/250 [03:00<00:40,  1.12it/s, loss=1.25]   \u001B[A\n",
      "Train step of epoch 0:  82%|████████▏ | 206/250 [03:01<00:39,  1.12it/s, loss=1.25]\u001B[A\n",
      "Train step of epoch 0:  82%|████████▏ | 206/250 [03:01<00:39,  1.12it/s, loss=0.268]\u001B[A\n",
      "Train step of epoch 0:  83%|████████▎ | 207/250 [03:02<00:38,  1.12it/s, loss=0.268]\u001B[A\n",
      "Train step of epoch 0:  83%|████████▎ | 207/250 [03:02<00:38,  1.12it/s, loss=0.00492]\u001B[A\n",
      "Train step of epoch 0:  83%|████████▎ | 208/250 [03:03<00:37,  1.12it/s, loss=0.00492]\u001B[A\n",
      "Train step of epoch 0:  83%|████████▎ | 208/250 [03:03<00:37,  1.12it/s, loss=0.954]  \u001B[A\n",
      "Train step of epoch 0:  84%|████████▎ | 209/250 [03:04<00:36,  1.12it/s, loss=0.954]\u001B[A\n",
      "Train step of epoch 0:  84%|████████▎ | 209/250 [03:04<00:36,  1.12it/s, loss=0.011]\u001B[A\n",
      "Train step of epoch 0:  84%|████████▍ | 210/250 [03:05<00:35,  1.12it/s, loss=0.011]\u001B[A\n",
      "Train step of epoch 0:  84%|████████▍ | 210/250 [03:05<00:35,  1.12it/s, loss=0.00307]\u001B[A\n",
      "Train step of epoch 0:  84%|████████▍ | 211/250 [03:06<00:34,  1.12it/s, loss=0.00307]\u001B[A\n",
      "Train step of epoch 0:  84%|████████▍ | 211/250 [03:06<00:34,  1.12it/s, loss=0.146]  \u001B[A\n",
      "Train step of epoch 0:  85%|████████▍ | 212/250 [03:07<00:34,  1.11it/s, loss=0.146]\u001B[A\n",
      "Train step of epoch 0:  85%|████████▍ | 212/250 [03:07<00:34,  1.11it/s, loss=0.00938]\u001B[A\n",
      "Train step of epoch 0:  85%|████████▌ | 213/250 [03:07<00:33,  1.12it/s, loss=0.00938]\u001B[A\n",
      "Train step of epoch 0:  85%|████████▌ | 213/250 [03:07<00:33,  1.12it/s, loss=0.46]   \u001B[A\n",
      "Train step of epoch 0:  86%|████████▌ | 214/250 [03:08<00:32,  1.12it/s, loss=0.46]\u001B[A\n",
      "Train step of epoch 0:  86%|████████▌ | 214/250 [03:08<00:32,  1.12it/s, loss=0.0711]\u001B[A\n",
      "Train step of epoch 0:  86%|████████▌ | 215/250 [03:09<00:31,  1.12it/s, loss=0.0711]\u001B[A\n",
      "Train step of epoch 0:  86%|████████▌ | 215/250 [03:09<00:31,  1.12it/s, loss=0.0133]\u001B[A\n",
      "Train step of epoch 0:  86%|████████▋ | 216/250 [03:10<00:30,  1.12it/s, loss=0.0133]\u001B[A\n",
      "Train step of epoch 0:  86%|████████▋ | 216/250 [03:10<00:30,  1.12it/s, loss=0.0403]\u001B[A\n",
      "Train step of epoch 0:  87%|████████▋ | 217/250 [03:11<00:29,  1.12it/s, loss=0.0403]\u001B[A\n",
      "Train step of epoch 0:  87%|████████▋ | 217/250 [03:11<00:29,  1.12it/s, loss=0.0225]\u001B[A\n",
      "Train step of epoch 0:  87%|████████▋ | 218/250 [03:12<00:28,  1.12it/s, loss=0.0225]\u001B[A\n",
      "Train step of epoch 0:  87%|████████▋ | 218/250 [03:12<00:28,  1.12it/s, loss=0.000153]\u001B[A\n",
      "Train step of epoch 0:  88%|████████▊ | 219/250 [03:13<00:27,  1.12it/s, loss=0.000153]\u001B[A\n",
      "Train step of epoch 0:  88%|████████▊ | 219/250 [03:13<00:27,  1.12it/s, loss=0.141]   \u001B[A\n",
      "Train step of epoch 0:  88%|████████▊ | 220/250 [03:14<00:26,  1.12it/s, loss=0.141]\u001B[A\n",
      "Train step of epoch 0:  88%|████████▊ | 220/250 [03:14<00:26,  1.12it/s, loss=0.0052]\u001B[A\n",
      "Train step of epoch 0:  88%|████████▊ | 221/250 [03:15<00:25,  1.12it/s, loss=0.0052]\u001B[A\n",
      "Train step of epoch 0:  88%|████████▊ | 221/250 [03:15<00:25,  1.12it/s, loss=0.218] \u001B[A\n",
      "Train step of epoch 0:  89%|████████▉ | 222/250 [03:15<00:25,  1.12it/s, loss=0.218]\u001B[A\n",
      "Train step of epoch 0:  89%|████████▉ | 222/250 [03:15<00:25,  1.12it/s, loss=0.132]\u001B[A\n",
      "Train step of epoch 0:  89%|████████▉ | 223/250 [03:16<00:24,  1.12it/s, loss=0.132]\u001B[A\n",
      "Train step of epoch 0:  89%|████████▉ | 223/250 [03:16<00:24,  1.12it/s, loss=0.26] \u001B[A\n",
      "Train step of epoch 0:  90%|████████▉ | 224/250 [03:17<00:23,  1.12it/s, loss=0.26]\u001B[A\n",
      "Train step of epoch 0:  90%|████████▉ | 224/250 [03:17<00:23,  1.12it/s, loss=0.311]\u001B[A\n",
      "Train step of epoch 0:  90%|█████████ | 225/250 [03:18<00:22,  1.12it/s, loss=0.311]\u001B[A\n",
      "Train step of epoch 0:  90%|█████████ | 225/250 [03:18<00:22,  1.12it/s, loss=0.262]\u001B[A\n",
      "Train step of epoch 0:  90%|█████████ | 226/250 [03:19<00:21,  1.12it/s, loss=0.262]\u001B[A\n",
      "Train step of epoch 0:  90%|█████████ | 226/250 [03:19<00:21,  1.12it/s, loss=0.0517]\u001B[A\n",
      "Train step of epoch 0:  91%|█████████ | 227/250 [03:20<00:20,  1.12it/s, loss=0.0517]\u001B[A\n",
      "Train step of epoch 0:  91%|█████████ | 227/250 [03:20<00:20,  1.12it/s, loss=0.0264]\u001B[A\n",
      "Train step of epoch 0:  91%|█████████ | 228/250 [03:21<00:19,  1.12it/s, loss=0.0264]\u001B[A\n",
      "Train step of epoch 0:  91%|█████████ | 228/250 [03:21<00:19,  1.12it/s, loss=0.00522]\u001B[A\n",
      "Train step of epoch 0:  92%|█████████▏| 229/250 [03:22<00:18,  1.12it/s, loss=0.00522]\u001B[A\n",
      "Train step of epoch 0:  92%|█████████▏| 229/250 [03:22<00:18,  1.12it/s, loss=0.265]  \u001B[A\n",
      "Train step of epoch 0:  92%|█████████▏| 230/250 [03:23<00:17,  1.12it/s, loss=0.265]\u001B[A\n",
      "Train step of epoch 0:  92%|█████████▏| 230/250 [03:23<00:17,  1.12it/s, loss=0.193]\u001B[A\n",
      "Train step of epoch 0:  92%|█████████▏| 231/250 [03:24<00:17,  1.12it/s, loss=0.193]\u001B[A\n",
      "Train step of epoch 0:  92%|█████████▏| 231/250 [03:24<00:17,  1.12it/s, loss=0.00248]\u001B[A\n",
      "Train step of epoch 0:  93%|█████████▎| 232/250 [03:24<00:16,  1.12it/s, loss=0.00248]\u001B[A\n",
      "Train step of epoch 0:  93%|█████████▎| 232/250 [03:24<00:16,  1.12it/s, loss=0.183]  \u001B[A\n",
      "Train step of epoch 0:  93%|█████████▎| 233/250 [03:25<00:15,  1.12it/s, loss=0.183]\u001B[A\n",
      "Train step of epoch 0:  93%|█████████▎| 233/250 [03:25<00:15,  1.12it/s, loss=0.607]\u001B[A\n",
      "Train step of epoch 0:  94%|█████████▎| 234/250 [03:26<00:14,  1.12it/s, loss=0.607]\u001B[A\n",
      "Train step of epoch 0:  94%|█████████▎| 234/250 [03:26<00:14,  1.12it/s, loss=0.0115]\u001B[A\n",
      "Train step of epoch 0:  94%|█████████▍| 235/250 [03:27<00:13,  1.12it/s, loss=0.0115]\u001B[A\n",
      "Train step of epoch 0:  94%|█████████▍| 235/250 [03:27<00:13,  1.12it/s, loss=0.221] \u001B[A\n",
      "Train step of epoch 0:  94%|█████████▍| 236/250 [03:28<00:12,  1.13it/s, loss=0.221]\u001B[A\n",
      "Train step of epoch 0:  94%|█████████▍| 236/250 [03:28<00:12,  1.13it/s, loss=0.0277]\u001B[A\n",
      "Train step of epoch 0:  95%|█████████▍| 237/250 [03:29<00:11,  1.12it/s, loss=0.0277]\u001B[A\n",
      "Train step of epoch 0:  95%|█████████▍| 237/250 [03:29<00:11,  1.12it/s, loss=0.0209]\u001B[A\n",
      "Train step of epoch 0:  95%|█████████▌| 238/250 [03:30<00:10,  1.12it/s, loss=0.0209]\u001B[A\n",
      "Train step of epoch 0:  95%|█████████▌| 238/250 [03:30<00:10,  1.12it/s, loss=0.00728]\u001B[A\n",
      "Train step of epoch 0:  96%|█████████▌| 239/250 [03:31<00:09,  1.12it/s, loss=0.00728]\u001B[A\n",
      "Train step of epoch 0:  96%|█████████▌| 239/250 [03:31<00:09,  1.12it/s, loss=0.0427] \u001B[A\n",
      "Train step of epoch 0:  96%|█████████▌| 240/250 [03:32<00:08,  1.12it/s, loss=0.0427]\u001B[A\n",
      "Train step of epoch 0:  96%|█████████▌| 240/250 [03:32<00:08,  1.12it/s, loss=0.218] \u001B[A\n",
      "Train step of epoch 0:  96%|█████████▋| 241/250 [03:32<00:08,  1.12it/s, loss=0.218]\u001B[A\n",
      "Train step of epoch 0:  96%|█████████▋| 241/250 [03:32<00:08,  1.12it/s, loss=0.00913]\u001B[A\n",
      "Train step of epoch 0:  97%|█████████▋| 242/250 [03:33<00:07,  1.12it/s, loss=0.00913]\u001B[A\n",
      "Train step of epoch 0:  97%|█████████▋| 242/250 [03:33<00:07,  1.12it/s, loss=0.0443] \u001B[A\n",
      "Train step of epoch 0:  97%|█████████▋| 243/250 [03:34<00:06,  1.11it/s, loss=0.0443]\u001B[A\n",
      "Train step of epoch 0:  97%|█████████▋| 243/250 [03:34<00:06,  1.11it/s, loss=0.272] \u001B[A\n",
      "Train step of epoch 0:  98%|█████████▊| 244/250 [03:35<00:05,  1.11it/s, loss=0.272]\u001B[A\n",
      "Train step of epoch 0:  98%|█████████▊| 244/250 [03:35<00:05,  1.11it/s, loss=0.127]\u001B[A\n",
      "Train step of epoch 0:  98%|█████████▊| 245/250 [03:36<00:04,  1.11it/s, loss=0.127]\u001B[A\n",
      "Train step of epoch 0:  98%|█████████▊| 245/250 [03:36<00:04,  1.11it/s, loss=0.0101]\u001B[A\n",
      "Train step of epoch 0:  98%|█████████▊| 246/250 [03:37<00:03,  1.12it/s, loss=0.0101]\u001B[A\n",
      "Train step of epoch 0:  98%|█████████▊| 246/250 [03:37<00:03,  1.12it/s, loss=0.000519]\u001B[A\n",
      "Train step of epoch 0:  99%|█████████▉| 247/250 [03:38<00:02,  1.11it/s, loss=0.000519]\u001B[A\n",
      "Train step of epoch 0:  99%|█████████▉| 247/250 [03:38<00:02,  1.11it/s, loss=0.0127]  \u001B[A\n",
      "Train step of epoch 0:  99%|█████████▉| 248/250 [03:39<00:01,  1.11it/s, loss=0.0127]\u001B[A\n",
      "Train step of epoch 0:  99%|█████████▉| 248/250 [03:39<00:01,  1.11it/s, loss=0.00959]\u001B[A\n",
      "Train step of epoch 0: 100%|█████████▉| 249/250 [03:40<00:00,  1.11it/s, loss=0.00959]\u001B[A\n",
      "Train step of epoch 0: 100%|█████████▉| 249/250 [03:40<00:00,  1.11it/s, loss=0.00118]\u001B[A\n",
      "Train step of epoch 0: 100%|██████████| 250/250 [03:41<00:00,  1.12it/s, loss=0.00118]\u001B[A\n",
      "Train epoch: 100%|██████████| 1/1 [03:56<00:00, 236.04s/it]0,  1.12it/s, loss=0.0718] \u001B[A\n",
      "Train step of epoch 0: 100%|██████████| 250/250 [03:56<00:00,  1.06it/s, loss=0.246, dist_mean=8.58]\u001B[A\n",
      "Train epoch: 100%|██████████| 1/1 [03:56<00:00, 236.04s/it]\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(use_lora=0)\n",
    "\n",
    "model.save_pretrained('aiffel/KoChatGPT/output_2_RM_cleaned')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "82be3170",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: 인공지능은 똥멍청이 입니다\n",
      "reward score: -1.4\n"
     ]
    }
   ],
   "source": [
    "def inference_RM(input_text):\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors='pt').to(\n",
    "        torch.cuda.current_device())\n",
    "    output = model(input_ids)\n",
    "    output_reward = output.cpu().detach().numpy()[0]\n",
    "\n",
    "    print('input: %s\\nreward score: %.1f'%(input_text, output_reward))\n",
    "\n",
    "    return output_reward\n",
    "\n",
    "input_text = '인공지능은 똥멍청이 입니다'\n",
    "output_reward = inference_RM(input_text=input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "14502431",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: 인공지능(AI)은 컴퓨터에서 음성 및 작성된 언어를 보고 이해하고 번역하고 데이터를 분석하고 추천하는 기능을 포함하여 다양한 고급 기능을 수행할 수 있는 일련의 기술입니다.\n",
      "reward score: -0.3\n"
     ]
    }
   ],
   "source": [
    "input_text = '인공지능(AI)은 컴퓨터에서 음성 및 작성된 언어를 보고 이해하고 번역하고 데이터를 분석하고 추천하는 기능을 포함하여 다양한 고급 기능을 수행할 수 있는 일련의 기술입니다.'\n",
    "\n",
    "output_reward = inference_RM(input_text=input_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0120b741",
   "metadata": {},
   "source": [
    "### **PPO**\n",
    "\n",
    "드디어 RLHF의 마지막 세번째 단계인 PPO를 실습해볼 차례입니다.\n",
    "\n",
    "사용할 라이브러리들을 불러오도록 하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0f347811",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "from chatgpt.models.base import RewardModel\n",
    "from chatgpt.models.gpt import GPTActor, GPTCritic\n",
    "from chatgpt.trainer import PPOTrainer\n",
    "from chatgpt.trainer.strategies import NaiveStrategy\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "00fe821d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with NaiveStrategy().model_init_context():\n",
    "    actor = GPTActor(pretrained='/aiffel/KoChatGPT/output_1_SFT_cleaned', lora_rank=0).to(torch.cuda.current_device())\n",
    "    critic = GPTCritic(pretrained='aiffel/KoChatGPT/output_2_RM_cleaned', lora_rank=0).to(torch.cuda.current_device())\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        'skt/kogpt2-base-v2', bos_token='</s>', eos_token='</s>', unk_token='</s>', pad_token='</s>',\n",
    "        padding_side=\"right\", \n",
    "        model_max_length=512\n",
    "    )\n",
    "\n",
    "    initial_model = deepcopy(actor)\n",
    "    reward_model = RewardModel(deepcopy(critic.model), deepcopy(critic.value_head)).to(torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e9ad12ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_optim = Adam(actor.parameters(), lr=5e-6)\n",
    "critic_optim = Adam(critic.parameters(), lr=5e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6a2c217a",
   "metadata": {},
   "outputs": [],
   "source": [
    "(actor, actor_optim), (critic, critic_optim), reward_model, initial_model = NaiveStrategy().prepare(\n",
    "    (actor, actor_optim), (critic, critic_optim), reward_model, initial_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1703ec3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(ppo_path, \"r\", encoding='utf-8-sig') as json_file:\n",
    "    list_data_dict = json.load(json_file)\n",
    "    list_data_dict = json.loads(list_data_dict)\n",
    "    list_prompt = [tmp['prompt'] for tmp in list_data_dict]\n",
    "\n",
    "def tokenize_fn(texts):\n",
    "    batch = tokenizer(texts, return_tensors='pt', max_length=96, padding=True, truncation=True)\n",
    "    return {k: v.cuda() for k, v in batch.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1d647af7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11984"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "98ca3137",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = PPOTrainer(NaiveStrategy(),\n",
    "                     actor,\n",
    "                     critic,\n",
    "                     reward_model,\n",
    "                     initial_model,\n",
    "                     actor_optim,\n",
    "                     critic_optim,\n",
    "                     max_epochs=1,  \n",
    "                     train_batch_size=8, \n",
    "                     tokenizer=tokenize_fn,\n",
    "                     max_length=128,\n",
    "                     do_sample=True,\n",
    "                     temperature=1.0,\n",
    "                     top_k=50,\n",
    "                     pad_token_id=tokenizer.pad_token_id,\n",
    "                     eos_token_id=tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "627dff77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode [1/10]:  67%|██████▋   | 2/3 [00:11<00:05,  5.59s/it]\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s]\u001B[A\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s, actor_loss=0.0642, critic_loss=0.00418]\u001B[A\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:00<00:01,  1.93it/s, actor_loss=0.0642, critic_loss=0.00418]\u001B[A\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:01<00:01,  1.93it/s, actor_loss=0.0667, critic_loss=0.031]  \u001B[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.94it/s, actor_loss=0.0667, critic_loss=0.031]\u001B[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.94it/s, actor_loss=0.0267, critic_loss=0.0297]\u001B[A\n",
      "Train epoch [1/1]: 100%|██████████| 3/3 [00:01<00:00,  1.93it/s, actor_loss=0.0267, critic_loss=0.0297]\u001B[A\n",
      "Episode [1/10]: 100%|██████████| 3/3 [00:18<00:00,  6.08s/it]\n",
      "Episode [2/10]:  67%|██████▋   | 2/3 [00:11<00:05,  5.64s/it]\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s]\u001B[A\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s, actor_loss=0.218, critic_loss=0.0333]\u001B[A\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:00<00:01,  1.91it/s, actor_loss=0.218, critic_loss=0.0333]\u001B[A\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:01<00:01,  1.91it/s, actor_loss=0.21, critic_loss=0.0286] \u001B[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.92it/s, actor_loss=0.21, critic_loss=0.0286]\u001B[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.92it/s, actor_loss=0.202, critic_loss=0.0165]\u001B[A\n",
      "Train epoch [1/1]: 100%|██████████| 3/3 [00:01<00:00,  1.92it/s, actor_loss=0.202, critic_loss=0.0165]\u001B[A\n",
      "Episode [2/10]: 100%|██████████| 3/3 [00:18<00:00,  6.15s/it]\n",
      "Episode [3/10]:  67%|██████▋   | 2/3 [00:11<00:05,  5.66s/it]\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s]\u001B[A\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s, actor_loss=-.0683, critic_loss=0.0339]\u001B[A\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:00<00:01,  1.91it/s, actor_loss=-.0683, critic_loss=0.0339]\u001B[A\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:01<00:01,  1.91it/s, actor_loss=0.0309, critic_loss=0.0108]\u001B[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.91it/s, actor_loss=0.0309, critic_loss=0.0108]\u001B[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.91it/s, actor_loss=-.0113, critic_loss=0.0189]\u001B[A\n",
      "Train epoch [1/1]: 100%|██████████| 3/3 [00:01<00:00,  1.90it/s, actor_loss=-.0113, critic_loss=0.0189]\u001B[A\n",
      "Episode [3/10]: 100%|██████████| 3/3 [00:18<00:00,  6.22s/it]\n",
      "Episode [4/10]:  67%|██████▋   | 2/3 [00:11<00:05,  5.66s/it]\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s]\u001B[A\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s, actor_loss=-.0457, critic_loss=0.0327]\u001B[A\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:00<00:01,  1.89it/s, actor_loss=-.0457, critic_loss=0.0327]\u001B[A\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:01<00:01,  1.89it/s, actor_loss=-.074, critic_loss=0.0126] \u001B[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.89it/s, actor_loss=-.074, critic_loss=0.0126]\u001B[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.89it/s, actor_loss=-.044, critic_loss=0.00913]\u001B[A\n",
      "Train epoch [1/1]: 100%|██████████| 3/3 [00:01<00:00,  1.89it/s, actor_loss=-.044, critic_loss=0.00913]\u001B[A\n",
      "Episode [4/10]: 100%|██████████| 3/3 [00:18<00:00,  6.21s/it]\n",
      "Episode [5/10]:  67%|██████▋   | 2/3 [00:11<00:05,  5.86s/it]\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s]\u001B[A\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s, actor_loss=0.14, critic_loss=0.0211]\u001B[A\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:00<00:01,  1.89it/s, actor_loss=0.14, critic_loss=0.0211]\u001B[A\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:01<00:01,  1.89it/s, actor_loss=0.0686, critic_loss=0.0117]\u001B[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.88it/s, actor_loss=0.0686, critic_loss=0.0117]\u001B[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.88it/s, actor_loss=0.0772, critic_loss=0.00712]\u001B[A\n",
      "Train epoch [1/1]: 100%|██████████| 3/3 [00:01<00:00,  1.88it/s, actor_loss=0.0772, critic_loss=0.00712]\u001B[A\n",
      "Episode [5/10]: 100%|██████████| 3/3 [00:19<00:00,  6.40s/it]\n",
      "Episode [6/10]:  67%|██████▋   | 2/3 [00:11<00:05,  5.77s/it]\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s]\u001B[A\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s, actor_loss=0.105, critic_loss=0.0153]\u001B[A\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:00<00:01,  1.88it/s, actor_loss=0.105, critic_loss=0.0153]\u001B[A\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:01<00:01,  1.88it/s, actor_loss=0.104, critic_loss=0.0121]\u001B[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.87it/s, actor_loss=0.104, critic_loss=0.0121]\u001B[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.87it/s, actor_loss=0.147, critic_loss=0.0277]\u001B[A\n",
      "Train epoch [1/1]: 100%|██████████| 3/3 [00:01<00:00,  1.86it/s, actor_loss=0.147, critic_loss=0.0277]\u001B[A\n",
      "Episode [6/10]: 100%|██████████| 3/3 [00:18<00:00,  6.27s/it]\n",
      "Episode [7/10]:  67%|██████▋   | 2/3 [00:11<00:05,  5.82s/it]\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s]\u001B[A\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s, actor_loss=-.0189, critic_loss=0.017]\u001B[A\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:00<00:01,  1.86it/s, actor_loss=-.0189, critic_loss=0.017]\u001B[A\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:01<00:01,  1.86it/s, actor_loss=0.0212, critic_loss=0.00727]\u001B[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.87it/s, actor_loss=0.0212, critic_loss=0.00727]\u001B[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.87it/s, actor_loss=0.0349, critic_loss=0.0239] \u001B[A\n",
      "Train epoch [1/1]: 100%|██████████| 3/3 [00:01<00:00,  1.86it/s, actor_loss=0.0349, critic_loss=0.0239]\u001B[A\n",
      "Episode [7/10]: 100%|██████████| 3/3 [00:19<00:00,  6.38s/it]\n",
      "Episode [8/10]:  67%|██████▋   | 2/3 [00:11<00:05,  5.74s/it]\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s]\u001B[A\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s, actor_loss=-.0186, critic_loss=0.00736]\u001B[A\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:00<00:01,  1.86it/s, actor_loss=-.0186, critic_loss=0.00736]\u001B[A\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:01<00:01,  1.86it/s, actor_loss=-.0662, critic_loss=0.00893]\u001B[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.85it/s, actor_loss=-.0662, critic_loss=0.00893]\u001B[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.85it/s, actor_loss=-.044, critic_loss=0.00221] \u001B[A\n",
      "Train epoch [1/1]: 100%|██████████| 3/3 [00:01<00:00,  1.85it/s, actor_loss=-.044, critic_loss=0.00221]\u001B[A\n",
      "Episode [8/10]: 100%|██████████| 3/3 [00:18<00:00,  6.26s/it]\n",
      "Episode [9/10]:  67%|██████▋   | 2/3 [00:11<00:05,  5.75s/it]\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s]\u001B[A\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s, actor_loss=0.0571, critic_loss=0.0104]\u001B[A\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:00<00:01,  1.85it/s, actor_loss=0.0571, critic_loss=0.0104]\u001B[A\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:01<00:01,  1.85it/s, actor_loss=0.0649, critic_loss=0.0128]\u001B[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.85it/s, actor_loss=0.0649, critic_loss=0.0128]\u001B[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.85it/s, actor_loss=0.0468, critic_loss=0.00839]\u001B[A\n",
      "Train epoch [1/1]: 100%|██████████| 3/3 [00:01<00:00,  1.85it/s, actor_loss=0.0468, critic_loss=0.00839]\u001B[A\n",
      "Episode [9/10]: 100%|██████████| 3/3 [00:19<00:00,  6.35s/it]\n",
      "Episode [10/10]:  67%|██████▋   | 2/3 [00:11<00:05,  5.98s/it]\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s]\u001B[A\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s, actor_loss=0.111, critic_loss=0.0121]\u001B[A\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:00<00:01,  1.84it/s, actor_loss=0.111, critic_loss=0.0121]\u001B[A\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:01<00:01,  1.84it/s, actor_loss=0.123, critic_loss=0.0168]\u001B[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.84it/s, actor_loss=0.123, critic_loss=0.0168]\u001B[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.84it/s, actor_loss=0.105, critic_loss=0.0093]\u001B[A\n",
      "Train epoch [1/1]: 100%|██████████| 3/3 [00:01<00:00,  1.84it/s, actor_loss=0.105, critic_loss=0.0093]\u001B[A\n",
      "Episode [10/10]: 100%|██████████| 3/3 [00:19<00:00,  6.52s/it]\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(list_prompt, \n",
    "            num_episodes=10,  \n",
    "            max_timesteps=3,\n",
    "            update_timesteps=3)\n",
    "\n",
    "model.save_pretrained('aiffel/KoChatGPT/output_3_PPO_cleaned')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "abb9df4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Instruction(명령어):\n",
      "불고기용 고기 한우에요?\n",
      "\n",
      "### Response(응답):불고기용 고기를 즐기는 것은 맛입니다. 따라서 고기를 먹을 때는 건강하고 영양가를 고려하여 선택합니다. 하지만 쇠고기 고기를 먹을 때는 쇠고기를 먹는 것보다 고기를 먹는 것이 좋습니다. 또한, 고기를 먹는 것은 건강에 대한 기본과 원칙도 중요합니다. 다만, 특정 음식을 먹으면 건강에 해로운 영향을 미칠 수 있으니 주의해야 합니다. 고기를 먹는 것은 좋지만, 쇠고기 고기를 먹을 때는 안전한\n",
      "\n",
      "### Instruction(명령어):\n",
      "리처드 닉슨이 43대 부통령직을 수행한 년도는?\n",
      "\n",
      "### Response(응답):리처드 닉슨이 44대 부통령직을 수행한 기간은 1945년 9월 5일부터 1945년 11월 15일까지입니다. American Interpersonal Tocusberne으로 불립니다. American Interpersonal Corporation)라는 용어는 이전에 미국 대통령이던 McDonnauff, Altbry\n",
      "\n",
      "### Instruction(명령어):\n",
      "시카고 오헤어 국제공항은 어디에 있어\n",
      "\n",
      "### Response(응답):시카고 오헤어 국제공항은 미국 캘리포니아주 샌프란시스코 국제공항에도 있습니다. 이 공항은 샌프란시스코 국제공항 근처의 샌디에이고 국제공항에도 있습니다. offici-later-enborin 국제공항에도 시카고 오헤어 국제공항이 있습니다. oftening-adular Inputes 국제공항(Jun\n",
      "\n",
      "### Instruction(명령어):\n",
      "오늘 미세먼지 어때?\n",
      "\n",
      "### Response(응답):저도 미세먼지 때문에 피부가 건조해지고 지치고 예민해질 수 있습니다. 하지만 각 부위마다 적절한 수분을 공급하여 피부를 청결히 유지하도록 도와주세요. 피부 건강을 유지하는 방법은 피부 청결제 복용과 충분한 수분과 수분 공급으로 이루어진 수분 공급. 또한 과도한 세정 및 노폐물 제거제를 사용하여 피부의 상태를 유지하는 것이 중요\n"
     ]
    }
   ],
   "source": [
    "def generation(input_text):\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors='pt').to(\n",
    "        torch.cuda.current_device())\n",
    "    outputs = actor.generate(input_ids,\n",
    "                             max_length=100,\n",
    "                             do_sample=True,\n",
    "                             top_k=50,\n",
    "                             top_p=0.95,\n",
    "                             num_return_sequences=1)\n",
    "    output = tokenizer.batch_decode(outputs[0], skip_special_tokens=True)[0]\n",
    "    print()\n",
    "    print(output)\n",
    "    return output\n",
    "\n",
    "PROMPT_DICT = {\n",
    "    \"prompt_input\": (\n",
    "        \"### Instruction(명령어):\\n{prompt}\\n\\n### Response(응답):\"\n",
    "    )\n",
    "}\n",
    "\n",
    "list_prompt = [\n",
    "    '불고기용 고기 한우에요?', \n",
    "    '리처드 닉슨이 43대 부통령직을 수행한 년도는?', \n",
    "    '시카고 오헤어 국제공항은 어디에 있어',\n",
    "    '오늘 미세먼지 어때?']\n",
    "\n",
    "list_prompt = [PROMPT_DICT['prompt_input'].format_map({'prompt': tmp}) for tmp in list_prompt]\n",
    "\n",
    "for input_text in list_prompt:\n",
    "    output = generation(input_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4597ae0b",
   "metadata": {},
   "source": [
    "## 새로운 데이터셋 추가\n",
    "KoChatGPT는 human feedback이 반영된 데이터를 직접 사용하는 대신\n",
    "ChatGPT API를 사용하는 대안을 선택했습니다.\n",
    "LLM Trend Note1 에서 살펴보았듯이\n",
    "Anthropic의 RLHF는 StackExchange 같은 온라인 상의 댓글정보를 활용하여\n",
    "ranking dataset을 구축해 구현되었습니다.\n",
    "우리도 비슷한 로직을 적용해볼 수 있습니다.\n",
    "\n",
    "하나의 prompt에 대한 다양한 수준의 품질로 댓글이 달린 한국어로 된 웹사이트를 찾아봅시다.\n",
    "웹크롤링 기법을 사용해 reward 점수를 차등적으로 적용해볼 수 있는\n",
    "instruction dataset과 ranking dataset을 구축해봅니다.\n",
    "\n",
    "KorQuAD 2.0 같은 한국어 이해 benchmark를 활용해 고품질의 데이터셋을 확보하고,\n",
    "KoGPT-2를 사용해 빠르게 저품질 데이터셋을 페어링해볼 수도 있습니다.\n",
    "다양한 데이터 증량전략을 구사하여 기존 데이터셋에 새로 구축한 데이터셋을 추가해\n",
    "모델을 재학습시키고 추론 결과를 비교해 분석하여 제시해보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5546b6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124cdf71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "042bc12d",
   "metadata": {},
   "source": [
    "## foundation model 교체\n",
    "현재 제공되는 LMS GPU 사양으로는 수십 billion 단위 이상의 LLM을 튜닝하기 어렵습니다.\n",
    "그러나 허깅페이스에서 제공하는 큰 규모의 모델을 적은 컴퓨팅 자원으로도 사용할 수 있게 해주는\n",
    "경량화, 최적화 라이브러리를 사용하면\n",
    "속도는 느리지만 우리의 LMS에서도 학습 및 추론이 가능해질 수 있습니다.\n",
    "(힌트 : LLM Trend Note1 노드의 마지막 스텝을 참고해보세요)\n",
    "\n",
    "허깅페이스에서 제공되는 1.2B 사이즈의 한국어 GPT pretrain model로 skt/ko-gpt-trinity-1.2B-v0.5 가 있습니다.\n",
    "해당 모델로 foundation model을 교체해보세요.\n",
    "(단 OOM 문제를 해소하기 위해 허깅페이스에서 제공하는\n",
    "다양한 training argument들을 조합하여 최상의 하이퍼파라미터를 찾아내야 합니다.)\n",
    "데이터셋을 아예 바꿔 모델 선택의 폭을 늘려보는 것도 좋은 선택지입니다.\n",
    "\n",
    "foundation model 교체에 성공했다면, generator 함수를 수정하여 모델 인퍼런스 결과를 제시해보세요.\n",
    "\n",
    "### 참고\n",
    "LLM Trend Note2 노드에서 살펴본 KoChatGPT 소스코드는\n",
    "빠르게 baseline모델을 설계해 실습해보기 위해 오리지널 코드를 일부 수정한 버전입니다.\n",
    "프로젝트 진행을 위해 모델을 커스터마이징할 때, 필요시 \"colossalai_ChatGPT_230319\" 폴더 내의 원본 스크립트들을 참고하세요."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "- 메모리 오류로 전혀 돌아가지 않음",
   "id": "9fac98a4e6c5970b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f112fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import pandas as pd\n",
    "import numpy\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# model_name = \"EleutherAI/polyglot-ko-1.3b\"\n",
    "model_name = \"skt/ko-gpt-trinity-1.2B-v0.5\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "## SFT\n",
    "import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.optim import Adam\n",
    "from datasets import load_dataset\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from copy import deepcopy\n",
    "import copy\n",
    "import logging\n",
    "import json\n",
    "from dataclasses import dataclass\n",
    "torch.cuda.memory_allocated()\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name, bos_token='</s>', eos_token='</s>', unk_token='</s>', pad_token='</s>',\n",
    "    padding_side=\"right\",\n",
    "    model_max_length=100,\n",
    ")\n",
    "\n",
    "print(tokenizer)\n",
    "from typing import Optional, Dict, Sequence\n",
    "\n",
    "class SFT_dataset(Dataset):\n",
    "\n",
    "    def __init__(self, data_path_1_SFT: str, tokenizer: transformers.PreTrainedTokenizer, verbose=False):\n",
    "        super(SFT_dataset, self).__init__()\n",
    "        logging.warning(\"Loading data...\")\n",
    "\n",
    "        pattern_instruction = 'prompt'  # instruction\n",
    "        pattern_output = 'completion'  # response\n",
    "\n",
    "        with open(data_path_1_SFT, \"r\", encoding='utf-8-sig') as json_file:\n",
    "            list_data_dict = json.load(json_file)\n",
    "            list_data_dict = json.loads(list_data_dict)\n",
    "\n",
    "        PROMPT_DICT = {\n",
    "            \"prompt_input\": (\n",
    "                \"### Instruction(명령어):\\n{prompt}\\n\\n### Response(응답):\"\n",
    "            )\n",
    "        }\n",
    "\n",
    "        prompt_input = PROMPT_DICT[\"prompt_input\"]\n",
    "\n",
    "        sources = []\n",
    "        for example in list_data_dict:\n",
    "            tmp = prompt_input.format_map(example)\n",
    "            sources.append(tmp)\n",
    "\n",
    "        targets = []\n",
    "        for example in list_data_dict:\n",
    "            targets.append(f\"{example[pattern_output]}{tokenizer.eos_token}\")\n",
    "        examples = [s + t for s, t in zip(sources, targets)]\n",
    "\n",
    "        sources_tokenized = self._tokenize_fn(sources, tokenizer)  # source\n",
    "        examples_tokenized = self._tokenize_fn(examples, tokenizer)  # source + target\n",
    "\n",
    "        input_ids = examples_tokenized[\"input_ids\"]\n",
    "        labels = copy.deepcopy(input_ids)\n",
    "        for label, source_len in zip(labels, sources_tokenized[\"input_ids_lens\"]):\n",
    "            label[:source_len] = -100\n",
    "\n",
    "        data_dict = dict(input_ids=input_ids, labels=labels)\n",
    "\n",
    "        self.input_ids = data_dict[\"input_ids\"]\n",
    "        self.labels = data_dict[\"labels\"]\n",
    "        logging.warning(\"Loading data done!!: %d\"%(len(self.labels)))\n",
    "\n",
    "\n",
    "    def _tokenize_fn(self, strings: Sequence[str], tokenizer: transformers.PreTrainedTokenizer) -> Dict:\n",
    "        tokenized_list = [\n",
    "            tokenizer(\n",
    "                text,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=\"longest\",\n",
    "                max_length=tokenizer.model_max_length,\n",
    "                truncation=True,\n",
    "            )\n",
    "            for text in strings\n",
    "        ]\n",
    "        input_ids = labels = [tokenized.input_ids[0] for tokenized in tokenized_list]\n",
    "        input_ids_lens = labels_lens = [\n",
    "            tokenized.input_ids.ne(tokenizer.pad_token_id).sum().item() for tokenized in tokenized_list\n",
    "        ]\n",
    "        return dict(\n",
    "            input_ids=input_ids,\n",
    "            labels=labels,\n",
    "            input_ids_lens=input_ids_lens,\n",
    "            labels_lens=labels_lens,\n",
    "        )\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "\n",
    "    def __getitem__(self, i) -> Dict[str, torch.Tensor]:\n",
    "        return dict(input_ids=self.input_ids[i], labels=self.labels[i])\n",
    "@dataclass\n",
    "class DataCollatorForSupervisedDataset(object): \n",
    "\n",
    "    tokenizer: transformers.PreTrainedTokenizer\n",
    "\n",
    "    def __call__(self, instances: Sequence[Dict]) -> Dict[str, torch.Tensor]:\n",
    "        input_ids, labels = tuple([instance[key] for instance in instances] for key in (\"input_ids\", \"labels\"))\n",
    "        input_ids = torch.nn.utils.rnn.pad_sequence(\n",
    "            input_ids, batch_first=True, padding_value=self.tokenizer.pad_token_id\n",
    "        )\n",
    "        labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value= -100)\n",
    "        return dict(\n",
    "            input_ids=input_ids,\n",
    "            labels=labels,\n",
    "            attention_mask=input_ids.ne(self.tokenizer.pad_token_id),\n",
    "        )\n",
    "sft_path = os.getenv('HOME')+'/aiffel/KoChatGPT/data_kochatgpt/kochatgpt_1_SFT_cleaned.jsonl' \n",
    "rm_path = os.getenv('HOME')+'/aiffel/KoChatGPT/data_kochatgpt/kochatgpt_2_RM_cleaned.jsonl'\n",
    "ppo_path = os.getenv('HOME')+'/aiffel/KoChatGPT/data_kochatgpt/kochatgpt_3_PPO_cleaned.jsonl'\n",
    "train_dataset = SFT_dataset(data_path_1_SFT=sft_path, tokenizer=tokenizer)\n",
    "data_collator = DataCollatorForSupervisedDataset(tokenizer=tokenizer)\n",
    "\n",
    "# print('input : %s'%train_dataset.input_ids[0])\n",
    "# print('output: %s'%train_dataset.labels[0])\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"garbage_collection_threshold:0.6,max_split_size_mb:14158451200\"\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"/aiffel/KoChatGPT/output_1_SFT_cleaned_ver2\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    warmup_steps=5,\n",
    "    prediction_loss_only=True,\n",
    "    fp16 = True\n",
    "    )\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset\n",
    ")\n",
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "trainer.train()\n",
    "model.save_pretrained('/aiffel/KoChatGPT/output_1_SFT_cleaned_ver2')\n",
    "generator = pipeline('text-generation', model='/aiffel/KoChatGPT/output_1_SFT_cleaned_ver2', tokenizer=tokenizer)\n",
    "\n",
    "generation_args = dict(   \n",
    "    num_beams=4,\n",
    "    repetition_penalty=2.0,\n",
    "    no_repeat_ngram_size=4,\n",
    "    eos_token_id=375, # \\n   \n",
    "    max_new_tokens=64,\n",
    "    do_sample=True,\n",
    "    top_k=50,\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "PROMPT_DICT = {\n",
    "    \"prompt_input\": (\n",
    "        \"### Instruction(명령어):\\n{prompt}\\n\\n### Response(응답):\"\n",
    "    )\n",
    "}\n",
    "\n",
    "list_prompt = ['불고기용 고기 한우에요?',\n",
    "               '리처드 닉슨이 43대 부통령직을 수행한 년도는?',\n",
    "               '시카고 오헤어 국제공항은 어디에 있어?',\n",
    "               '오늘 미세먼지 어때?']\n",
    "\n",
    "list_prompt = [PROMPT_DICT['prompt_input'].format_map({'prompt' : tmp}) for tmp in list_prompt]\n",
    "\n",
    "list_result = generator(list_prompt, **generation_args)   \n",
    "for prompt, result in zip(list_prompt, list_result):\n",
    "    print()\n",
    "    print((result[0]['generated_text']))\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "## RM\n",
    "import os\n",
    "import json\n",
    "from typing import Optional\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from chatgpt.dataset import RewardDataset\n",
    "from chatgpt.models.base import RewardModel\n",
    "from chatgpt.trainer import RewardModelTrainer\n",
    "from chatgpt.trainer.strategies import NaiveStrategy\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModel, AutoConfig\n",
    "from transformers.models.gpt2.configuration_gpt2 import GPT2Config\n",
    "from transformers.models.gpt2.modeling_gpt2 import GPT2Model\n",
    "import loralib as lora\n",
    "\n",
    "### Reward model\n",
    "class GPTRM_custom(RewardModel):\n",
    "\n",
    "    def __init__(self,\n",
    "                 pretrained: Optional[str] = None,\n",
    "                 config: Optional[GPT2Config] = None,\n",
    "                 checkpoint: bool = False,\n",
    "                 lora_rank: int = 0,\n",
    "                 lora_train_bias: str = 'none',\n",
    "                 tokenizer=None) -> None:\n",
    "        if pretrained is not None:\n",
    "            model = GPT2Model.from_pretrained(pretrained)\n",
    "            model.resize_token_embeddings(len(tokenizer))\n",
    "        elif config is not None:\n",
    "            model = GPT2Model(config)\n",
    "        else:\n",
    "            model = GPT2Model(GPT2Config())\n",
    "        if checkpoint:\n",
    "            model.gradient_checkpointing_enable()\n",
    "\n",
    "        value_head = nn.Linear(model.config.n_embd, 1)\n",
    "        super().__init__(model, value_head, lora_rank, lora_train_bias)\n",
    "\n",
    "        if pretrained is not None:\n",
    "            self.model = model\n",
    "            self.pretrained = pretrained\n",
    "\n",
    "\n",
    "    def save_pretrained(self, dir):\n",
    "        if self.pretrained is not None:\n",
    "            self.model.save_pretrained(dir)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name, bos_token='</s>', eos_token='</s>', unk_token='</s>', pad_token='</s>',\n",
    "    padding_side=\"right\",\n",
    "    model_max_length=100,\n",
    ")\n",
    "\n",
    "with NaiveStrategy().model_init_context():\n",
    "        model = GPTRM_custom(pretrained=model_name, lora_rank=0, tokenizer=tokenizer).cuda()\n",
    "with open(rm_path, \"r\", encoding='utf-8-sig') as json_file:\n",
    "    list_data_dict = json.load(json_file)\n",
    "    list_data_dict = json.loads(list_data_dict)\n",
    "\n",
    "total_data_ranking2chosen = []\n",
    "for tmp in list_data_dict:\n",
    "    one_data_ranking2chosen = []\n",
    "\n",
    "    data = {}\n",
    "    data['prompt'] = tmp['prompt']\n",
    "    if tmp['ranking'][0] < tmp['ranking'][1]:\n",
    "        data['chosen'] = tmp['completion_0']\n",
    "        data['rejected'] = tmp['completion_1']\n",
    "    else:\n",
    "        data['chosen'] = tmp['completion_1']\n",
    "        data['rejected'] = tmp['completion_0']\n",
    "    one_data_ranking2chosen.append(data)\n",
    "\n",
    "    data = {}\n",
    "    data['prompt'] = tmp['prompt']\n",
    "    if tmp['ranking'][0] < tmp['ranking'][2]:\n",
    "        data['chosen'] = tmp['completion_0']\n",
    "        data['rejected'] = tmp['completion_2']\n",
    "    else:\n",
    "        data['chosen'] = tmp['completion_2']\n",
    "        data['rejected'] = tmp['completion_0']\n",
    "    one_data_ranking2chosen.append(data)\n",
    "\n",
    "    data = {}\n",
    "    data['prompt'] = tmp['prompt']\n",
    "    if tmp['ranking'][1] < tmp['ranking'][2]:\n",
    "        data['chosen'] = tmp['completion_1']\n",
    "        data['rejected'] = tmp['completion_2']\n",
    "    else:\n",
    "        data['chosen'] = tmp['completion_2']\n",
    "        data['rejected'] = tmp['completion_1']\n",
    "    one_data_ranking2chosen.append(data)\n",
    "\n",
    "\n",
    "\n",
    "    total_data_ranking2chosen.extend(one_data_ranking2chosen)\n",
    "\n",
    "print('before data num: %d'%(len(list_data_dict)))\n",
    "print('after  data num: %d'%(len(total_data_ranking2chosen)))\n",
    "print('data example: \\n%s'%total_data_ranking2chosen[45])\n",
    "total_data_ranking2chosen = []\n",
    "\n",
    "for tmp in list_data_dict:\n",
    "    prompt = tmp['prompt']\n",
    "    ranking = tmp['ranking']\n",
    "\n",
    "    for index in range(1, len(ranking)):\n",
    "        n = ranking[0]\n",
    "        m = ranking[index]\n",
    "\n",
    "\n",
    "        data = {\n",
    "            'prompt': prompt,\n",
    "            'chosen': tmp['completion_{}'.format(n)],\n",
    "            'rejected': tmp['completion_{}'.format(m)]\n",
    "        }\n",
    "\n",
    "        total_data_ranking2chosen.append(data)\n",
    "import random\n",
    "random.seed(230319)\n",
    "random.shuffle(total_data_ranking2chosen)\n",
    "print(total_data_ranking2chosen[45])\n",
    "train_data = total_data_ranking2chosen[:1000] \n",
    "eval_data = total_data_ranking2chosen[1000:1200]\n",
    "\n",
    "print(len(train_data))\n",
    "print(len(eval_data))\n",
    "\n",
    "train_dataset = RewardDataset(train_data, tokenizer, 512)\n",
    "eval_dataset = RewardDataset(eval_data, tokenizer, 512)\n",
    "idx = 1\n",
    "print('#'*70)\n",
    "print('## prompt ##')\n",
    "print(train_data[idx]['prompt'])\n",
    "print('#'*70)\n",
    "print('## chosen ##')\n",
    "print(train_data[idx]['chosen'])\n",
    "print('#'*70)\n",
    "print('## rejected ##')\n",
    "print(train_data[idx]['rejected'])\n",
    "trainer = RewardModelTrainer(model=model,\n",
    "                             strategy=NaiveStrategy(),\n",
    "                             optim=Adam(model.parameters(), lr=5e-5),\n",
    "                             train_dataset=train_dataset,\n",
    "                             eval_dataset=eval_dataset,\n",
    "                             batch_size=4,\n",
    "                             max_epochs=1)\n",
    "trainer.fit(use_lora=0)\n",
    "\n",
    "model.save_pretrained('aiffel/KoChatGPT/output_2_RM_cleaned')\n",
    "def inference_RM(input_text):\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors='pt').to(\n",
    "        torch.cuda.current_device())\n",
    "    output = model(input_ids)\n",
    "    output_reward = output.cpu().detach().numpy()[0]\n",
    "\n",
    "    print('input: %s\\nreward score: %.1f'%(input_text, output_reward))\n",
    "\n",
    "    return output_reward\n",
    "\n",
    "input_text = '인공지능은 똥멍청이 입니다'\n",
    "output_reward = inference_RM(input_text=input_text)\n",
    "input_text = '인공지능(AI)은 컴퓨터에서 음성 및 작성된 언어를 보고 이해하고 번역하고 데이터를 분석하고 추천하는 기능을 포함하여 다양한 고급 기능을 수행할 수 있는 일련의 기술입니다.'\n",
    "\n",
    "output_reward = inference_RM(input_text=input_text)\n",
    "\n",
    "\n",
    "### **PPO**\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "from chatgpt.models.base import RewardModel\n",
    "from chatgpt.models.gpt import GPTActor, GPTCritic\n",
    "from chatgpt.trainer import PPOTrainer\n",
    "from chatgpt.trainer.strategies import NaiveStrategy\n",
    "from transformers import AutoTokenizer\n",
    "with NaiveStrategy().model_init_context():\n",
    "    actor = GPTActor(pretrained='/aiffel/KoChatGPT/output_1_SFT_cleaned', lora_rank=0).to(torch.cuda.current_device())\n",
    "    critic = GPTCritic(pretrained='aiffel/KoChatGPT/output_2_RM_cleaned', lora_rank=0).to(torch.cuda.current_device())\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        'skt/kogpt2-base-v2', bos_token='</s>', eos_token='</s>', unk_token='</s>', pad_token='</s>',\n",
    "        padding_side=\"right\", \n",
    "        model_max_length=512\n",
    "    )\n",
    "\n",
    "    initial_model = deepcopy(actor)\n",
    "    reward_model = RewardModel(deepcopy(critic.model), deepcopy(critic.value_head)).to(torch.cuda.current_device())\n",
    "actor_optim = Adam(actor.parameters(), lr=5e-6)\n",
    "critic_optim = Adam(critic.parameters(), lr=5e-6)\n",
    "(actor, actor_optim), (critic, critic_optim), reward_model, initial_model = NaiveStrategy().prepare(\n",
    "    (actor, actor_optim), (critic, critic_optim), reward_model, initial_model)\n",
    "with open(ppo_path, \"r\", encoding='utf-8-sig') as json_file:\n",
    "    list_data_dict = json.load(json_file)\n",
    "    list_data_dict = json.loads(list_data_dict)\n",
    "    list_prompt = [tmp['prompt'] for tmp in list_data_dict]\n",
    "\n",
    "def tokenize_fn(texts):\n",
    "    batch = tokenizer(texts, return_tensors='pt', max_length=96, padding=True, truncation=True)\n",
    "    return {k: v.cuda() for k, v in batch.items()}\n",
    "len(list_prompt)\n",
    "trainer = PPOTrainer(NaiveStrategy(),\n",
    "                     actor,\n",
    "                     critic,\n",
    "                     reward_model,\n",
    "                     initial_model,\n",
    "                     actor_optim,\n",
    "                     critic_optim,\n",
    "                     max_epochs=1,  \n",
    "                     train_batch_size=8, \n",
    "                     tokenizer=tokenize_fn,\n",
    "                     max_length=128,\n",
    "                     do_sample=True,\n",
    "                     temperature=1.0,\n",
    "                     top_k=50,\n",
    "                     pad_token_id=tokenizer.pad_token_id,\n",
    "                     eos_token_id=tokenizer.eos_token_id)\n",
    "trainer.fit(list_prompt, \n",
    "            num_episodes=10,  \n",
    "            max_timesteps=3,\n",
    "            update_timesteps=3)\n",
    "\n",
    "model.save_pretrained('aiffel/KoChatGPT/output_3_PPO_cleaned')\n",
    "def generation(input_text):\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors='pt').to(\n",
    "        torch.cuda.current_device())\n",
    "    outputs = actor.generate(input_ids,\n",
    "                             max_length=100,\n",
    "                             do_sample=True,\n",
    "                             top_k=50,\n",
    "                             top_p=0.95,\n",
    "                             num_return_sequences=1)\n",
    "    output = tokenizer.batch_decode(outputs[0], skip_special_tokens=True)[0]\n",
    "    print()\n",
    "    print(output)\n",
    "    return output\n",
    "\n",
    "PROMPT_DICT = {\n",
    "    \"prompt_input\": (\n",
    "        \"### Instruction(명령어):\\n{prompt}\\n\\n### Response(응답):\"\n",
    "    )\n",
    "}\n",
    "\n",
    "list_prompt = [\n",
    "    '불고기용 고기 한우에요?', \n",
    "    '리처드 닉슨이 43대 부통령직을 수행한 년도는?', \n",
    "    '시카고 오헤어 국제공항은 어디에 있어',\n",
    "    '오늘 미세먼지 어때?']\n",
    "\n",
    "list_prompt = [PROMPT_DICT['prompt_input'].format_map({'prompt': tmp}) for tmp in list_prompt]\n",
    "\n",
    "for input_text in list_prompt:\n",
    "    output = generation(input_text)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 회고\n",
    "- 배운 점\n",
    "    - chatGPT의 구조에 대해 알게 되었다\n",
    "- 아쉬운 점\n",
    "    - 계속 메모리 오류가 발생해 학습을 하지 못해서 아쉬웠다\n",
    "- 느낀 점\n",
    "    - 자원이 정말 많이 필요하다는 것을 느꼈다\n",
    "- 어려웠던 점\n",
    "    - lora나 메모리를 어떻게 효율적으로 쓸지에 대해서 잘 모르겠었다\n",
    "    - 한자를 안나오게 하기 위해 많은 노력을 기울였는데 어떻게 해야할지 잘 모르겠었다"
   ],
   "id": "1479f3714214378c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
