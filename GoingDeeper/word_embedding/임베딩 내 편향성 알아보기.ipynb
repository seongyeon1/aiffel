{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28208b25",
   "metadata": {},
   "source": [
    "# 임베딩 내 편향성 알아보기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eff99cc",
   "metadata": {},
   "source": [
    "## WEAT 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7c3122ce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-04T02:15:55.587220Z",
     "start_time": "2024-07-04T02:15:55.585021Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "3fbd089e",
   "metadata": {},
   "source": [
    "우선 두 개의 target 단어 셋 X, Y와 두 개의 attribute 단어 셋 A, B를 정의합니다.\n",
    "단어 셋을 정할 때는 두 개의 target 셋의 크기가 같아야 하고, 두 개의 attribute 셋의 크기가 같아야 합니다.\n",
    "\n",
    "###### targets\n",
    "- X set(꽃) : 장미, 튤립, 백합, 데이지\n",
    "- Y set(곤충) : 거미, 모기, 파리, 메뚜기\n",
    "\n",
    "###### attributes\n",
    "- A set(유쾌) : 사랑, 행복, 웃음\n",
    "- B set(불쾌) : 재난, 고통, 증오\n",
    "위 단어들의 임베딩 결과가 다음과 같다고 해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5718a60f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-04T02:17:43.177544Z",
     "start_time": "2024-07-04T02:17:43.168767Z"
    }
   },
   "source": [
    "target_X = {\n",
    "    '장미': [4.1, 1.2, -2.4, 0.5, 4.1],\n",
    "    '튤립': [3.1, 0.5, 3.6, 1.7, 5.8],\n",
    "    '백합': [2.9, -1.3, 0.4, 1.1, 3.7],\n",
    "    '데이지': [5.4, 2.5, 4.6, -1.0, 3.6]\n",
    "}\n",
    "target_Y = {\n",
    "    '거미': [-1.5, 0.2, -0.6, -4.6, -5.3],\n",
    "    '모기': [0.4, 0.7, -1.9, -4.5, -2.9],\n",
    "    '파리': [0.9, 1.4, -2.3, -3.9, -4.7],\n",
    "    '메뚜기': [0.7, 0.9, -0.4, -4.1, -3.9]\n",
    "}\n",
    "attribute_A = {\n",
    "    '사랑':[2.8,  4.2, 4.3,  0.3, 5.0],\n",
    "    '행복':[3.8,  3. , -1.2,  4.4, 4.9],\n",
    "    '웃음':[3.7, -0.3,  1.2, -2.5, 3.9]\n",
    "}\n",
    "attribute_B = {\n",
    "    '재난': [-0.2, -2.8, -4.7, -4.3, -4.7],\n",
    "    '고통': [-4.5, -2.1, -3.8, -3.6, -3.1],\n",
    "    '증오': [-3.6, -3.3, -3.5, -3.7, -4.4]\n",
    "}\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "09af01c4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-04T02:17:56.892358Z",
     "start_time": "2024-07-04T02:17:56.884724Z"
    }
   },
   "source": [
    "X = np.array([v for v in target_X.values()])\n",
    "Y = np.array([v for v in target_Y.values()])\n",
    "print(X)\n",
    "print(Y)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5f9abc2a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-04T02:18:09.913007Z",
     "start_time": "2024-07-04T02:18:09.885771Z"
    }
   },
   "source": [
    "A = np.array([v for v in attribute_A.values()])\n",
    "B = np.array([v for v in attribute_B.values()])\n",
    "print(A)\n",
    "print(B)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "42484d35",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-04T02:18:21.055670Z",
     "start_time": "2024-07-04T02:18:21.042783Z"
    }
   },
   "source": [
    "def cos_sim(i, j):\n",
    "    return dot(i, j.T)/(norm(i)*norm(j))\n",
    "\n",
    "def s(w, A, B):\n",
    "    c_a = cos_sim(w, A)\n",
    "    c_b = cos_sim(w, B)\n",
    "    mean_A = np.mean(c_a, axis=-1)\n",
    "    mean_B = np.mean(c_b, axis=-1)\n",
    "    return mean_A - mean_B #, c_a, c_b\n",
    "\n",
    "print(s(target_X['장미'], A, B))"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "9b4afd76",
   "metadata": {},
   "source": [
    "WEAT score값이 양수이므로, target_X에 있는 '장미'라는 단어는 attribute_B(불쾌)보다 attribute_A(유쾌)와 더 가깝다는 것을 알 수 있습니다. target_Y에 있는 '거미'와 attribute_A, attribute_B와의 관계도 계산해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ef3902b4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-04T02:18:27.396087Z",
     "start_time": "2024-07-04T02:18:27.392481Z"
    }
   },
   "source": [
    "print(s(target_Y['거미'], A, B))"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "2aed3e1d",
   "metadata": {},
   "source": [
    "위와 반대로 WEAT score가 음수가 나왔으므로, '거미'는 attribute_B와 더 가깝다는 것을 알 수 있습니다.\n",
    "\n",
    "그럼 target_X와 attribute_A, attribute_B 사의의 평균값, 그리고 target_Y와 attribute_A, attribute_B 사의의 평균값은 어떻게 될까요?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2059d5c3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-04T02:18:35.287341Z",
     "start_time": "2024-07-04T02:18:35.279736Z"
    }
   },
   "source": [
    "print(s(X, A, B))\n",
    "print(round(np.mean(s(X, A, B)), 3))"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "34703c41",
   "metadata": {},
   "source": [
    "target_X와 attribute_A, attribute_B 사이의 평균값은 0.397입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cd166b20",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-04T02:19:03.633813Z",
     "start_time": "2024-07-04T02:19:03.629915Z"
    }
   },
   "source": [
    "print(s(Y, A, B))\n",
    "print(round(np.mean(s(Y, A, B)), 3))"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "a138f222",
   "metadata": {},
   "source": [
    "target_Y와 attribute_A, attribute_B 사이의 평균값은 -0.33입니다.\n",
    "\n",
    "그럼 이번에는 WEAT score의 수식 전체를 코드로 나타내 봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c59b21a1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-04T02:20:10.225913Z",
     "start_time": "2024-07-04T02:20:10.203494Z"
    }
   },
   "source": [
    "def weat_score(X, Y, A, B):\n",
    "    \n",
    "    s_X = s(X, A, B)\n",
    "    s_Y = s(Y, A, B)\n",
    "\n",
    "    mean_X = np.mean(s_X)\n",
    "    mean_Y = np.mean(s_Y)\n",
    "    \n",
    "    std_dev = np.std(np.concatenate([s_X, s_Y], axis=0))\n",
    "    \n",
    "    return  (mean_X-mean_Y)/std_dev\n",
    "\n",
    "print(round(weat_score(X, Y, A, B), 3))"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "d3a91f23",
   "metadata": {},
   "source": [
    "WEAT score가 매우 높게 나온 것을 알 수 있습니다. 즉, 꽃은 유쾌한 단어와 상대적으로 가깝고, 곤충은 불쾌한 단어와 가깝다는 것을 수치적으로 확인할 수 있었습니다.\n",
    "\n",
    "이제 이를 시각적으로 확인해볼까요? PCA를 통해 5차원이었던 벡터를 2차원으로 줄여 그림을 그려보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e4b391c1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-04T02:20:26.422964Z",
     "start_time": "2024-07-04T02:20:25.949163Z"
    }
   },
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "pc_A = pca.fit_transform(A)\n",
    "pc_B = pca.fit_transform(B)\n",
    "pc_X = pca.fit_transform(X)\n",
    "pc_Y = pca.fit_transform(Y)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(pc_A[:,0],pc_A[:,1], c='blue', label='A')\n",
    "ax.scatter(pc_B[:,0],pc_B[:,1], c='red', label='B')\n",
    "ax.scatter(pc_X[:,0],pc_X[:,1], c='skyblue', label='X')\n",
    "ax.scatter(pc_Y[:,0],pc_Y[:,1], c='pink', label='Y')"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "7af3fb9b",
   "metadata": {},
   "source": [
    "## 사전학습된 Word Embedding에 WEAT 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e93bb97",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-04T02:18:36.098268Z",
     "start_time": "2024-07-04T02:18:36.096437Z"
    }
   },
   "source": [
    "import os\n",
    "\n",
    "data_dir = '~/aiffel/weat' \n",
    "model_dir = os.path.join(data_dir, 'GoogleNews-vectors-negative300.bin')\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# 50만개의 단어만 활용합니다. 메모리가 충분하다면 limit 파라미터값을 생략하여 300만개를 모두 활용할 수 있습니다. \n",
    "w2v = KeyedVectors.load_word2vec_format(model_dir, binary=True, limit=500000)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40cd93b0",
   "metadata": {},
   "source": [
    "w2v"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "b9daebc0",
   "metadata": {},
   "source": [
    "w2v에 있는 단어 개수와 벡터 크기를 살펴볼까요?\n",
    "\n",
    "💡 참고\n",
    "2021년 3월, Gensim이 4.0 으로 버전업되면서 KeyedVectors에 vocab dict가 제거되었습니다. 상세한 내용은 아래 링크를 참고해 주세요.\n",
    "\n",
    "Migrating from Gensim 3.x to 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81cf31df",
   "metadata": {},
   "source": [
    "# print(len(w2v.vocab))   # Gensim 3.X 버전까지는 w2v.vocab을 직접 접근할 수 있습니다. \n",
    "print(len(w2v.index_to_key))   # Gensim 4.0부터는 index_to_key를 활용해 vocab size를 알 수 있습니다. \n",
    "print(len(w2v['I']))                    # 혹은 단어를 key로 직접 vector를 얻을 수 있습니다. \n",
    "print(w2v.vectors.shape)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0798dd4",
   "metadata": {},
   "source": [
    "w2v['happy']"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65718d41",
   "metadata": {},
   "source": [
    "w2v.most_similar(positive=['happy'])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "48020a8c",
   "metadata": {},
   "source": [
    "w2v.most_similar(positive=['family'])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23673a83",
   "metadata": {},
   "source": [
    "w2v.most_similar(positive=['school'])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a58b21af",
   "metadata": {},
   "source": [
    "target_X = ['science', 'technology', 'physics', 'chemistry', 'Einstein', 'NASA', 'experiment', 'astronomy']\n",
    "target_Y = ['poetry', 'art', 'Shakespeare', 'dance', 'literature', 'novel', 'symphony', 'drama']\n",
    "attribute_A = ['brother', 'father', 'uncle', 'grandfather', 'son', 'he', 'his', 'him']\n",
    "attribute_B = ['sister', 'mother', 'aunt', 'grandmother', 'daughter', 'she', 'hers', 'her']\n",
    "\n",
    "X = np.array([w2v[word] for word in target_X])\n",
    "Y = np.array([w2v[word] for word in target_Y])\n",
    "A = np.array([w2v[word] for word in attribute_A])\n",
    "B = np.array([w2v[word] for word in attribute_B])\n",
    "\n",
    "weat_score(X, Y, A, B)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "6439e997",
   "metadata": {},
   "source": [
    "과학과 관련된 단어가 남성과 관련된 단어와 가깝고, 예술과 관련된 단어가 여성과 관련된 단어와 가깝게 나타났습니다. 사람의 편향성을 실험하는 IAT에서도 이와 같게 나타났었죠? 많은 사람이 가진 편향이 임베딩 모델에 반영되었다고 볼 수 있습니다.\n",
    "\n",
    "이제 다른 셋을 구성해볼까요? target_X는 인스턴트 식품들로 단어를 구성하였고 target_Y는 그 반대로 구성했습니다. attribute_A는 인스턴트를 의미하는 단어들로, attributes_B는 그 반대로 구성했습니다.\n",
    "\n",
    "이 단어 셋들을 보면 target_X는 attribute_A와 attribute_B 중 어떤 것과 가깝다고 생각하시나요?\n",
    "보통 target_X와 attribute_A가 가깝고, traget_Y는 attribute_B와 가깝다고 대답할 것입니다.\n",
    "임베딩 모델도 그렇게 생각할까요?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4ad8ffdd",
   "metadata": {},
   "source": [
    "target_X = ['pizza', 'coke', 'hamburger', 'ham', 'ramen', 'icecream', 'candy']\n",
    "target_Y = ['salad', 'fruit', 'vegetable', 'herb', 'root', 'greens', 'wholesome']\n",
    "attribute_A = ['junk', 'canned', 'convenience', 'frozen', 'fast']\n",
    "attribute_B = ['health', 'beneficial', 'good', 'nourishing', 'nutritious']\n",
    "\n",
    "X = np.array([w2v[word] for word in target_X])\n",
    "Y = np.array([w2v[word] for word in target_Y])\n",
    "A = np.array([w2v[word] for word in attribute_A])\n",
    "B = np.array([w2v[word] for word in attribute_B])\n",
    "\n",
    "weat_score(X, Y, A, B)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "9275996a",
   "metadata": {},
   "source": [
    "모델도 우리의 예상과 맞는 방향으로 상당히 높은 수치를 보이는 것을 확인했습니다. 인스턴트 식품의 예시와 인스턴트를 의미하는 단어가 가까운 것은 당연합니다. 이 경우 모델이 편향되어있다기보다 단어의 의미를 잘 파악했다고 볼 수 있습니다.\n",
    "\n",
    "동일한 target 셋에 다른 attribute 셋을 만들볼까요? attribute_A에는 책과 관련된 단어로 구성하고, attribute_B는 뉴스와 관련된 단어로 구성했습니다. 이번에는 어떤 결과를 가져올까요?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e5887ccd",
   "metadata": {},
   "source": [
    "target_X = ['pizza', 'coke', 'hamburger', 'ham', 'ramen', 'icecream', 'candy']\n",
    "target_Y = ['salad', 'fruit', 'vegetable', 'herb', 'root', 'greens', 'wholesome']\n",
    "attribute_A = ['book', 'essay', 'dictionary', 'magazine', 'novel']\n",
    "attribute_B = ['news', 'report', 'statement', 'broadcast', 'word']\n",
    "\n",
    "X = np.array([w2v[word] for word in target_X])\n",
    "Y = np.array([w2v[word] for word in target_Y])\n",
    "A = np.array([w2v[word] for word in attribute_A])\n",
    "B = np.array([w2v[word] for word in attribute_B])\n",
    "\n",
    "weat_score(X, Y, A, B)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "ba2e3fae",
   "metadata": {},
   "source": [
    "0에 굉장히 가까운 결과를 보였습니다. 즉, 임베딩 모델이 판단하기에 어느 것끼리 가깝다고 말할 수 없는 것이지요.\n",
    "\n",
    "여러분이 target, attribute 셋을 만들어서 WEAT score를 구해보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "307eac51",
   "metadata": {},
   "source": [
    "target_X = ['student']\n",
    "target_Y = ['teacher']\n",
    "attribute_A = ['new']\n",
    "attribute_B = ['old']\n",
    "\n",
    "X = np.array([w2v[word] for word in target_X])\n",
    "Y = np.array([w2v[word] for word in target_Y])\n",
    "A = np.array([w2v[word] for word in attribute_A])\n",
    "B = np.array([w2v[word] for word in attribute_B])\n",
    "\n",
    "weat_score(X, Y, A, B)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ba903bf1",
   "metadata": {},
   "source": [
    "#메모리를 다시 비워줍시다.\n",
    "del w2v\n",
    "print(\"삭제 완료\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "69a1f69a",
   "metadata": {},
   "source": [
    "## 직접 만드는 Word Embedding에 WEAT 적용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f56db6",
   "metadata": {},
   "source": [
    "지금까지는 제시된 모델과 단어들로 WEAT score를 구해보았습니다. 이제 주어진 데이터로 다음과 같은 과정을 수행해보도록 하겠습니다.\n",
    "\n",
    "1. 형태소 분석기를 이용하여 품사가 명사인 경우, 해당 단어를 추출하기\n",
    "2. 추출된 결과로 embedding model 만들기\n",
    "3. TF/IDF로 해당 데이터를 가장 잘 표현하는 단어 셋 만들기\n",
    "4. embedding model과 단어 셋으로 WEAT score 구해보기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6fa4c7",
   "metadata": {},
   "source": [
    "## 1. 형태소 분석기를 이용하여 품사가 명사인 경우 해당 단어를 추출하기\n",
    "synopsis.txt(대략 17MB)에는 2001년부터 2019년 8월까지 제작된 영화들의 시놉시스 정보가 있습니다. (개봉된 영화 중 일부만 포함되어있습니다. 더 많은 영화 정보를 원하시면 KOBIS에서 확인하시기 바랍니다.) synopsis.txt의 일부를 읽어볼까요?"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-04T03:38:19.725958Z",
     "start_time": "2024-07-04T03:38:19.723371Z"
    }
   },
   "cell_type": "code",
   "source": "import os",
   "id": "278f8e38f7c7e6d0",
   "execution_count": 13,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "ed98f6f7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-04T03:38:59.919226Z",
     "start_time": "2024-07-04T03:38:59.912525Z"
    }
   },
   "source": [
    "with open('./synopsis/synopsis.txt', 'r') as file:\n",
    "    for i in range(20):\n",
    "        print(file.readline(), end='')"
   ],
   "execution_count": 15,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "efdf3ff6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-04T03:55:13.559517Z",
     "start_time": "2024-07-04T03:55:13.331717Z"
    }
   },
   "source": [
    "# 약 15분정도 걸립니다.\n",
    "from konlpy.tag import Okt\n",
    "okt = Okt()\n",
    "tokenized = []\n",
    "with open(os.getenv('HOME')+'/aiffel/weat/synopsis.txt', 'r') as file:\n",
    "    while True:\n",
    "        line = file.readline()\n",
    "        if not line: break\n",
    "        words = okt.pos(line, stem=True, norm=True)\n",
    "        res = []\n",
    "        for w in words:\n",
    "            if w[1] in [\"Noun\"]:      # \"Adjective\", \"Verb\" 등을 포함할 수도 있습니다.\n",
    "                res.append(w[0])    # 명사일 때만 tokenized 에 저장하게 됩니다. \n",
    "        tokenized.append(res)\n",
    "\n",
    "print(\"슝~\")"
   ],
   "execution_count": 18,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7a1da7a5",
   "metadata": {},
   "source": [
    "print(len(tokenized))"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "3f994636",
   "metadata": {},
   "source": [
    "## 2. 추출된 결과로 embedding model 만들기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0bcf07cd",
   "metadata": {},
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# tokenized에 담긴 데이터를 가지고 나만의 Word2Vec을 생성합니다. (Gensim 4.0 기준)\n",
    "model = Word2Vec(tokenized, vector_size=100, window=5, min_count=3, sg=0)  \n",
    "model.wv.most_similar(positive=['영화'])\n",
    "\n",
    "# Gensim 3.X 에서는 아래와 같이 생성합니다. \n",
    "# model = Word2Vec(tokenized, size=100, window=5, min_count=3, sg=0)  \n",
    "# model.most_similar(positive=['영화'])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e7dcc25d",
   "metadata": {},
   "source": [
    "model.wv.most_similar(positive=['사랑'])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "23301fef",
   "metadata": {},
   "source": [
    "model.wv.most_similar(positive=['연극'])"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "6a87b13d",
   "metadata": {},
   "source": [
    "## 3. TF-IDF로 해당 데이터를 가장 잘 표현하는 단어 셋 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b238b6b6",
   "metadata": {},
   "source": [
    "WEAT score를 구할 때 단어 셋을 만들어주어야 합니다. targets_X, targets_Y, attribute_A, attribute_B를 만들어주었던 것이 기억나시죠? 이제 두 축을 어떤 기준으로 잡고, 해당 축의 어떤 항목을 사용할지 정해야 합니다.\n",
    "\n",
    "여기서는 두 축을 영화 장르, 영화 구분 정보를 이용하겠습니다. (영화 구분 정보란 일반영화, 예술영화, 독립영화로 구분된 정보입니다. KOBIS에서 제공한 정보를 기준으로 분류하였습니다.)\n",
    "\n",
    "#### 영화 구분\n",
    "- synopsis_art.txt : 예술영화\n",
    "- synopsis_gen.txt : 일반영화(상업영화)\n",
    "- 그 외는 독립영화 등으로 분류됩니다.\n",
    "\n",
    "#### 장르 구분\n",
    "- synopsis_SF.txt: SF\n",
    "- synopsis_가족.txt: 가족\n",
    "- synopsis_공연.txt: 공연\n",
    "- synopsis_공포(호러).txt: 공포(호러)\n",
    "- synopsis_기타.txt: 기타\n",
    "- synopsis_다큐멘터리.txt: 다큐멘터리\n",
    "- synopsis_드라마.txt: 드라마\n",
    "- synopsis_멜로로맨스.txt: 멜로로맨스\n",
    "- synopsis_뮤지컬.txt: 뮤지컬\n",
    "- synopsis_미스터리.txt: 미스터리\n",
    "- synopsis_범죄.txt: 범죄\n",
    "- synopsis_사극.txt: 사극\n",
    "- synopsis_서부극(웨스턴).txt: 서부극(웨스턴)\n",
    "- synopsis_성인물(에로).txt: 성인물(에로)\n",
    "- synopsis_스릴러.txt: 스릴러\n",
    "- synopsis_애니메이션.txt: 애니메이션\n",
    "- synopsis_액션.txt: 액션\n",
    "- synopsis_어드벤처.txt: 어드벤처\n",
    "- synopsis_전쟁.txt: 전쟁\n",
    "- synopsis_코미디.txt: 코미디\n",
    "- synopsis_판타지.txt: 판타지\n",
    "\n",
    "이번에는 예술영화와 일반영화(상업영화)라는 영화구분을 target으로 삼고, 드라마 장르와 액션 장르라는 장르구분을 attribute로 삼아 WEAT score를 계산해 보겠습니다. 즉 드라마 장르에는 예술영화적 성격이 강하고, 액션 장르에는 일반(상업)영화적 성격이 강할 것이라는 편향성이 워드 임베딩 상에 얼마나 나타나고 있는지를 측정해 보겠다는 것입니다.\n",
    "\n",
    "'synopsis_art.txt', 'synopsis_gen.txt' 두 파일을 읽고, 위에서 했던 것과 마찬가지로 명사에 대해서만 추출하여 art, gen 변수에 할당하시기 바랍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b570b4b9",
   "metadata": {},
   "source": [
    "import os\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "art_txt = 'synopsis_art.txt'\n",
    "gen_txt = 'synopsis_gen.txt'\n",
    "\n",
    "def read_token(file_name):\n",
    "    okt = Okt()\n",
    "    result = []\n",
    "    with open(os.getenv('HOME')+'/aiffel/weat/'+file_name, 'r') as fread: \n",
    "        print(file_name, '파일을 읽고 있습니다.')\n",
    "        while True:\n",
    "            line = fread.readline() \n",
    "            if not line: break \n",
    "            tokenlist = okt.pos(line, stem=True, norm=True) \n",
    "            for word in tokenlist:\n",
    "                if word[1] in [\"Noun\"]:#, \"Adjective\", \"Verb\"]:\n",
    "                    result.append((word[0])) \n",
    "    return ' '.join(result)\n",
    "\n",
    "print(\"슝~\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f6ca68",
   "metadata": {},
   "source": [
    "# 2개의 파일을 처리하는데 10분 가량 걸립니다. \n",
    "art = read_token(art_txt)\n",
    "gen = read_token(gen_txt)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d394e23a",
   "metadata": {},
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform([art, gen])\n",
    "\n",
    "print(X.shape)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9ac93a",
   "metadata": {},
   "source": [
    "print(vectorizer.vocabulary_['영화'])\n",
    "print(vectorizer.get_feature_names()[23976])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b9d146",
   "metadata": {},
   "source": [
    "m1 = X[0].tocoo()   # art를 TF-IDF로 표현한 sparse matrix를 가져옵니다. \n",
    "m2 = X[1].tocoo()   # gen을 TF-IDF로 표현한 sparse matrix를 가져옵니다. \n",
    "\n",
    "w1 = [[i, j] for i, j in zip(m1.col, m1.data)]\n",
    "w2 = [[i, j] for i, j in zip(m2.col, m2.data)]\n",
    "\n",
    "w1.sort(key=lambda x: x[1], reverse=True)   #art를 구성하는 단어들을 TF-IDF가 높은 순으로 정렬합니다. \n",
    "w2.sort(key=lambda x: x[1], reverse=True)   #gen을 구성하는 단어들을 TF-IDF가 높은 순으로 정렬합니다. \n",
    "\n",
    "print('예술영화를 대표하는 단어들:')\n",
    "for i in range(100):\n",
    "    print(vectorizer.get_feature_names()[w1[i][0]], end=', ')\n",
    "\n",
    "print('\\n')\n",
    "    \n",
    "print('일반영화를 대표하는 단어들:')\n",
    "for i in range(100):\n",
    "    print(vectorizer.get_feature_names()[w2[i][0]], end=', ')"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "a869c590",
   "metadata": {},
   "source": [
    "어떤가요? 두 개념을 대표하는 단어를 TF-IDF가 높은 순으로 추출하고 싶었는데, 양쪽에 중복된 단어가 너무 많은 것을 볼 수 있습니다. 두 개념축이 대조되도록 대표하는 단어 셋을 만들고 싶기 때문에 단어가 서로 중복되지 않게 단어셋을 추출해야 합니다. 우선 상위 100개의 단어들 중 중복되는 단어를 제외하고 상위 n(=15)개의 단어를 추출합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a5fb1d",
   "metadata": {},
   "source": [
    "n = 15\n",
    "w1_, w2_ = [], []\n",
    "for i in range(100):\n",
    "    w1_.append(vectorizer.get_feature_names()[w1[i][0]])\n",
    "    w2_.append(vectorizer.get_feature_names()[w2[i][0]])\n",
    "\n",
    "# w1에만 있고 w2에는 없는, 예술영화를 잘 대표하는 단어를 15개 추출한다.\n",
    "target_art, target_gen = [], []\n",
    "for i in range(100):\n",
    "    if (w1_[i] not in w2_) and (w1_[i] in model.wv): target_art.append(w1_[i])\n",
    "    if len(target_art) == n: break \n",
    "\n",
    "# w2에만 있고 w1에는 없는, 일반영화를 잘 대표하는 단어를 15개 추출한다.\n",
    "for i in range(100):\n",
    "    if (w2_[i] not in w1_) and (w2_[i] in model.wv): target_gen.append(w2_[i])\n",
    "    if len(target_gen) == n: break"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a60812",
   "metadata": {},
   "source": [
    "print(target_art)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f38acf",
   "metadata": {},
   "source": [
    "print(target_gen)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "39a2a293",
   "metadata": {},
   "source": [
    "이번에는 장르별 대표 단어를 추출해 봅시다. 이번에는 드라마 장르와 액션 장르를 다루어 보려고 합니다. 그러나 드라마와 액션 단 2개의 장르만 고려하기보다는 여러 장르의 코퍼스를 두루 고려하는 것이 특정 장르를 대표하는 단어를 선택하는 데 더 유리할 것입니다. 이번에는 주요 장르 5개만 고려해 봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0a9f76",
   "metadata": {},
   "source": [
    "genre_txt = ['synopsis_drama.txt', 'synopsis_romance.txt', 'synopsis_action.txt', 'synopsis_comedy.txt', 'synopsis_war.txt', 'synopsis_horror.txt']\n",
    "genre_name = ['드라마', '멜로로맨스', '액션', '코미디', '전쟁', '공포(호러)']\n",
    "\n",
    "print(\"슝~\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e458fc9",
   "metadata": {},
   "source": [
    "# 약 10분정도 걸립니다.\n",
    "genre = []\n",
    "for file_name in genre_txt:\n",
    "    genre.append(read_token(file_name))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832f1536",
   "metadata": {},
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(genre)\n",
    "\n",
    "print(X.shape)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d6bacd",
   "metadata": {},
   "source": [
    "m = [X[i].tocoo() for i in range(X.shape[0])]\n",
    "\n",
    "w = [[[i, j] for i, j in zip(mm.col, mm.data)] for mm in m]\n",
    "\n",
    "for i in range(len(w)):\n",
    "    w[i].sort(key=lambda x: x[1], reverse=True)\n",
    "attributes = []\n",
    "for i in range(len(w)):\n",
    "    print(genre_name[i], end=': ')\n",
    "    attr = []\n",
    "    j = 0\n",
    "    while (len(attr) < 15):\n",
    "        if vectorizer.get_feature_names()[w[i][j][0]] in model.wv:\n",
    "            attr.append(vectorizer.get_feature_names()[w[i][j][0]])\n",
    "            print(vectorizer.get_feature_names()[w[i][j][0]], end=', ')\n",
    "        j += 1\n",
    "    attributes.append(attr)\n",
    "    print()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "8fd7537a",
   "metadata": {},
   "source": [
    "## 4. embedding model과 단어 셋으로 WEAT score 구해보기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e66c693",
   "metadata": {},
   "source": [
    "이제 WEAT_score를 구해봅시다.\n",
    "\n",
    "traget_X는 art, target_Y는 gen, attribute_A는 '드라마', attribute_B는 '액션' 과 같이 정해줄 수 있습니다.\n",
    "\n",
    "target_X 는 art, target_Y 는 gen으로 고정하고 attribute_A, attribute_B를 바꿔가면서 구해봅시다. 구한 결과를 21x21 매트릭스 형태로 표현해서 matrix 라는 변수에 담아봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2abd4ea",
   "metadata": {},
   "source": [
    "matrix = [[0 for _ in range(len(genre_name))] for _ in range(len(genre_name))]\n",
    "print(\"슝~\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430d0b4a",
   "metadata": {},
   "source": [
    "X = np.array([model.wv[word] for word in target_art])\n",
    "Y = np.array([model.wv[word] for word in target_gen])\n",
    "\n",
    "for i in range(len(genre_name)-1):\n",
    "    for j in range(i+1, len(genre_name)):\n",
    "        A = np.array([model.wv[word] for word in attributes[i]])\n",
    "        B = np.array([model.wv[word] for word in attributes[j]])\n",
    "        matrix[i][j] = weat_score(X, Y, A, B)\n",
    "\n",
    "print(\"슝~\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "dafdd5c4",
   "metadata": {},
   "source": [
    "matrix를 채워보았습니다. WEAT score 값을 보고, 과연 우리의 직관과 비슷한지 살펴볼까요?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebabc927",
   "metadata": {},
   "source": [
    "for i in range(len(genre_name)-1):\n",
    "    for j in range(i+1, len(genre_name)):\n",
    "        print(genre_name[i], genre_name[j],matrix[i][j])"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "2d3596ba",
   "metadata": {},
   "source": [
    "WEAT score가 0.8 이상, -0.8 이하의 경우만 해석해 보면 아래와 같습니다.\n",
    "\n",
    "예술영화와 일반영화, 그리고 드라마와 멜로로맨스의 WEAT score의 의미를 해석해보면 예술영화는 멜로로맨스, 일반영화는 드라마와 가깝다고 볼 수 있습니다. 부호가 마이너스이므로 사람의 편향과 반대라는 것을 알 수 있습니다.\n",
    "예술영화와 일반영화, 그리고 멜로로맨스와 코미디의 WEAT score의 의미를 해석해보면 예술 영화는 멜로로맨스와 가깝고, 코디미는 일반 영화와 가깝다고 볼 수 있습니다.\n",
    "예술영화와 일반영화, 그리고 멜로로맨스와 전쟁의 WEAT score의 의미를 해석해보면 예술 영화는 멜로로맨스와 가깝고, 전쟁은 일반 영화와 가깝다고 볼 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6d1093",
   "metadata": {},
   "source": [
    "import numpy as np; \n",
    "import seaborn as sns; \n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "# 한글 지원 폰트\n",
    "sns.set(font='NanumGothic')\n",
    "\n",
    "# 마이너스 부호 \n",
    "\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "ax = sns.heatmap(matrix, xticklabels=genre_name, yticklabels=genre_name, annot=True,  cmap='RdYlGn_r')\n",
    "ax"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "077126a1",
   "metadata": {},
   "source": [
    "지금까지 word embedding model에 있는 편향성을 확인해보기 위해 WEAT score를 시도해보았습니다. 이 학습을 통해 여러분이 가진 데이터로 word embedding model을 만들 수 있고, 이 모델이 특정 분야에 대해 편향이 되어있는지 확인해볼 수 있게 되었기를 바랍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8303fb",
   "metadata": {},
   "source": [],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
