{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71161e94",
   "metadata": {},
   "source": [
    "# 프로젝트 : 모든 장르 간 편향성 측정해 보기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97675640",
   "metadata": {},
   "source": [
    "## 라이브러리 버전을 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c44e4901",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "import konlpy\n",
    "import gensim\n",
    "import sklearn\n",
    "import seaborn\n",
    "\n",
    "print(konlpy.__version__)\n",
    "print(gensim.__version__)\n",
    "print(sklearn.__version__)\n",
    "print(seaborn.__version__)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f18f1c3f",
   "metadata": {},
   "source": [
    "from tqdm import tqdm, trange"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ddb2311",
   "metadata": {},
   "source": [
    "import os\n",
    "from konlpy.tag import Okt\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from collections import defaultdict, Counter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "1f1ff5b9",
   "metadata": {},
   "source": [
    "## STEP 1. 형태소 분석기를 이용하여 품사가 명사인 경우 해당 단어를 추출하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da8232e2",
   "metadata": {},
   "source": [
    "genre_txt = ['synopsis_SF.txt', 'synopsis_family.txt', 'synopsis_show.txt', 'synopsis_horror.txt', 'synopsis_etc.txt', \n",
    "             'synopsis_documentary.txt', 'synopsis_drama.txt', 'synopsis_romance.txt', 'synopsis_musical.txt', \n",
    "             'synopsis_mystery.txt', 'synopsis_crime.txt', 'synopsis_historical.txt', 'synopsis_western.txt', \n",
    "             'synopsis_adult.txt', 'synopsis_thriller.txt', 'synopsis_animation.txt', 'synopsis_action.txt', \n",
    "             'synopsis_adventure.txt', 'synopsis_war.txt', 'synopsis_comedy.txt', 'synopsis_fantasy.txt']\n",
    "genre_name = ['SF', '가족', '공연', '공포(호러)', '기타', '다큐멘터리', '드라마', '멜로로맨스', '뮤지컬', '미스터리', '범죄', '사극', '서부극(웨스턴)',\n",
    "         '성인물(에로)', '스릴러', '애니메이션', '액션', '어드벤처', '전쟁', '코미디', '판타지']"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "92d7b263",
   "metadata": {},
   "source": [
    "import os\n",
    "\n",
    "def read_synopsis(file_name):\n",
    "    with open(os.getenv('HOME')+'/aiffel/weat/'+file_name, 'r') as file:\n",
    "        for i in range(1):\n",
    "            print(file.readline(), end='')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b0749de",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "read_synopsis(genre_txt[0])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "7b40213e",
   "metadata": {},
   "source": [
    "def read_files(file_list):\n",
    "    texts = []\n",
    "    for file_name in tqdm(file_list):\n",
    "        with open(os.getenv('HOME')+'/aiffel/weat/'+file_name, 'r') as file:\n",
    "            texts.append(file.read())\n",
    "    return texts"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "95e5bfdf",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "texts = read_files(genre_txt)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5349dc30",
   "metadata": {},
   "source": [
    "from nltk.tokenize import sent_tokenize"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "229b1998",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2f1679c2",
   "metadata": {},
   "source": [
    "len(texts)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "69317c59",
   "metadata": {},
   "source": [
    "def preprocess_texts_korean(texts):\n",
    "    okt = Okt()\n",
    "    processed_texts = []\n",
    "    for text in tqdm(texts):\n",
    "        genre_texts = []\n",
    "        sentences = sent_tokenize(text)\n",
    "        for sentence in sentences:\n",
    "            words = []\n",
    "            # 명사만 추출\n",
    "            tokenlist = okt.pos(sentence, stem=True, norm=True)\n",
    "            for word in tokenlist:\n",
    "                if word[1] in [\"Noun\"]:\n",
    "                    words.append(word[0])\n",
    "        \n",
    "            genre_texts.append(words)\n",
    "\n",
    "        processed_texts.append(genre_texts)\n",
    "    \n",
    "    return processed_texts"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "09e2312b",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "processed_texts = preprocess_texts_korean(texts)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cdcd3a36",
   "metadata": {},
   "source": [
    "len(processed_texts)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "dec0d33d",
   "metadata": {},
   "source": [
    "\"nouns_\" + genre_txt[0]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e1996be5",
   "metadata": {},
   "source": [
    "for idx in trange(len(genre_txt)):\n",
    "    file_name = \"nouns_\"+genre_txt[idx]\n",
    "\n",
    "    with open(file_name, 'w+') as file:\n",
    "        for num, i in enumerate(processed_texts[idx]):\n",
    "            if num+1 < len(processed_texts[idx]):\n",
    "                file.write(', '.join(i) + \"\\n\")\n",
    "            else:\n",
    "                file.write(', '.join(i))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5a523d32",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "li = []\n",
    "with open(\"nouns_\" + genre_txt[0], \"r\") as file:\n",
    "    for fi in file:\n",
    "        ll = [ name.strip() for name in fi.split(\",\")]\n",
    "        li.append(ll)\n",
    "# li"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "d5329c7c",
   "metadata": {},
   "source": [
    "## STEP 2. 추출된 결과로 embedding model 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f3a853e0",
   "metadata": {},
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# tokenized에 담긴 데이터를 가지고 나만의 Word2Vec을 생성합니다. (Gensim 4.0 기준)\n",
    "model = Word2Vec(processed_texts, vector_size=100, window=5, min_count=3, sg=0)  \n",
    "model.wv.most_similar(positive=['영화'])\n",
    "\n",
    "# Gensim 3.X 에서는 아래와 같이 생성합니다. \n",
    "# model = Word2Vec(tokenized, size=100, window=5, min_count=3, sg=0)  \n",
    "# model.most_similar(positive=['영화'])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "ae5ae7f7",
   "metadata": {},
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "model.wv.save_word2vec_format('./w2v') "
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "74653eac",
   "metadata": {},
   "source": [
    "model = KeyedVectors.load_word2vec_format(\"./w2v\")\n",
    "print(\"모델  load 완료!\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e446e9d1",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "model.most_similar(positive=['사랑'])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "19d28ebe",
   "metadata": {},
   "source": [
    "model.wv.most_similar(positive=['연극'])"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "f825c8c5",
   "metadata": {},
   "source": [
    "## STEP 3. target, attribute 단어 셋 만들기\n",
    "----\n",
    "이전 스텝에서는 TF-IDF를 사용해서 단어 셋을 만들었습니다. 이 방법으로도 어느 정도는 대표 단어를 잘 선정할 수 있습니다. 그러나 TF-IDF가 높은 단어를 골랐음에도 불구하고 중복되는 단어가 발생하는 문제가 있었습니다. 개념축을 표현하는 단어가 제대로 선정되지 않은 것은 WEAT 계산 결과에 악영향을 미칩니다.\n",
    "\n",
    "TF-IDF를 적용했을 때의 문제점이 무엇인지 지적 가능하다면 그 문제점을 지적하고 스스로 방법을 개선하여 대표 단어 셋을 구축해 보기 바랍니다. TF-IDF 방식을 쓰더라도 중복된 단어를 잘 제거하면 여전히 유용한 방식이 될 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72eea53",
   "metadata": {},
   "source": [
    "## WEAT 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "70d913da",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-04T02:15:55.587220Z",
     "start_time": "2024-07-04T02:15:55.585021Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "66cb8c9a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-04T02:18:21.055670Z",
     "start_time": "2024-07-04T02:18:21.042783Z"
    }
   },
   "source": [
    "def cos_sim(i, j):\n",
    "    return dot(i, j.T)/(norm(i)*norm(j))\n",
    "\n",
    "def s(w, A, B):\n",
    "    c_a = cos_sim(w, A)\n",
    "    c_b = cos_sim(w, B)\n",
    "    mean_A = np.mean(c_a, axis=-1)\n",
    "    mean_B = np.mean(c_b, axis=-1)\n",
    "    return mean_A - mean_B #, c_a, c_b\n",
    "\n",
    "def weat_score(X, Y, A, B):\n",
    "    \n",
    "    s_X = s(X, A, B)\n",
    "    s_Y = s(Y, A, B)\n",
    "\n",
    "    mean_X = np.mean(s_X)\n",
    "    mean_Y = np.mean(s_Y)\n",
    "    \n",
    "    std_dev = np.std(np.concatenate([s_X, s_Y], axis=0))\n",
    "    \n",
    "    return  (mean_X-mean_Y)/std_dev"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "5ebd078a",
   "metadata": {},
   "source": [
    "## 3. TF-IDF로 해당 데이터를 가장 잘 표현하는 단어 셋 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "5a58aeaa",
   "metadata": {},
   "source": [
    "import os\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "def read_token(file_name):\n",
    "    okt = Okt()\n",
    "    result = []\n",
    "    with open(os.getenv('HOME')+'/aiffel/weat/'+file_name, 'r') as fread: \n",
    "        #print(file_name, '파일을 읽고 있습니다.')\n",
    "        while True:\n",
    "            line = fread.readline() \n",
    "            if not line: break \n",
    "            tokenlist = okt.pos(line, stem=True, norm=True) \n",
    "            for word in tokenlist:\n",
    "                if word[1] in [\"Noun\"]:#, \"Adjective\", \"Verb\"]:\n",
    "                    result.append((word[0])) \n",
    "    return ' '.join(result)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "79150b58",
   "metadata": {},
   "source": [
    "### art, gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "c5c9fb45",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "art_txt = 'synopsis_art.txt'\n",
    "gen_txt = 'synopsis_gen.txt'\n",
    "\n",
    "art = read_token(art_txt)\n",
    "gen = read_token(gen_txt)\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform([art, gen])\n",
    "\n",
    "print(vectorizer.vocabulary_['영화'])\n",
    "print(vectorizer.get_feature_names()[23976])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3042750c",
   "metadata": {},
   "source": [
    "m1 = X[0].tocoo()   # art를 TF-IDF로 표현한 sparse matrix를 가져옵니다. \n",
    "m2 = X[1].tocoo()   # gen을 TF-IDF로 표현한 sparse matrix를 가져옵니다. \n",
    "\n",
    "w1 = [[i, j] for i, j in zip(m1.col, m1.data)]\n",
    "w2 = [[i, j] for i, j in zip(m2.col, m2.data)]\n",
    "\n",
    "w1.sort(key=lambda x: x[1], reverse=True)   #art를 구성하는 단어들을 TF-IDF가 높은 순으로 정렬합니다. \n",
    "w2.sort(key=lambda x: x[1], reverse=True)   #gen을 구성하는 단어들을 TF-IDF가 높은 순으로 정렬합니다. \n",
    "\n",
    "print('예술영화를 대표하는 단어들:')\n",
    "for i in range(100):\n",
    "    print(vectorizer.get_feature_names()[w1[i][0]], end=', ')\n",
    "\n",
    "print('\\n')\n",
    "    \n",
    "print('일반영화를 대표하는 단어들:')\n",
    "for i in range(100):\n",
    "    print(vectorizer.get_feature_names()[w2[i][0]], end=', ')"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "cc2259d2",
   "metadata": {},
   "source": [
    "__<예술영화를 대표하는 단어들>__    \n",
    "그녀, 자신, 시작, 위해, 사랑, 사람, 영화, 친구, 남자, 가족, 이야기, 마을, 사건, 마음, 세상, 아버지, 아이, 엄마, 모든, 여자, 대한, 서로, 과연, 다시, 시간, 아들, 소녀, 아내, 다른, 사이, 영화제, 세계, 사실, 하나, 점점, 남편, 감독, 여행, 인생, 발견, 모두, 순간, 우리, 가장, 마지막, 생활, 아빠, 모습, 통해, 죽음, 기억, 비밀, 학교, 음악, 한편, 소년, 생각, 도시, 명의, 사고, 결혼, 전쟁, 때문, 위기, 이제, 최고, 이자, 과거, 일상, 경찰, 상황, 간다, 미국, 결심, 운명, 현실, 관계, 지금, 단편, 여인, 하루, 이름, 이후, 준비, 인간, 감정, 만난, 국제, 처음, 충격, 살인, 누구, 동안, 존재, 그린, 어머니, 연인, 계속, 동생, 작품, \n",
    "\n",
    "__<일반영화를 대표하는 단어들>__    \n",
    "자신, 그녀, 영화제, 위해, 사람, 시작, 국제, 영화, 친구, 사랑, 남자, 이야기, 대한, 서울, 여자, 사건, 남편, 아이, 가족, 아버지, 다른, 마을, 시간, 엄마, 아들, 모든, 단편, 마음, 사실, 다시, 세계, 모습, 작품, 통해, 생각, 서로, 세상, 발견, 감독, 아내, 관계, 소녀, 사이, 하나, 우리, 애니메이션, 때문, 여성, 죽음, 과연, 점점, 인간, 생활, 한편, 결혼, 상황, 모두, 기억, 명의, 소년, 여행, 가장, 간다, 순간, 이제, 도시, 비밀, 학교, 과거, 가지, 이자, 경찰, 마지막, 미국, 동안, 전쟁, 주인공, 대해, 존재, 현실, 연출, 사고, 살인, 일상, 어머니, 계속, 사회, 인생, 다큐멘터리, 부문, 섹스, 최고, 바로, 동생, 의도, 하루, 위기, 계획, 정체, 한국, "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7701d08e",
   "metadata": {},
   "source": [
    "두 개념을 대표하는 단어를 TF-IDF가 높은 순으로 추출하고 싶었는데, 양쪽에 중복된 단어가 너무 많은 것을 볼 수 있습니다.\n",
    "\n",
    "두 개념축이 대조되도록 대표하는 단어 셋을 만들고 싶기 때문에 단어가 서로 중복되지 않게 단어셋을 추출해야 합니다. 우선 상위 100개의 단어들 중 중복되는 단어를 제외하고 상위 n(=15)개의 단어를 추출합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9698b288",
   "metadata": {},
   "source": [
    "n = 15\n",
    "w1_, w2_ = [], []\n",
    "for i in range(100):\n",
    "    w1_.append(vectorizer.get_feature_names()[w1[i][0]])\n",
    "    w2_.append(vectorizer.get_feature_names()[w2[i][0]])\n",
    "\n",
    "# w1에만 있고 w2에는 없는, 예술영화를 잘 대표하는 단어를 15개 추출한다.\n",
    "target_art, target_gen = [], []\n",
    "for i in range(100):\n",
    "    if (w1_[i] not in w2_) and (w1_[i] in model.wv): target_art.append(w1_[i])\n",
    "    if len(target_art) == n: break \n",
    "\n",
    "# w2에만 있고 w1에는 없는, 일반영화를 잘 대표하는 단어를 15개 추출한다.\n",
    "for i in range(100):\n",
    "    if (w2_[i] not in w1_) and (w2_[i] in model.wv): target_gen.append(w2_[i])\n",
    "    if len(target_gen) == n: break"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91cfa74c",
   "metadata": {},
   "source": [
    "print('예술영화를 대표하는 단어들:')\n",
    "print(target_art)\n",
    "print('\\n')\n",
    "print('일반영화를 대표하는 단어들:')\n",
    "print(target_gen)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "c57f2205",
   "metadata": {},
   "source": [
    "target_art = ['아빠', '음악', '결심', '운명', '지금', '여인', '이름', '이후', '준비', '감정', '만난', '처음', '충격', '누구', '그린']\n",
    "     \n",
    "     \n",
    "target_gen = ['서울', '애니메이션', '여성', '가지', '주인공', '대해', '연출', '사회', '다큐멘터리', '부문', '섹스', '바로', '의도', '계획', '정체']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0270d3c6",
   "metadata": {},
   "source": [
    "## 전체 장르 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1c129ee8",
   "metadata": {},
   "source": [
    "genre_txt = ['synopsis_SF.txt', 'synopsis_family.txt', 'synopsis_show.txt', 'synopsis_horror.txt', 'synopsis_etc.txt', \n",
    "             'synopsis_documentary.txt', 'synopsis_drama.txt', 'synopsis_romance.txt', 'synopsis_musical.txt', \n",
    "             'synopsis_mystery.txt', 'synopsis_crime.txt', 'synopsis_historical.txt', 'synopsis_western.txt', \n",
    "             'synopsis_adult.txt', 'synopsis_thriller.txt', 'synopsis_animation.txt', 'synopsis_action.txt', \n",
    "             'synopsis_adventure.txt', 'synopsis_war.txt', 'synopsis_comedy.txt', 'synopsis_fantasy.txt']\n",
    "genre_name = ['SF', '가족', '공연', '공포(호러)', '기타', '다큐멘터리', '드라마', '멜로로맨스', '뮤지컬', '미스터리', '범죄', '사극', '서부극(웨스턴)',\n",
    "         '성인물(에로)', '스릴러', '애니메이션', '액션', '어드벤처', '전쟁', '코미디', '판타지']"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c0dd1ac5",
   "metadata": {},
   "source": [
    "genre = []\n",
    "for file_name in tqdm(genre_txt):\n",
    "    genre.append(read_token(file_name))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91eedb5d",
   "metadata": {},
   "source": [
    "len(genre)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29889393",
   "metadata": {},
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(genre)\n",
    "\n",
    "print(X.shape)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4f3887b0",
   "metadata": {},
   "source": [
    "m = [X[i].tocoo() for i in range(X.shape[0])]\n",
    "\n",
    "w = [[[i, j] for i, j in zip(mm.col, mm.data)] for mm in m]\n",
    "\n",
    "for i in range(len(w)):\n",
    "    w[i].sort(key=lambda x: x[1], reverse=True)\n",
    "attributes = []\n",
    "for i in range(len(w)):\n",
    "    print(genre_name[i], end=': ')\n",
    "    attr = []\n",
    "    j = 0\n",
    "    while (len(attr) < 15):\n",
    "        if vectorizer.get_feature_names()[w[i][j][0]] in model.wv:\n",
    "            attr.append(vectorizer.get_feature_names()[w[i][j][0]])\n",
    "            print(vectorizer.get_feature_names()[w[i][j][0]], end=', ')\n",
    "        j += 1\n",
    "    attributes.append(attr)\n",
    "    print()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "1ffd634c",
   "metadata": {},
   "source": [
    "단순히 tf-idf로 구하게 되면 중복되는 단어셋이 많이 보인다    \n",
    "중복단어 제거를 해줘야 한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ed378d94",
   "metadata": {},
   "source": [
    "all_words = [text for texts in processed_texts for text in texts]\n",
    "word_counts = Counter(all_words)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1f7e6ad2",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "word_counts.most_common(5)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5fffe250",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "word_counts.most_common(30)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c79fa236",
   "metadata": {},
   "source": [
    "len(word_counts)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2aba69c0",
   "metadata": {},
   "source": [
    "import pandas as pd"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "157d7fb5",
   "metadata": {},
   "source": [
    "wc = pd.DataFrame({'word': word_counts.keys(),\n",
    "             'cnt': word_counts.values()})"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "af54f467",
   "metadata": {},
   "source": [
    "wc[wc.cnt>50].cnt.describe()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "bc2fa35b",
   "metadata": {},
   "source": [
    "wc[wc.cnt>100]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d699d142",
   "metadata": {},
   "source": [
    "wc = wc.sort_values(by='cnt',ascending=False)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "860545fb",
   "metadata": {},
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np; \n",
    "import seaborn as sns; \n",
    "\n",
    "np.random.seed(0)\n",
    "plt.figure(figsize=(15,4))\n",
    "# 한글 지원 폰트\n",
    "sns.set(font='NanumGothic')\n",
    "\n",
    "fig.set_facecolor('white') #fig의 배경색\n",
    "\n",
    "c = wc.head(40)\n",
    "\n",
    "ax = sns.barplot(x=c['word'], y= c['cnt'])\n",
    "ax.set_title('전체 단어 빈도')\n",
    "plt.xticks(rotation=45) #xticks는 고객 명\n",
    "\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "87bec6f2",
   "metadata": {},
   "source": [
    "wc.head(50)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "cb96b145",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "sns.boxplot(wc.cnt)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "050cca52",
   "metadata": {},
   "source": [
    "# 공통 단어 리스트 생성\n",
    "common_words = set([word for word, count in word_counts.items() if count > 1000])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "642e13f6",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "# common_words"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "cb8402d7",
   "metadata": {},
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(genre)\n",
    "\n",
    "print(X.shape)\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "331c25f8",
   "metadata": {},
   "source": [
    "m = [X[i].tocoo() for i in range(X.shape[0])]\n",
    "w = [[[i, j] for i, j in zip(mm.col, mm.data)] for mm in m]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "65d967ce",
   "metadata": {},
   "source": [
    "for i in range(len(w)):\n",
    "    w[i].sort(key=lambda x: x[1], reverse=True)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "0817db63",
   "metadata": {},
   "source": [
    "new_attributes = []\n",
    "for i in range(len(w)):\n",
    "    print(genre_name[i], end=': ')\n",
    "    attr = []\n",
    "    j = 0\n",
    "    while (len(attr) <= 15):\n",
    "        word = vectorizer.get_feature_names_out()[w[i][j][0]]\n",
    "        if (word in model.wv) and (word not in common_words):\n",
    "            attr.append(word)\n",
    "            print(word, end=', ')\n",
    "        j += 1\n",
    "    new_attributes.append(attr)\n",
    "    print()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "262c53aa",
   "metadata": {},
   "source": [
    "## 4. embedding model과 단어 셋으로 WEAT score 구해보기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7381907e",
   "metadata": {},
   "source": [
    "이제 WEAT_score를 구해봅시다.\n",
    "\n",
    "traget_X는 art, target_Y는 gen, attribute_A는 '드라마', attribute_B는 '액션' 과 같이 정해줄 수 있습니다.\n",
    "\n",
    "target_X 는 art, target_Y 는 gen으로 고정하고 attribute_A, attribute_B를 바꿔가면서 구해봅시다. 구한 결과를 21x21 매트릭스 형태로 표현해서 matrix 라는 변수에 담아봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "7b553705",
   "metadata": {},
   "source": [
    "from tqdm import trange"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "ccb8cd17",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "matrix = [[0 for _ in range(len(genre_name))] for _ in trange(len(genre_name))]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "190fd93f",
   "metadata": {},
   "source": [
    "len(new_attributes)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "f493f895",
   "metadata": {},
   "source": [
    "X = np.array([model.wv[word] for word in target_art])\n",
    "Y = np.array([model.wv[word] for word in target_gen])\n",
    "\n",
    "for i in range(len(genre_name)-1):\n",
    "    for j in range(i+1, len(genre_name)):\n",
    "        A = np.array([model.wv[word] for word in new_attributes[i]])\n",
    "        B = np.array([model.wv[word] for word in new_attributes[j]])\n",
    "        matrix[i][j] = weat_score(X, Y, A, B)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "7006f9ce",
   "metadata": {},
   "source": [
    "matrix를 채워보았습니다. WEAT score 값을 보고, 과연 우리의 직관과 비슷한지 살펴볼까요?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "ee6b5160",
   "metadata": {},
   "source": [
    "import pandas as pd"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "c0234418",
   "metadata": {},
   "source": [
    "for i in range(len(genre_name)-1):\n",
    "    for j in range(i+1, len(genre_name)):\n",
    "        print(genre_name[i], genre_name[j],matrix[i][j])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "6cafad75",
   "metadata": {},
   "source": [
    "corr_df = pd.DataFrame(matrix, columns=genre_name, index=genre_name)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "f5629e40",
   "metadata": {},
   "source": [
    "corr_df_unstack = pd.DataFrame(corr_df.unstack(), columns=['score']).sort_values(by='score')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "4f08fed3",
   "metadata": {},
   "source": [
    "corr_df_unstack.to_csv('corr_df_unstack.csv')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "e0024561",
   "metadata": {},
   "source": [
    "corr_df_unstack.reset_index(inplace=True)\n",
    "corr_df_unstack.columns = ['A','B','score']"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "eb9378d8",
   "metadata": {},
   "source": [
    "corr_df_unstack.to_csv('corr_df_unstack.csv', index=False)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "48dbe845",
   "metadata": {},
   "source": [
    "## WEAT score가 0.8 이상, -0.8 이하의 경우\n",
    "\n",
    "- 양수 점수: X(예술영화) 단어들이 A 속성 단어들과 더 많이 연관되어 있고, Y(일반영화) 단어들이 B 속성 단어들과 더 많이 연관되어 있음\n",
    "- 음수 점수: X(예술영화) 단어들이 B 속성 단어들과 더 많이 연관되어 있고, Y(일반영화) 단어들이 A 속성 단어들과 더 많이 연관되어 있음\n",
    "- 점수의 크기: 절대값이 클수록 편향의 정도가 큼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "677b34f1",
   "metadata": {},
   "source": [
    "corr_df_unstack[np.abs(corr_df_unstack.score) >= 0.8]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "d196208a",
   "metadata": {},
   "source": [
    "high_corr = corr_df_unstack[np.abs(corr_df_unstack.score) >= 0.8]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "8a381abc",
   "metadata": {},
   "source": [
    "high_corr.reset_index(inplace=True)\n",
    "high_corr.columns = ['A','B','score']"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "86013cb1",
   "metadata": {},
   "source": [
    "### 예술 영화와 가까운 장르"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "05ee2281",
   "metadata": {},
   "source": [
    "x1 = high_corr[high_corr.score > 0].A.tolist()\n",
    "x2 = high_corr[high_corr.score < 0].B.tolist()\n",
    "\n",
    "print(x1)\n",
    "print(x2)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "dc474222",
   "metadata": {},
   "source": [
    "### 일반영화와 가까운 장르"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "7b274625",
   "metadata": {},
   "source": [
    "y1 = high_corr[high_corr.score < 0].A.tolist()\n",
    "y2 = high_corr[high_corr.score > 0].B.tolist()\n",
    "\n",
    "print(y1)\n",
    "print(y2)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "9fbb3f37",
   "metadata": {},
   "source": [
    "### 결과 해석\n",
    "- weat score에 따른 결과를 해석해보면 예술 영화와 가까운 장르는 다큐멘터리, SF, 공연, 뮤지컬이 나타났고 이는 우리의 상식에 부합한 결과가 나왔다\n",
    "- 일반 영화와 가까운 장르는 판타지, 코미디, 멜로로멘스, 드라마, 가족 등 일반적인 우리의 생각과 유사한 결과가 나왔다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40af669e",
   "metadata": {},
   "source": [
    "## STEP 4. WEAT score 계산과 시각화\n",
    "---\n",
    "영화 구분, 영화 장르에 따른 편향성을 측정하여 WEAT score로 계산해 보고 이를 Heatmap 형태로 시각화해 봅시다. 편향성이 두드러지는 영화장르 attribute 구성에는 어떤 케이스가 있는지 시각적으로 두드러지게 구성되면 좋습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "a6702ccf",
   "metadata": {},
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np; \n",
    "import seaborn as sns; \n",
    "\n",
    "np.random.seed(0)\n",
    "plt.figure(figsize=(15,8))\n",
    "# 한글 지원 폰트\n",
    "sns.set(font='NanumGothic')\n",
    "\n",
    "# 마이너스 부호 \n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "ax = sns.heatmap(matrix, xticklabels=genre_name, yticklabels=genre_name, annot=True,  cmap='RdYlGn_r')\n",
    "ax"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "f45f0208",
   "metadata": {},
   "source": [
    "# 더 좋은 키워드를 뽑아 내기 위한 시도\n",
    "\n",
    "## Bertopic\n",
    "- 결과적으로 먼저 말하자면 좋은 토픽을 뽑아내지 못했다.\n",
    "- 이는 시놉시스를 바탕으로 키워드를 뽑아내서 줄거리의 내용들을 바탕으로 키워드를 뽑아서 잘 안된 것 같다\n",
    "- 오히려 장르별 리뷰 데이터가 있다면 더 좋은 결과가 나오지 않을까 싶다 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5c7b3869",
   "metadata": {},
   "source": [
    "# !pip install transformers\n",
    "# !pip install bertopic"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24767a0e",
   "metadata": {},
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "from bertopic import BERTopic\n",
    "import torch"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a8b459d",
   "metadata": {},
   "source": [
    "def get_bertopic_keywords(data):\n",
    "    num_topics = 2\n",
    "\n",
    "    topic_model = BERTopic(language='korean', nr_topics=num_topics)\n",
    "    topics, _ = topic_model.fit_transform(data)\n",
    "\n",
    "    #topic_info = topic_model.get_topic_info()\n",
    "    representative_keywords = topic_model.get_topic(0)\n",
    "\n",
    "    return representative_keywords"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "3b54af6f",
   "metadata": {},
   "source": [
    "### 위에서 저장해둔 processed_text 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ed46520f",
   "metadata": {},
   "source": [
    "processed_texts = []\n",
    "\n",
    "for txt in tqdm(genre_txt):\n",
    "    li = []\n",
    "    with open(\"nouns_\" + txt, \"r\") as file:\n",
    "        for fi in file:\n",
    "            ll = [ name.strip() for name in fi.split(\",\")]\n",
    "            li.append(ll)\n",
    "    processed_texts.append(li)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b69a9e87",
   "metadata": {},
   "source": [
    "len(processed_texts)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ee69acbb",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "keywords = {}\n",
    "\n",
    "for i in trange(len(processed_texts)):\n",
    "    data = [' '.join(texts) for texts in processed_texts[i]]\n",
    "    representative_keywords = get_bertopic_keywords(data)\n",
    "    keywords[genre_name[i]] = representative_keywords\n",
    "    print(f'{genre_name[i]} : ', representative_keywords)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dcb3e936",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "keywords[genre_name[1]]"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "af1e2d1d",
   "metadata": {},
   "source": [
    "- 좋은 키워드가 잘 뽑히지 않고 시간도 너무 오래걸려서 사용하지 않기로 결정했다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5afa13d2",
   "metadata": {},
   "source": [
    "## krwordrank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "78d81d70",
   "metadata": {},
   "source": [
    "# krwordrank 설치\n",
    "\n",
    "!pip install krwordrank"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e45627be",
   "metadata": {},
   "source": [
    "# library import\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from krwordrank.word import KRWordRank\n",
    "from krwordrank.word import summarize_with_keywords\n",
    "from krwordrank.sentence import summarize_with_sentences\n",
    "import pickle"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6bb41708",
   "metadata": {},
   "source": [
    "def get_krwordrank(texts, min_count, max_length, beta, maxiter):\n",
    "\n",
    "    a = [' '.join(text) for text in texts]\n",
    "    \n",
    "    # Keyword 알고리즘에 데이터 적용\n",
    "    keywords = summarize_with_keywords(a, min_count=min_count, max_length=max_length, beta=beta, max_iter=maxiter, verbose=True)\n",
    "    \n",
    "    # 키워드 뽑아내기.\n",
    "    for word, r in sorted(keywords.items(), key=lambda x:x[1], reverse=True)[:30]:\n",
    "       print(word, end=', ')\n",
    "        \n",
    "    return keywords"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d348f48b",
   "metadata": {},
   "source": [
    "# 하이퍼파라미터\n",
    "min_count=2 \n",
    "max_length=100\n",
    "beta=0.85\n",
    "max_iter=15\n",
    "\n",
    "# keywords = get_krwordrank(processed_texts[0], min_count, max_length, beta, max_iter)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "39965ce6",
   "metadata": {},
   "source": [
    "kr_keywords = {}\n",
    "\n",
    "for i in range(len(processed_texts)):\n",
    "    keywords = get_krwordrank(processed_texts[i], min_count, max_length, beta, max_iter)\n",
    "    kr_keywords[genre_name[i]] = keywords"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "727a6cd6",
   "metadata": {},
   "source": [
    "genre_keywords = {}\n",
    "\n",
    "for k in kr_keywords.keys():\n",
    "    print(k)\n",
    "    words = []\n",
    "    for word, r in sorted(kr_keywords[k].items(), key=lambda x:x[1], reverse=True)[:30]:\n",
    "        words.append(word)\n",
    "    print(words)\n",
    "    print()\n",
    "    genre_keywords[k] = words"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "4f439b1d",
   "metadata": {},
   "source": [
    "- 위의 방식으로 단순하게 키워드를 뽑아내니 겹치는 단어들이 자주 등장하였다\n",
    "- 전체 데이터셋에서 키워드를 뽑아내서 상위 키워드를 common_keywords에 넣어둔뒤 이를 불용어 처리하여 키워드에서 제거해 줄 것이다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e11c11",
   "metadata": {},
   "source": [
    "### 모든 장르의 데이터를 합친 뒤 키워드 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "56461a79",
   "metadata": {},
   "source": [
    "all_texts = [text for texts in processed_texts for text in texts]\n",
    "all_keywords = get_krwordrank(all_texts, min_count, max_length, beta, max_iter)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "435736c1",
   "metadata": {},
   "source": [
    "common_keywords = []\n",
    "\n",
    "for word, r in sorted(all_keywords.items(), key=lambda x:x[1], reverse=True)[:50]:\n",
    "    common_keywords.append(word)\n",
    "    print(word, r)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "207d0a3c",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "' '.join(common_keywords)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7da67984",
   "metadata": {},
   "source": [
    "common_keywords.extend(['충무','몰래','한편','이자','사실','어머','만난'])"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "3fe75875",
   "metadata": {},
   "source": [
    "- 추가적으로 직관에 맞지않는 단어들을 제거해 주었다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c91ae3e4",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "genre_keywords_stopwords = {}\n",
    "\n",
    "for k in kr_keywords.keys():\n",
    "    print(k)\n",
    "    words = []\n",
    "    for word, r in sorted(kr_keywords[k].items(), key=lambda x:x[1], reverse=True):\n",
    "        if word not in common_keywords:\n",
    "            words.append(word)\n",
    "    print(words[:30])\n",
    "    print()\n",
    "    genre_keywords[k] = words[:30]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1fd02a09",
   "metadata": {},
   "source": [
    "genre_keywords"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b1100324",
   "metadata": {},
   "source": [
    "pd.DataFrame(genre_keywords).T.to_csv('kr_keywords.csv')"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "96714893",
   "metadata": {},
   "source": [
    "## weat score 다시 측정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547a4eb0",
   "metadata": {},
   "source": [
    "target_art = ['아빠', '음악', '결심', '운명', '지금', '여인', '이름', '이후', '준비', '감정', '만난', '처음', '충격', '누구', '그린']\n",
    "     \n",
    "     \n",
    "target_gen = ['서울', '애니메이션', '여성', '가지', '주인공', '대해', '연출', '사회', '다큐멘터리', '부문', '섹스', '바로', '의도', '계획', '정체']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60831073",
   "metadata": {},
   "source": [
    "- 예술 영화를 대표하는 단어와 일반영화를 대표하는 단어 자체도 이미 장르에 편향이 되어있는 것으로 보아 다시 키워드 추출부터 하고자 한다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06445f79",
   "metadata": {},
   "source": [
    "### 예술영화와 일반영화 키워드 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "935f7ca2",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "art_txt = 'synopsis_art.txt'\n",
    "gen_txt = 'synopsis_gen.txt'\n",
    "\n",
    "art_gen_txt = [art_txt, gen_txt]\n",
    "art_gen_txt "
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "21861dee",
   "metadata": {},
   "source": [
    "texts = read_files(art_gen_txt)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "973c6730",
   "metadata": {},
   "source": [
    "# art_gen_texts = preprocess_texts_korean(texts)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "4282e7d7",
   "metadata": {},
   "source": [
    "len(art_gen_texts)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "1d0e48f7",
   "metadata": {},
   "source": [
    "for idx in trange(len(art_gen_txt)):\n",
    "    file_name = \"nouns_\"+art_gen_txt[idx]\n",
    "\n",
    "    with open(file_name, 'w+') as file:\n",
    "        for num, i in enumerate(art_gen_texts[idx]):\n",
    "            if num+1 < len(art_gen_texts[idx]):\n",
    "                file.write(', '.join(i) + \"\\n\")\n",
    "            else:\n",
    "                file.write(', '.join(i))"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "50e33b7b",
   "metadata": {},
   "source": [
    "## 데이터셋 다시 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "565095b6",
   "metadata": {},
   "source": [
    "art_gen_texts = []\n",
    "\n",
    "for txt in tqdm(art_gen_txt):\n",
    "    li = []\n",
    "    with open(\"nouns_\" + txt, \"r\") as file:\n",
    "        for fi in file:\n",
    "            ll = [ name.strip() for name in fi.split(\",\")]\n",
    "            li.append(ll)\n",
    "    art_gen_texts.append(li)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bb03d818",
   "metadata": {},
   "source": [
    "len(art_gen_texts)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "548eb3b9",
   "metadata": {},
   "source": [
    "art_gen_keywords = {}\n",
    "name = ['art','gen']\n",
    "\n",
    "for i in range(len(art_gen_texts)):\n",
    "    keywords = get_krwordrank(art_gen_texts[i], min_count, max_length, beta, max_iter)\n",
    "    print(name[i])\n",
    "    words = []\n",
    "    for word, r in sorted(keywords.items(), key=lambda x:x[1], reverse=True):\n",
    "        if word not in common_keywords:\n",
    "            words.append(word)\n",
    "    print(words[:30])\n",
    "    print()\n",
    "    art_gen_keywords[name[i]] = words"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "9c283fe2",
   "metadata": {},
   "source": [
    "### 겹치는 단어들을 제거하고 상위 15개 단어를 선정했다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e93d3497",
   "metadata": {},
   "source": [
    "art_gen_list = list(art_gen_keywords.values())"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "331d30cc",
   "metadata": {},
   "source": [
    "art = art_gen_list[0]\n",
    "gen = art_gen_list[1]\n",
    "\n",
    "art = [x for x in art if x not in gen[:30]]\n",
    "gen = [x for x in gen if x not in art[:30]]\n",
    "\n",
    "target_art = art[:15]\n",
    "target_gen = gen[:15]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3046e1fb",
   "metadata": {},
   "source": [
    "print('예술 영화의 주요 키워드 :', ' '.join(target_art))\n",
    "print('일반 영화의 주요 키워드 :', ' '.join(target_gen))"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "8d17b776",
   "metadata": {},
   "source": [
    "### weat score 다시 계산하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "59a0ba2f",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "matrix = [[0 for _ in range(len(genre_name))] for _ in trange(len(genre_name))]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d88f65c1",
   "metadata": {},
   "source": [
    "attrs = list(genre_keywords.values())"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b9da0d3c",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "len(attrs)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0bbf71d9",
   "metadata": {},
   "source": [
    "from gensim.models import KeyedVectors"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "bdc3d44a",
   "metadata": {},
   "source": [
    "model = KeyedVectors.load_word2vec_format(\"./w2v\")\n",
    "print(\"모델  load 완료!\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "91c49300",
   "metadata": {},
   "source": [
    "X = np.array([model[word] for word in target_art])\n",
    "Y = np.array([model[word] for word in target_gen])\n",
    "\n",
    "for i in range(len(genre_name)-1):\n",
    "    for j in range(i+1, len(genre_name)):\n",
    "        A = np.array([model[word] for word in attrs[i] if word in model])\n",
    "        B = np.array([model[word] for word in attrs[j] if word in model])\n",
    "        matrix[i][j] = weat_score(X, Y, A, B)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "ee741106",
   "metadata": {},
   "source": [
    "matrix를 채워보았습니다. WEAT score 값을 보고, 과연 우리의 직관과 비슷한지 살펴볼까요?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3de148e7",
   "metadata": {},
   "source": [
    "import pandas as pd"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "cbabab5d",
   "metadata": {},
   "source": [
    "for i in range(len(genre_name)-1):\n",
    "    for j in range(i+1, len(genre_name)):\n",
    "        print(genre_name[i], genre_name[j],matrix[i][j])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "155da4e9",
   "metadata": {},
   "source": [
    "corr_df = pd.DataFrame(matrix, columns=genre_name, index=genre_name)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ccc8db71",
   "metadata": {},
   "source": [
    "corr_df_unstack = pd.DataFrame(corr_df.unstack(), columns=['score']).sort_values(by='score')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b0611383",
   "metadata": {},
   "source": [
    "corr_df_unstack.reset_index(inplace=True)\n",
    "corr_df_unstack.columns = ['A','B','score']"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6bf4c75f",
   "metadata": {},
   "source": [
    "corr_df_unstack.to_csv('new_corr_df_unstack.csv', index=False)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "3060ecbf",
   "metadata": {},
   "source": [
    "corr_df_unstack[np.abs(corr_df_unstack.score) >= 0.6]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "69ea4131",
   "metadata": {},
   "source": [
    "high_corr = corr_df_unstack[np.abs(corr_df_unstack.score) >= 0.7]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "9a159af9",
   "metadata": {},
   "source": [
    "high_corr"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "ea111acb",
   "metadata": {},
   "source": [
    "### 예술 영화와 가까운 장르"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "7922792f",
   "metadata": {},
   "source": [
    "x1 = high_corr[high_corr.score > 0].A.tolist()\n",
    "x2 = high_corr[high_corr.score < 0].B.tolist()\n",
    "\n",
    "print(x1)\n",
    "print(x2)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "552dfd59",
   "metadata": {},
   "source": [
    "### 일반영화와 가까운 장르"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "4e3ccba8",
   "metadata": {},
   "source": [
    "y1 = high_corr[high_corr.score < 0].A.tolist()\n",
    "y2 = high_corr[high_corr.score > 0].B.tolist()\n",
    "\n",
    "print(y1)\n",
    "print(y2)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "4990105b",
   "metadata": {},
   "source": [
    "- 예술 영화에 대한 키워드가 아쉬웠는지 직관과 유사한 결과가 나오지 않아서 아쉽다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4764ee1",
   "metadata": {},
   "source": [
    "## STEP 4. WEAT score 계산과 시각화\n",
    "---\n",
    "영화 구분, 영화 장르에 따른 편향성을 측정하여 WEAT score로 계산해 보고 이를 Heatmap 형태로 시각화해 봅시다. 편향성이 두드러지는 영화장르 attribute 구성에는 어떤 케이스가 있는지 시각적으로 두드러지게 구성되면 좋습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "ac1f3e65",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np; \n",
    "import seaborn as sns; \n",
    "\n",
    "np.random.seed(0)\n",
    "plt.figure(figsize=(15,8))\n",
    "# 한글 지원 폰트\n",
    "sns.set(font='NanumGothic')\n",
    "\n",
    "# 마이너스 부호 \n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "ax = sns.heatmap(matrix, xticklabels=genre_name, yticklabels=genre_name, annot=True,  cmap='RdYlGn_r')\n",
    "ax"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "e9fbeaea",
   "metadata": {},
   "source": [
    "## KeyBert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "464c0e01",
   "metadata": {},
   "source": [
    "# !pip install keybert"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "a096b8ae",
   "metadata": {},
   "source": [
    "from keybert import KeyBERT\n",
    "\n",
    "doc = texts[0]\n",
    "# kw_model = KeyBERT()\n",
    "# keywords = kw_model.extract_keywords(doc)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "a56c738d",
   "metadata": {},
   "source": [
    "' '.join(common_keywords)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "6a6c2b05",
   "metadata": {},
   "source": [
    "from keybert import KeyBERT\n",
    "from transformers import BertModel"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "911c4d7e",
   "metadata": {},
   "source": [
    "model = BertModel.from_pretrained('skt/kobert-base-v1')\n",
    "kw_model = KeyBERT(model)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "9ffdf10c",
   "metadata": {},
   "source": [
    "input_texts = [[' '.join(text) for text in texts] for texts in processed_texts]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "dff41318",
   "metadata": {},
   "source": [
    "len(input_texts)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "852e3625",
   "metadata": {},
   "source": [
    "doc = ' '.join(input_texts[0])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "576bd2ae",
   "metadata": {},
   "source": [
    "genre_name[0]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "fcae5518",
   "metadata": {},
   "source": [
    "keybert_kw = {}\n",
    "\n",
    "for i in trange(len(input_texts)):\n",
    "    kw = kw_model.extract_keywords(' '.join(input_texts[i]), keyphrase_ngram_range=(1, 1), top_n=20)\n",
    "    keybert_kw[genre_name[i]] = kw\n",
    "    \n",
    "    print(genre_name[i])\n",
    "    for k, _ in kw:\n",
    "        print(k, end=' ')\n",
    "        \n",
    "    print()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "c33eb2b5",
   "metadata": {},
   "source": [
    "- 생각해보니 이 데이터는 하나의 시놉시스를 담고 있기 때문에 그 시놉시스에 specific하게 키워드가 추출되는 것 같다\n",
    "- 더 많은 데이터가 있다면 더 좋은 키워드가 뽑힐 수 있지 않을까 싶다\n",
    "- keyBert는 인물 위주로 많이 뽑히는 것 같다 왜그럴까?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "bc3bee7a",
   "metadata": {},
   "source": [
    "for key in keybert_kw :\n",
    "    val = keybert_kw[key]\n",
    "    print(f'{key} :', ', '.join([v[0] for v in val]))\n",
    "    print()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "d7ab0930",
   "metadata": {},
   "source": [
    "## 전체 데이터 비지도학습으로 분리 (LDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "0bbfc791",
   "metadata": {},
   "source": [
    "docs = [' '.join(t) for text in processed_texts for t in text]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "f2bb026a",
   "metadata": {},
   "source": [
    "# 상위 1,000개의 단어를 보존 \n",
    "vectorizer = TfidfVectorizer(max_features= 1000)\n",
    "X = vectorizer.fit_transform(docs)\n",
    "\n",
    "# TF-IDF 행렬의 크기 확인\n",
    "print('TF-IDF 행렬의 크기 :', X.shape)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "1a808ebe",
   "metadata": {},
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "4bf01f68",
   "metadata": {},
   "source": [
    "lda_model = LatentDirichletAllocation(n_components=21,learning_method='online',random_state=777, max_iter=1)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "1343e1b8",
   "metadata": {},
   "source": [
    "lda_top = lda_model.fit_transform(X)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "ec96bffb",
   "metadata": {},
   "source": [
    "print(lda_model.components_)\n",
    "print(lda_model.components_.shape)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "ea7571c6",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# 단어 집합. 1,000개의 단어가 저장됨.\n",
    "terms = vectorizer.get_feature_names()\n",
    "\n",
    "def get_topics(components, feature_names, n=5):\n",
    "    for idx, topic in enumerate(components):\n",
    "        print(\"Topic %d:\" % (idx+1), [feature_names[i] for i in topic.argsort()[:-n - 1:-1]])\n",
    "\n",
    "get_topics(lda_model.components_, terms, n=10)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "bc75c833",
   "metadata": {},
   "source": [
    "# !pip install pyLDAvis"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "b6165569",
   "metadata": {},
   "source": [
    "from gensim.models.ldamodel import LdaModel\n",
    "import pyLDAvis.gensim"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "95553753",
   "metadata": {},
   "source": [
    "import gensim\n",
    "\n",
    "all_tokens = [t for text in processed_texts for t in text]\n",
    "dictionary = gensim.corpora.Dictionary(all_tokens)\n",
    "# 출현빈도가 적거나 자주 등장하는 단어는 제거\n",
    "dictionary.filter_extremes(no_below=10, no_above=0.7)\n",
    "print('Number of unique tokens: %d' % len(dictionary))\n",
    "\n",
    "corpus = [dictionary.doc2bow(text) for text in all_tokens]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "43cb9129",
   "metadata": {},
   "source": [
    "num_topics = 21"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "b428aa07",
   "metadata": {},
   "source": [
    "lda_model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=num_topics)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "24da1b15",
   "metadata": {},
   "source": [
    "# Get the topics\n",
    "topics = lda_model.show_topics(num_topics=num_topics, num_words=10, log=False, formatted=False)\n",
    "\n",
    "# Print the topics\n",
    "for topic_id, topic in topics:\n",
    "    print(\"Topic: {}\".format(topic_id))\n",
    "    print(\"Words: {}\".format([word for word, _ in topic]))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "1fbd5351",
   "metadata": {},
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "import seaborn as sns\n",
    "\n",
    "# 한글 지원 폰트\n",
    "sns.set(font='NanumGothic')\n",
    "\n",
    "# 워드클라우드를 그릴 수 있는 레이아웃 크기 설정\n",
    "num_rows = 5\n",
    "num_cols = 5\n",
    "\n",
    "# Figure 크기 설정\n",
    "fig, axes = plt.subplots(num_rows, num_cols, figsize=(20, 20))\n",
    "\n",
    "# 각 subplot에 워드클라우드 그리기\n",
    "for topic_id, topic in enumerate(lda_model.print_topics(num_topics=num_topics, num_words=20)):\n",
    "    topic_words = \" \".join([word.split(\"*\")[1].strip() for word in topic[1].split(\" + \")])\n",
    "    wordcloud = WordCloud(width=800, height=800, random_state=21, max_font_size=110, font_path='/usr/share/fonts/truetype/nanum/NanumGothic.ttf').generate(topic_words)\n",
    "    \n",
    "    ax = axes[topic_id // num_cols, topic_id % num_cols]\n",
    "    ax.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "    ax.axis(\"off\")\n",
    "    ax.set_title(\"Topic: {}\".format(topic_id), fontsize=15)\n",
    "\n",
    "# 빈 subplot 처리\n",
    "for i in range(len(lda_model.print_topics(num_topics=num_topics, num_words=20)), num_rows * num_cols):\n",
    "    ax = axes[i // num_cols, i % num_cols]\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "plt.tight_layout(pad=0)\n",
    "plt.show()\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "563fe33e",
   "metadata": {},
   "source": [
    "- topic 7 : 가족영화처럼 보인다\n",
    "- topic 11, 18 : 애니메이션, 영화제, 부천 판타스틱 영화제 등 예술 영화 관련 내용들로 보인다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f14f2c",
   "metadata": {},
   "source": [
    "## 예술영화와 일반영화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "504e7507",
   "metadata": {},
   "source": [
    "art_gen_docs = [' '.join(t) for text in art_gen_texts for t in text]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "17e73250",
   "metadata": {},
   "source": [
    "# 상위 1,000개의 단어를 보존 \n",
    "ag_vectorizer = TfidfVectorizer(max_features= 1000)\n",
    "ag_X = ag_vectorizer.fit_transform(art_gen_docs)\n",
    "\n",
    "# TF-IDF 행렬의 크기 확인\n",
    "print('TF-IDF 행렬의 크기 :', ag_X.shape)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "cb576c78",
   "metadata": {},
   "source": [
    "lda_model = LatentDirichletAllocation(n_components=2,learning_method='online',random_state=777, max_iter=1)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "d111c57b",
   "metadata": {},
   "source": [
    "lda_top = lda_model.fit_transform(ag_X)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "16dac84e",
   "metadata": {},
   "source": [
    "print(lda_model.components_)\n",
    "print(lda_model.components_.shape)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "3e5c81bc",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# 단어 집합. 1,000개의 단어가 저장됨.\n",
    "terms = ag_vectorizer.get_feature_names()\n",
    "get_topics(lda_model.components_, terms, n=10)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "0db3f819",
   "metadata": {},
   "source": [
    "## 결론\n",
    "- 결론적으로 이번 task는 시놉시스도 부족하고 일반영화와 예술영화의 label의 문제도 있을 것이고 데이터도 단어의 양적인 측면에서 일반영화의 데이터가 훨씬 많고, 예술영화가 그에 비해 반도 안되는 용량이였기에 데이터도 불균형 한 문제가 있다\n",
    "- tf-idf의 결과가 우리의 직관과 유사한 결론이 나왔지만 이 결과도 사실 이미 일반 영화의 키워드에 '다큐멘터리','섹스'등 특정 장르를 지칭하는 단어들이 존재하였고, 이에 따라 나온 결과로 볼 수 있다\n",
    "- 더 많은 데이터셋으로 장르를 대표할 수 있을 키워드를 찾는 것이 더 합당한 결론이 나올 것으로 보인다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b77069",
   "metadata": {},
   "source": [
    "# 회고\n",
    "- 배운 점 \n",
    "    - 기존에 배웠던 토픽 모델링을 적용해보고 나아가 bertopic도 해봤다\n",
    "- 아쉬운 점 \n",
    "    - 좀 더 체계적으로 비교를 해보고 싶었는데 시간상의 이유로 제대로 비교해보지 못해서 아쉽다\n",
    "    - 생각보다 좋은 결과가 나오지 않아서 아쉽다\n",
    "- 느낀 점\n",
    "    - 시간이 너무 오래걸리고 내 가정과 비슷한 결론을 가져가는 것은 쉽지 않다\n",
    "    - 더 많은 데이터가 필요하다\n",
    "- 어려웠던 점 \n",
    "    - 커널이 죽지 않게 잘 관리하는 것이 어렵다"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
