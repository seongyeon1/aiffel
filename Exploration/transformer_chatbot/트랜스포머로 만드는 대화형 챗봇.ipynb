{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53c65dc3",
   "metadata": {},
   "source": [
    "## íŠ¸ëœìŠ¤í¬ë¨¸ì˜ ì…ë ¥ ì´í•´í•˜ê¸°\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0955a66c",
   "metadata": {},
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65831e24",
   "metadata": {},
   "source": [
    "# í¬ì§€ì…”ë„ ì¸ì½”ë”© ë ˆì´ì–´\n",
    "class PositionalEncoding(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, position, d_model):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.pos_encoding = self.positional_encoding(position, d_model)\n",
    "\n",
    "    def get_angles(self, position, i, d_model):\n",
    "        angles = 1 / tf.pow(10000, (2 * (i // 2)) / tf.cast(d_model, tf.float32))\n",
    "        return position * angles\n",
    "\n",
    "    def positional_encoding(self, position, d_model):\n",
    "        # ê°ë„ ë°°ì—´ ìƒì„±\n",
    "        angle_rads = self.get_angles(\n",
    "            position=tf.range(position, dtype=tf.float32)[:, tf.newaxis],\n",
    "            i=tf.range(d_model, dtype=tf.float32)[tf.newaxis, :],\n",
    "            d_model=d_model)\n",
    "\n",
    "        # ë°°ì—´ì˜ ì§ìˆ˜ ì¸ë±ìŠ¤ì—ëŠ” sin í•¨ìˆ˜ ì ìš©\n",
    "        sines = tf.math.sin(angle_rads[:, 0::2])\n",
    "        # ë°°ì—´ì˜ í™€ìˆ˜ ì¸ë±ìŠ¤ì—ëŠ” cosine í•¨ìˆ˜ ì ìš©\n",
    "        cosines = tf.math.cos(angle_rads[:, 1::2])\n",
    "\n",
    "        # sinê³¼ cosineì´ êµì°¨ë˜ë„ë¡ ì¬ë°°ì—´\n",
    "        pos_encoding = tf.stack([sines, cosines], axis=0)\n",
    "        pos_encoding = tf.transpose(pos_encoding,[1, 2, 0]) \n",
    "        pos_encoding = tf.reshape(pos_encoding, [position, d_model])\n",
    "\n",
    "        pos_encoding = pos_encoding[tf.newaxis, ...]\n",
    "        return tf.cast(pos_encoding, tf.float32)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return inputs + self.pos_encoding[:, :tf.shape(inputs)[1], :]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99c33d78",
   "metadata": {},
   "source": [
    "position = 50\n",
    "d_model = 512\n",
    "\n",
    "sample_pos_encoding = PositionalEncoding(position, d_model)\n",
    "\n",
    "plt.pcolormesh(sample_pos_encoding.pos_encoding.numpy()[0], cmap='RdBu')\n",
    "plt.xlabel('Depth')\n",
    "plt.xlim((0, 512))\n",
    "plt.ylabel('Position')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "dc07a313",
   "metadata": {},
   "source": [
    "**Q. ì„ë² ë”© ë²¡í„°ì˜ ì°¨ì›ì´ 256ì´ê³  ìµœëŒ€ ë¬¸ì¥ì˜ ê¸¸ì´ê°€ 30ì¸ í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥ìœ¼ë¡œ í•˜ëŠ” íŠ¸ëœìŠ¤í¬ë¨¸ë¥¼ êµ¬í˜„í•œë‹¤ê³  í•˜ì˜€ì„ ë•Œ, ì ì ˆí•œ í¬ì§€ì…”ë„ ì¸ì½”ë”© í–‰ë ¬ì˜ í¬ê¸°ë¥¼ ì¶”ì¸¡í•´ë³´ê³  ìœ„ì— êµ¬í˜„í•œ í¬ì§€ì…”ë„ ì¸ì½”ë”© ë ˆì´ì–´ë¥¼ ì‚¬ìš©í•´ í‘œí˜„í•´ ë³´ì„¸ìš”.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1abe2abf",
   "metadata": {},
   "source": [
    "d_model = 256\n",
    "position = 30"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a2ed5b7",
   "metadata": {},
   "source": [
    "sample_pos_encoding = PositionalEncoding(position, d_model)\n",
    "\n",
    "plt.pcolormesh(sample_pos_encoding.pos_encoding.numpy()[0], cmap='RdBu')\n",
    "plt.xlabel('Depth')\n",
    "plt.xlim((0, 256))\n",
    "plt.ylabel('Position')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "21b46852",
   "metadata": {},
   "source": [
    "## ì–´í…ì…˜? ì–´í…ì…˜!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96f60d2",
   "metadata": {},
   "source": [
    "êµ¬í˜„í•˜ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dde582d0",
   "metadata": {},
   "source": [
    "# ìŠ¤ì¼€ì¼ë“œ ë‹· í”„ë¡œë•íŠ¸ ì–´í…ì…˜ í•¨ìˆ˜\n",
    "def scaled_dot_product_attention(query, key, value, mask):\n",
    "    # ì–´í…ì…˜ ê°€ì¤‘ì¹˜ëŠ” Qì™€ Kì˜ ë‹· í”„ë¡œë•íŠ¸\n",
    "    matmul_qk = tf.matmul(query, key, transpose_b=True)\n",
    "\n",
    "    # ê°€ì¤‘ì¹˜ë¥¼ ì •ê·œí™”\n",
    "    depth = tf.cast(tf.shape(key)[-1], tf.float32)\n",
    "    logits = matmul_qk / tf.math.sqrt(depth)\n",
    "\n",
    "    # íŒ¨ë”©ì— ë§ˆìŠ¤í¬ ì¶”ê°€\n",
    "    if mask is not None:\n",
    "        logits += (mask * -1e9)\n",
    "\n",
    "    # softmaxì ìš©\n",
    "    attention_weights = tf.nn.softmax(logits, axis=-1)\n",
    "\n",
    "    # ìµœì¢… ì–´í…ì…˜ì€ ê°€ì¤‘ì¹˜ì™€ Vì˜ ë‹· í”„ë¡œë•íŠ¸\n",
    "    output = tf.matmul(attention_weights, value)\n",
    "    return output\n",
    "\n",
    "print(\"ìŠ=3\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4ab3c8ed",
   "metadata": {},
   "source": [
    "512 / 8"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "b0cc1b61",
   "metadata": {},
   "source": [
    "ë©€í‹° í—¤ë“œ ì–´í…ì…˜ì„ êµ¬í˜„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1de5778d",
   "metadata": {},
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, d_model, num_heads, name=\"multi_head_attention\"):\n",
    "        super(MultiHeadAttention, self).__init__(name=name)\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        assert d_model % self.num_heads == 0\n",
    "\n",
    "        self.depth = d_model // self.num_heads\n",
    "\n",
    "        self.query_dense = tf.keras.layers.Dense(units=d_model)\n",
    "        self.key_dense = tf.keras.layers.Dense(units=d_model)\n",
    "        self.value_dense = tf.keras.layers.Dense(units=d_model)\n",
    "\n",
    "        self.dense = tf.keras.layers.Dense(units=d_model)\n",
    "\n",
    "    def split_heads(self, inputs, batch_size):\n",
    "        inputs = tf.reshape(\n",
    "            inputs, shape=(batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(inputs, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, inputs):\n",
    "        query, key, value, mask = inputs['query'], inputs['key'], inputs['value'], inputs['mask']\n",
    "        batch_size = tf.shape(query)[0]\n",
    "\n",
    "        # Q, K, Vì— ê°ê° Denseë¥¼ ì ìš©í•©ë‹ˆë‹¤\n",
    "        query = self.query_dense(query)\n",
    "        key = self.key_dense(key)\n",
    "        value = self.value_dense(value)\n",
    "\n",
    "        # ë³‘ë ¬ ì—°ì‚°ì„ ìœ„í•œ ë¨¸ë¦¬ë¥¼ ì—¬ëŸ¬ ê°œ ë§Œë“­ë‹ˆë‹¤\n",
    "        query = self.split_heads(query, batch_size)\n",
    "        key = self.split_heads(key, batch_size)\n",
    "        value = self.split_heads(value, batch_size)\n",
    "\n",
    "        # ìŠ¤ì¼€ì¼ë“œ ë‹· í”„ë¡œë•íŠ¸ ì–´í…ì…˜ í•¨ìˆ˜\n",
    "        scaled_attention = scaled_dot_product_attention(query, key, value, mask)\n",
    "\n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
    "\n",
    "        # ì–´í…ì…˜ ì—°ì‚° í›„ì— ê° ê²°ê³¼ë¥¼ ë‹¤ì‹œ ì—°ê²°(concatenate)í•©ë‹ˆë‹¤\n",
    "        concat_attention = tf.reshape(scaled_attention,\n",
    "                                      (batch_size, -1, self.d_model))\n",
    "\n",
    "        # ìµœì¢… ê²°ê³¼ì—ë„ Denseë¥¼ í•œ ë²ˆ ë” ì ìš©í•©ë‹ˆë‹¤\n",
    "        outputs = self.dense(concat_attention)\n",
    "\n",
    "        return outputs\n",
    "print(\"ìŠ=3\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "702af551",
   "metadata": {},
   "source": [
    "íŒ¨ë”© ë§ˆìŠ¤í‚¹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e9e9c4d0",
   "metadata": {},
   "source": [
    "def create_padding_mask(x):\n",
    "    mask = tf.cast(tf.math.equal(x, 0), tf.float32)\n",
    "    # (batch_size, 1, 1, sequence length)\n",
    "    return mask[:, tf.newaxis, tf.newaxis, :]\n",
    "print(\"ìŠ=3\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ac1fb5fc",
   "metadata": {},
   "source": [
    "print(create_padding_mask(tf.constant([[1, 2, 0, 3, 0], [0, 0, 0, 4, 5]])))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d117b685",
   "metadata": {},
   "source": [
    "def create_look_ahead_mask(x):\n",
    "    seq_len = tf.shape(x)[1]\n",
    "    look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
    "    padding_mask = create_padding_mask(x)\n",
    "    return tf.maximum(look_ahead_mask, padding_mask)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b63cd753",
   "metadata": {},
   "source": [
    "print(create_look_ahead_mask(tf.constant([[1, 2, 3, 4, 5]])))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "700726df",
   "metadata": {},
   "source": [
    "print(create_look_ahead_mask(tf.constant([[0, 5, 1, 5, 5]])))"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "0da6cd2f",
   "metadata": {},
   "source": [
    "## ì¸ì½”ë”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1fedde4a",
   "metadata": {},
   "source": [
    "# ì¸ì½”ë” í•˜ë‚˜ì˜ ë ˆì´ì–´ë¥¼ í•¨ìˆ˜ë¡œ êµ¬í˜„.\n",
    "# ì´ í•˜ë‚˜ì˜ ë ˆì´ì–´ ì•ˆì—ëŠ” ë‘ ê°œì˜ ì„œë¸Œ ë ˆì´ì–´ê°€ ì¡´ì¬í•©ë‹ˆë‹¤.\n",
    "def encoder_layer(units, d_model, num_heads, dropout, name=\"encoder_layer\"):\n",
    "    inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n",
    "\n",
    "    # íŒ¨ë”© ë§ˆìŠ¤í¬ ì‚¬ìš©\n",
    "    padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
    "\n",
    "    # ì²« ë²ˆì§¸ ì„œë¸Œ ë ˆì´ì–´ : ë©€í‹° í—¤ë“œ ì–´í…ì…˜ ìˆ˜í–‰ (ì…€í”„ ì–´í…ì…˜)\n",
    "    attention = MultiHeadAttention(\n",
    "      d_model, num_heads, name=\"attention\")({\n",
    "          'query': inputs,\n",
    "          'key': inputs,\n",
    "          'value': inputs,\n",
    "          'mask': padding_mask\n",
    "      })\n",
    "\n",
    "    # ì–´í…ì…˜ì˜ ê²°ê³¼ëŠ” Dropoutê³¼ Layer Normalizationì´ë¼ëŠ” í›ˆë ¨ì„ ë•ëŠ” í…Œí¬ë‹‰ì„ ìˆ˜í–‰\n",
    "    attention = tf.keras.layers.Dropout(rate=dropout)(attention)\n",
    "    attention = tf.keras.layers.LayerNormalization(\n",
    "      epsilon=1e-6)(inputs + attention)\n",
    "\n",
    "    # ë‘ ë²ˆì§¸ ì„œë¸Œ ë ˆì´ì–´ : 2ê°œì˜ ì™„ì „ì—°ê²°ì¸µ\n",
    "    outputs = tf.keras.layers.Dense(units=units, activation='relu')(attention)\n",
    "    outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n",
    "\n",
    "    # ì™„ì „ì—°ê²°ì¸µì˜ ê²°ê³¼ëŠ” Dropoutê³¼ LayerNormalizationì´ë¼ëŠ” í›ˆë ¨ì„ ë•ëŠ” í…Œí¬ë‹‰ì„ ìˆ˜í–‰\n",
    "    outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
    "    outputs = tf.keras.layers.LayerNormalization(\n",
    "      epsilon=1e-6)(attention + outputs)\n",
    "\n",
    "    return tf.keras.Model(\n",
    "        inputs=[inputs, padding_mask], outputs=outputs, name=name)\n",
    "print(\"ìŠ=3\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d58933dc",
   "metadata": {},
   "source": [
    "def encoder(vocab_size, num_layers, units, d_model, num_heads, dropout, name=\"encoder\"):\n",
    "    inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n",
    "\n",
    "    # íŒ¨ë”© ë§ˆìŠ¤í¬ ì‚¬ìš©\n",
    "    padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
    "\n",
    "    # ì„ë² ë”© ë ˆì´ì–´\n",
    "    embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n",
    "    embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
    "\n",
    "    # í¬ì§€ì…”ë„ ì¸ì½”ë”©\n",
    "    embeddings = PositionalEncoding(vocab_size, d_model)(embeddings)\n",
    "\n",
    "    outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n",
    "\n",
    "    # num_layersë§Œí¼ ìŒ“ì•„ì˜¬ë¦° ì¸ì½”ë”ì˜ ì¸µ.\n",
    "    for i in range(num_layers):\n",
    "        outputs = encoder_layer(\n",
    "            units=units,\n",
    "            d_model=d_model,\n",
    "            num_heads=num_heads,\n",
    "            dropout=dropout,\n",
    "            name=\"encoder_layer_{}\".format(i),)([outputs, padding_mask])\n",
    "\n",
    "    return tf.keras.Model(\n",
    "      inputs=[inputs, padding_mask], outputs=outputs, name=name)\n",
    "\n",
    "print(\"ìŠ=3\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "335ea7b0",
   "metadata": {},
   "source": [
    "## ë””ì½”ë”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "847f90af",
   "metadata": {},
   "source": [
    "# ë””ì½”ë” í•˜ë‚˜ì˜ ë ˆì´ì–´ë¥¼ í•¨ìˆ˜ë¡œ êµ¬í˜„.\n",
    "# ì´ í•˜ë‚˜ì˜ ë ˆì´ì–´ ì•ˆì—ëŠ” ì„¸ ê°œì˜ ì„œë¸Œ ë ˆì´ì–´ê°€ ì¡´ì¬í•©ë‹ˆë‹¤.\n",
    "def decoder_layer(units, d_model, num_heads, dropout, name=\"decoder_layer\"):\n",
    "    inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n",
    "    enc_outputs = tf.keras.Input(shape=(None, d_model), name=\"encoder_outputs\")\n",
    "    look_ahead_mask = tf.keras.Input(\n",
    "      shape=(1, None, None), name=\"look_ahead_mask\")\n",
    "    padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\n",
    "    \n",
    "    #################################################\n",
    "    # ì²« ë²ˆì§¸ ì„œë¸Œ ë ˆì´ì–´ : ë©€í‹° í—¤ë“œ ì–´í…ì…˜ ìˆ˜í–‰ (ì…€í”„ ì–´í…ì…˜) #\n",
    "    ################################################\n",
    "    attention1 = MultiHeadAttention(\n",
    "      d_model, num_heads, name=\"attention_1\")(inputs={\n",
    "          'query': inputs,\n",
    "          'key': inputs,\n",
    "          'value': inputs,\n",
    "          'mask': look_ahead_mask\n",
    "      })\n",
    "\n",
    "    # ë©€í‹° í—¤ë“œ ì–´í…ì…˜ì˜ ê²°ê³¼ëŠ” LayerNormalizationì´ë¼ëŠ” í›ˆë ¨ì„ ë•ëŠ” í…Œí¬ë‹‰ì„ ìˆ˜í–‰\n",
    "    attention1 = tf.keras.layers.LayerNormalization(\n",
    "      epsilon=1e-6)(attention1 + inputs)\n",
    "\n",
    "    ##############################################################\n",
    "    # ë‘ ë²ˆì§¸ ì„œë¸Œ ë ˆì´ì–´ : ë§ˆìŠ¤í¬ë“œ ë©€í‹° í—¤ë“œ ì–´í…ì…˜ ìˆ˜í–‰ (ì¸ì½”ë”-ë””ì½”ë” ì–´í…ì…˜) #\n",
    "    ##############################################################\n",
    "    attention2 = MultiHeadAttention(\n",
    "      d_model, num_heads, name=\"attention_2\")(inputs={\n",
    "          'query': attention1,\n",
    "          'key': enc_outputs,\n",
    "          'value': enc_outputs,\n",
    "          'mask': padding_mask\n",
    "      })\n",
    "\n",
    "    # ë§ˆìŠ¤í¬ë“œ ë©€í‹° í—¤ë“œ ì–´í…ì…˜ì˜ ê²°ê³¼ëŠ”\n",
    "    # Dropoutê³¼ LayerNormalizationì´ë¼ëŠ” í›ˆë ¨ì„ ë•ëŠ” í…Œí¬ë‹‰ì„ ìˆ˜í–‰\n",
    "    attention2 = tf.keras.layers.Dropout(rate=dropout)(attention2)\n",
    "    attention2 = tf.keras.layers.LayerNormalization(\n",
    "      epsilon=1e-6)(attention2 + attention1)\n",
    "\n",
    "    ##################################\n",
    "    # ì„¸ ë²ˆì§¸ ì„œë¸Œ ë ˆì´ì–´ : 2ê°œì˜ ì™„ì „ì—°ê²°ì¸µ #\n",
    "    #################################\n",
    "    outputs = tf.keras.layers.Dense(units=units, activation='relu')(attention2)\n",
    "    outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n",
    "\n",
    "    # ì™„ì „ì—°ê²°ì¸µì˜ ê²°ê³¼ëŠ” Dropoutê³¼ LayerNormalization ìˆ˜í–‰\n",
    "    outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
    "    outputs = tf.keras.layers.LayerNormalization(\n",
    "      epsilon=1e-6)(outputs + attention2)\n",
    "\n",
    "    return tf.keras.Model(\n",
    "      inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],\n",
    "      outputs=outputs,\n",
    "      name=name)\n",
    "print(\"ìŠ=3\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "31fba871",
   "metadata": {},
   "source": [
    "ë””ì½”ë” ì¸µì„ ìŒ“ì•„ ë””ì½”ë” ë§Œë“¤ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a7f9f58e",
   "metadata": {},
   "source": [
    "def decoder(vocab_size,\n",
    "            num_layers,\n",
    "            units,\n",
    "            d_model,\n",
    "            num_heads,\n",
    "            dropout,\n",
    "            name='decoder'):\n",
    "    inputs = tf.keras.Input(shape=(None,), name='inputs')\n",
    "    enc_outputs = tf.keras.Input(shape=(None, d_model), name='encoder_outputs')\n",
    "    look_ahead_mask = tf.keras.Input(\n",
    "      shape=(1, None, None), name='look_ahead_mask')\n",
    "\n",
    "    # íŒ¨ë”© ë§ˆìŠ¤í¬\n",
    "    padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\n",
    "\n",
    "    # ì„ë² ë”© ë ˆì´ì–´\n",
    "    embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n",
    "    embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
    "\n",
    "    # í¬ì§€ì…”ë„ ì¸ì½”ë”©\n",
    "    embeddings = PositionalEncoding(vocab_size, d_model)(embeddings)\n",
    "\n",
    "    # Dropoutì´ë¼ëŠ” í›ˆë ¨ì„ ë•ëŠ” í…Œí¬ë‹‰ì„ ìˆ˜í–‰\n",
    "    outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n",
    "\n",
    "    for i in range(num_layers):\n",
    "        outputs = decoder_layer(\n",
    "            units=units,\n",
    "            d_model=d_model,\n",
    "            num_heads=num_heads,\n",
    "            dropout=dropout,\n",
    "            name='decoder_layer_{}'.format(i),\n",
    "        )(inputs=[outputs, enc_outputs, look_ahead_mask, padding_mask])\n",
    "\n",
    "    return tf.keras.Model(\n",
    "      inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],\n",
    "      outputs=outputs,\n",
    "      name=name)\n",
    "\n",
    "print(\"ìŠ=3\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "a64321a6",
   "metadata": {},
   "source": [
    "## ì±—ë´‡ì˜ ë³‘ë ¬ ë°ì´í„° ë°›ì•„ì˜¤ê¸°\n",
    "\n",
    "ì—¬ê¸°ì„œëŠ”Â Cornell Movie-Dialogs Corpusë¼ëŠ” ì˜í™” ë° TV í”„ë¡œê·¸ë¨ì—ì„œ ì‚¬ìš©ë˜ì—ˆë˜ ëŒ€í™”ì˜ ìŒìœ¼ë¡œ êµ¬ì„±ëœ ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. ëŒ€í™”ì˜ ìŒì´ë¼ê³  í•˜ëŠ” ê²ƒì€ ê¸°ë³¸ì ìœ¼ë¡œ ë¨¼ì € ë§í•˜ëŠ” ì‚¬ëŒì˜ ëŒ€í™” ë¬¸ì¥ì´ ìˆê³ , ê·¸ì— ì‘ë‹µí•˜ëŠ” ëŒ€í™” ë¬¸ì¥ì˜ ìŒìœ¼ë¡œ ì´ë£¨ì–´ì§‘ë‹ˆë‹¤.\n",
    "ë°ì´í„°ë¥¼ ë°›ì•„ì˜¤ëŠ” ì´ë²ˆ ìŠ¤í…ì—ì„œ ëª©í‘œë¡œ í•˜ëŠ” ê²ƒì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\n",
    "\n",
    "1. ì •í•´ì§„Â ê°œìˆ˜ì¸Â 50,000ê°œì˜Â ì§ˆë¬¸ê³¼Â ë‹µë³€ì˜Â ìŒì„Â ì¶”ì¶œí•œë‹¤.\n",
    "2. ë¬¸ì¥ì—ì„œ ë‹¨ì–´ì™€ êµ¬ë‘ì  ì‚¬ì´ì— ê³µë°±ì„ ì¶”ê°€í•œë‹¤.\n",
    "3. ì•ŒíŒŒë²³ê³¼ ! ? , . ì´ 4ê°œì˜ êµ¬ë‘ì ì„ ì œì™¸í•˜ê³  ë‹¤ë¥¸ íŠ¹ìˆ˜ë¬¸ìëŠ” ëª¨ë‘ ì œê±°í•œë‹¤.\n",
    "\n",
    "\n",
    "ë°ì´í„°ë¥¼ ë‹¤ìš´ë¡œë“œí•´ ë³´ê² ìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fd8a4b4d",
   "metadata": {},
   "source": [
    "path_to_zip = tf.keras.utils.get_file(\n",
    "    'cornell_movie_dialogs.zip',\n",
    "    origin='http://www.cs.cornell.edu/~cristian/data/cornell_movie_dialogs_corpus.zip',\n",
    "    extract=True)\n",
    "\n",
    "path_to_dataset = os.path.join(\n",
    "    os.path.dirname(path_to_zip), \"cornell movie-dialogs corpus\")\n",
    "\n",
    "path_to_movie_lines = os.path.join(path_to_dataset, 'movie_lines.txt')\n",
    "path_to_movie_conversations = os.path.join(path_to_dataset,'movie_conversations.txt')\n",
    "print(\"ìŠ=3\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "de37d26f",
   "metadata": {},
   "source": [
    "# ì‚¬ìš©í•  ìƒ˜í”Œì˜ ìµœëŒ€ ê°œìˆ˜\n",
    "MAX_SAMPLES = 50000\n",
    "print(MAX_SAMPLES)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7ad440ba",
   "metadata": {},
   "source": [
    "# ì „ì²˜ë¦¬ í•¨ìˆ˜\n",
    "def preprocess_sentence(sentence):\n",
    "    # ì…ë ¥ë°›ì€ sentenceë¥¼ ì†Œë¬¸ìë¡œ ë³€ê²½í•˜ê³  ì–‘ìª½ ê³µë°±ì„ ì œê±°\n",
    "    sentence = sentence.lower().strip()\n",
    "\n",
    "    # ë‹¨ì–´ì™€ êµ¬ë‘ì (punctuation) ì‚¬ì´ì˜ ê±°ë¦¬ë¥¼ ë§Œë“­ë‹ˆë‹¤.\n",
    "    # ì˜ˆë¥¼ ë“¤ì–´ì„œ \"I am a student.\" => \"I am a student .\"ì™€ ê°™ì´\n",
    "    # studentì™€ ì˜¨ì  ì‚¬ì´ì— ê±°ë¦¬ë¥¼ ë§Œë“­ë‹ˆë‹¤.\n",
    "    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "\n",
    "    # (a-z, A-Z, \".\", \"?\", \"!\", \",\")ë¥¼ ì œì™¸í•œ ëª¨ë“  ë¬¸ìë¥¼ ê³µë°±ì¸ ' 'ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.\n",
    "    sentence = re.sub(r'[^a-zA-Z.?!,]', ' ', sentence)\n",
    "    sentence = sentence.strip()\n",
    "    return sentence\n",
    "\n",
    "print(\"ìŠ=3\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f5afb3ad",
   "metadata": {},
   "source": [
    "# sentence = \"I am a student .\"\n",
    "# preprocess_sentence(sentence)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a2ad9c55",
   "metadata": {},
   "source": [
    "# ì§ˆë¬¸ê³¼ ë‹µë³€ì˜ ìŒì¸ ë°ì´í„°ì…‹ì„ êµ¬ì„±í•˜ê¸° ìœ„í•œ ë°ì´í„° ë¡œë“œ í•¨ìˆ˜\n",
    "def load_conversations():\n",
    "    id2line = {}\n",
    "    with open(path_to_movie_lines, errors='ignore') as file:\n",
    "        lines = file.readlines()\n",
    "    for line in lines:\n",
    "        parts = line.replace('\\n', '').split(' +++$+++ ')\n",
    "        id2line[parts[0]] = parts[4]\n",
    "\n",
    "    inputs, outputs = [], []\n",
    "    with open(path_to_movie_conversations, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    for line in lines:\n",
    "        parts = line.replace('\\n', '').split(' +++$+++ ')\n",
    "        conversation = [line[1:-1] for line in parts[3][1:-1].split(', ')]\n",
    "\n",
    "        for i in range(len(conversation) - 1):\n",
    "            # ì „ì²˜ë¦¬ í•¨ìˆ˜ë¥¼ ì§ˆë¬¸ì— í•´ë‹¹ë˜ëŠ” inputsì™€ ë‹µë³€ì— í•´ë‹¹ë˜ëŠ” outputsì— ì ìš©.\n",
    "            inputs.append(preprocess_sentence(id2line[conversation[i]]))\n",
    "            outputs.append(preprocess_sentence(id2line[conversation[i + 1]]))\n",
    "\n",
    "            if len(inputs) >= MAX_SAMPLES:\n",
    "                return inputs, outputs\n",
    "    return inputs, outputs\n",
    "print(\"ìŠ=3\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0102a912",
   "metadata": {},
   "source": [
    "# ë°ì´í„°ë¥¼ ë¡œë“œí•˜ê³  ì „ì²˜ë¦¬í•˜ì—¬ ì§ˆë¬¸ì„ questions, ë‹µë³€ì„ answersì— ì €ì¥í•©ë‹ˆë‹¤.\n",
    "questions, answers = load_conversations()\n",
    "print('ì „ì²´ ìƒ˜í”Œ ìˆ˜ :', len(questions))\n",
    "print('ì „ì²´ ìƒ˜í”Œ ìˆ˜ :', len(answers))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0e28a8bf",
   "metadata": {},
   "source": [
    "print('ì „ì²˜ë¦¬ í›„ì˜ 22ë²ˆì§¸ ì§ˆë¬¸ ìƒ˜í”Œ: {}'.format(questions[21]))\n",
    "print('ì „ì²˜ë¦¬ í›„ì˜ 22ë²ˆì§¸ ë‹µë³€ ìƒ˜í”Œ: {}'.format(answers[21]))"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "90798492",
   "metadata": {},
   "source": [
    "### ë³‘ë ¬ ë°ì´í„° ì „ì²˜ë¦¬í•˜ê¸°\n",
    "ì§ˆë¬¸ê³¼ ë‹µë³€ì˜ ì…‹ì„ ê°ê° questionsì™€ answersì— ì €ì¥í•˜ì˜€ìœ¼ë¯€ë¡œ, ë³¸ê²©ì ìœ¼ë¡œ ì „ì²˜ë¦¬ë¥¼ ì§„í–‰í•´ë³´ê² ìŠµë‹ˆë‹¤. ì´ë²ˆ ìŠ¤í…ì—ì„œ ì§„í–‰í•  ì „ì²´ì ì¸ ê³¼ì •ì„ ìš”ì•½í•˜ë©´ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\n",
    "\n",
    "1. TensorFlowÂ DatasetsÂ SubwordTextEncoderÂ ë¥¼Â í† í¬ë‚˜ì´ì €ë¡œÂ ì‚¬ìš©í•œë‹¤.Â  ë‹¨ì–´ë³´ë‹¤Â ë”Â ì‘ì€Â ë‹¨ìœ„ì¸Â Subwordë¥¼Â ê¸°ì¤€ìœ¼ë¡œÂ í† í¬ë‚˜ì´ì§•í•˜ê³ ,Â  ê°Â í† í°ì„Â ê³ ìœ í•œÂ ì •ìˆ˜ë¡œ ì¸ì½”ë”©Â í•œë‹¤.\n",
    "2. ê°Â ë¬¸ì¥ì„Â í† í°í™”í•˜ê³ Â ê°Â ë¬¸ì¥ì˜Â ì‹œì‘ê³¼Â ëì„Â ë‚˜íƒ€ë‚´ëŠ”Â `START_TOKEN`Â ë°Â `END_TOKEN`ì„Â ì¶”ê°€í•œë‹¤.\n",
    "3. ìµœëŒ€Â ê¸¸ì´Â MAX_LENGTHÂ ì¸Â 40ì„Â ë„˜ëŠ”Â ë¬¸ì¥ë“¤ì€Â í•„í„°ë§í•œë‹¤.MAX_LENGTHë³´ë‹¤Â ê¸¸ì´ê°€Â ì§§ì€Â ë¬¸ì¥ë“¤ì€Â 40ì—Â ë§ë„ë¡Â íŒ¨ë”©Â í•œë‹¤.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02018d4",
   "metadata": {},
   "source": [
    "1. ë‹¨ì–´ì¥(Vocabulary) ë§Œë“¤ê¸°\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "63ad8c1a",
   "metadata": {},
   "source": [
    "import tensorflow_datasets as tfds\n",
    "print(\"ì‚´ì§ ì˜¤ë˜ ê±¸ë¦´ ìˆ˜ ìˆì–´ìš”. ìŠ¤íŠ¸ë ˆì¹­ í•œ ë²ˆ í•´ë³¼ê¹Œìš”? ğŸ‘\")\n",
    "\n",
    "# ì§ˆë¬¸ê³¼ ë‹µë³€ ë°ì´í„°ì…‹ì— ëŒ€í•´ì„œ Vocabulary ìƒì„±\n",
    "tokenizer = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(questions + answers, target_vocab_size=2**13)\n",
    "print(\"ìŠ=3 \")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "abc3faa1",
   "metadata": {},
   "source": [
    "ì‹œì‘ í† í°ê³¼ ì¢…ë£Œ í† í°ì— ë¶€ì—¬ëœ ì •ìˆ˜ë¥¼ ì¶œë ¥í•´ë´…ì‹œë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "00b75af6",
   "metadata": {},
   "source": [
    "# ì‹œì‘ í† í°ê³¼ ì¢…ë£Œ í† í°ì— ê³ ìœ í•œ ì •ìˆ˜ë¥¼ ë¶€ì—¬í•©ë‹ˆë‹¤.\n",
    "START_TOKEN, END_TOKEN = [tokenizer.vocab_size], [tokenizer.vocab_size + 1]\n",
    "print(\"ìŠ=3\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1deeb652",
   "metadata": {},
   "source": [
    "print('START_TOKENì˜ ë²ˆí˜¸ :' ,[tokenizer.vocab_size])\n",
    "print('END_TOKENì˜ ë²ˆí˜¸ :' ,[tokenizer.vocab_size + 1])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b4de06e9",
   "metadata": {},
   "source": [
    "# ì‹œì‘ í† í°ê³¼ ì¢…ë£Œ í† í°ì„ ê³ ë ¤í•˜ì—¬ +2ë¥¼ í•˜ì—¬ ë‹¨ì–´ì¥ì˜ í¬ê¸°ë¥¼ ì‚°ì •í•©ë‹ˆë‹¤.\n",
    "VOCAB_SIZE = tokenizer.vocab_size + 2\n",
    "print(VOCAB_SIZE)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "c45868bc",
   "metadata": {},
   "source": [
    "ê°ê° 8,331ê³¼ 8,332ë¼ëŠ” ì ì—ì„œ í˜„ì¬ ë‹¨ì–´ì¥ì˜ í¬ê¸°ê°€ 8,331(0ë²ˆë¶€í„° 8,330ë²ˆ)ì´ë¼ëŠ” ì˜ë¯¸ì…ë‹ˆë‹¤.\n",
    "\n",
    "ë‘ ê°œì˜ í† í°ì„ ì¶”ê°€í•´ ì£¼ì—ˆê¸° ë•Œë¬¸ì— ë‹¨ì–´ì¥ì˜ í¬ê¸°ë„ +2ì„ì„ ëª…ì‹œí•´ ì£¼ì–´ì•¼ í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe8129d",
   "metadata": {},
   "source": [
    "## 2. ê° ë‹¨ì–´ë¥¼ ê³ ìœ í•œ ì •ìˆ˜ë¡œ ì¸ì½”ë”©(Integer encoding) & íŒ¨ë”©(Padding)\n",
    "ìœ„ì—ì„œ tensorflow_datasetsì˜ SubwordTextEncoderë¥¼ ì‚¬ìš©í•´ì„œ tokenizerë¥¼ ì •ì˜í•˜ê³  Vocabularyë¥¼ ë§Œë“¤ì—ˆë‹¤ë©´, tokenizer.encode()ë¡œ ê° ë‹¨ì–´ë¥¼ ì •ìˆ˜ë¡œ ë³€í™˜í•  ìˆ˜ ìˆê³  ë˜ëŠ” tokenizer.decode()ë¥¼ í†µí•´ ì •ìˆ˜ ì‹œí€€ìŠ¤ë¥¼ ë‹¨ì–´ ì‹œí€€ìŠ¤ë¡œ ë³€í™˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "ì˜ˆë¥¼ ë“¤ì–´ì„œ 22ë²ˆì§¸ ìƒ˜í”Œì„ tokenizer.encode()ì˜ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©í•´ì„œ ë³€í™˜ ê²°ê³¼ë¥¼ ë´…ì‹œë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5344c666",
   "metadata": {},
   "source": [
    "# ì„ì˜ì˜ 22ë²ˆì§¸ ìƒ˜í”Œì— ëŒ€í•´ì„œ ì •ìˆ˜ ì¸ì½”ë”© ì‘ì—…ì„ ìˆ˜í–‰.\n",
    "# ê° í† í°ì„ ê³ ìœ í•œ ì •ìˆ˜ë¡œ ë³€í™˜\n",
    "print('ì •ìˆ˜ ì¸ì½”ë”© í›„ì˜ 21ë²ˆì§¸ ì§ˆë¬¸ ìƒ˜í”Œ: {}'.format(tokenizer.encode(questions[21])))\n",
    "print('ì •ìˆ˜ ì¸ì½”ë”© í›„ì˜ 21ë²ˆì§¸ ë‹µë³€ ìƒ˜í”Œ: {}'.format(tokenizer.encode(answers[21])))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "cd3c1f1c",
   "metadata": {},
   "source": [
    "# ìƒ˜í”Œì˜ ìµœëŒ€ í—ˆìš© ê¸¸ì´ ë˜ëŠ” íŒ¨ë”© í›„ì˜ ìµœì¢… ê¸¸ì´\n",
    "MAX_LENGTH = 40\n",
    "print(MAX_LENGTH)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "93d0bfad",
   "metadata": {},
   "source": [
    "# ì •ìˆ˜ ì¸ì½”ë”©, ìµœëŒ€ ê¸¸ì´ë¥¼ ì´ˆê³¼í•˜ëŠ” ìƒ˜í”Œ ì œê±°, íŒ¨ë”©\n",
    "def tokenize_and_filter(inputs, outputs):\n",
    "    tokenized_inputs, tokenized_outputs = [], []\n",
    "\n",
    "    for (sentence1, sentence2) in zip(inputs, outputs):\n",
    "        # ì •ìˆ˜ ì¸ì½”ë”© ê³¼ì •ì—ì„œ ì‹œì‘ í† í°ê³¼ ì¢…ë£Œ í† í°ì„ ì¶”ê°€\n",
    "        sentence1 = START_TOKEN + tokenizer.encode(sentence1) + END_TOKEN\n",
    "        sentence2 = START_TOKEN + tokenizer.encode(sentence2) + END_TOKEN\n",
    "\n",
    "        # ìµœëŒ€ ê¸¸ì´ 40 ì´í•˜ì¸ ê²½ìš°ì—ë§Œ ë°ì´í„°ì…‹ìœ¼ë¡œ í—ˆìš©\n",
    "        if len(sentence1) <= MAX_LENGTH and len(sentence2) <= MAX_LENGTH:\n",
    "            tokenized_inputs.append(sentence1)\n",
    "            tokenized_outputs.append(sentence2)\n",
    "\n",
    "    # ìµœëŒ€ ê¸¸ì´ 40ìœ¼ë¡œ ëª¨ë“  ë°ì´í„°ì…‹ì„ íŒ¨ë”©\n",
    "    tokenized_inputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "      tokenized_inputs, maxlen=MAX_LENGTH, padding='post')\n",
    "    tokenized_outputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "      tokenized_outputs, maxlen=MAX_LENGTH, padding='post')\n",
    "\n",
    "    return tokenized_inputs, tokenized_outputs\n",
    "print(\"ìŠ=3\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1a1923b2",
   "metadata": {},
   "source": [
    "questions, answers = tokenize_and_filter(questions, answers)\n",
    "print('ë‹¨ì–´ì¥ì˜ í¬ê¸° :',(VOCAB_SIZE))\n",
    "print('í•„í„°ë§ í›„ì˜ ì§ˆë¬¸ ìƒ˜í”Œ ê°œìˆ˜: {}'.format(len(questions)))\n",
    "print('í•„í„°ë§ í›„ì˜ ë‹µë³€ ìƒ˜í”Œ ê°œìˆ˜: {}'.format(len(answers)))"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "4846a266",
   "metadata": {},
   "source": [
    "## 3. êµì‚¬ ê°•ìš”(Teacher Forcing) ì‚¬ìš©í•˜ê¸°\n",
    "tf.data.Dataset API ëŠ” í›ˆë ¨ í”„ë¡œì„¸ìŠ¤ì˜ ì†ë„ê°€ ë¹¨ë¼ì§€ë„ë¡ ì…ë ¥ íŒŒì´í”„ë¼ì¸ì„ êµ¬ì¶•í•˜ëŠ” APIì…ë‹ˆë‹¤.\n",
    "\n",
    "ì´ë¥¼ ì ê·¹ ì‚¬ìš©í•˜ê¸° ìœ„í•´ì„œ ì§ˆë¬¸ê³¼ ë‹µë³€ì˜ ìŒì„ tf.data.Datasetì˜ ì…ë ¥ìœ¼ë¡œ ë„£ì–´ì£¼ëŠ” ì‘ì—…ì„ í•©ë‹ˆë‹¤.\n",
    "\n",
    "ì´ë•Œ, ë””ì½”ë”ì˜ ì…ë ¥ê³¼ ì‹¤ì œê°’(ë ˆì´ë¸”)ì„ ì •ì˜í•´ ì£¼ê¸° ìœ„í•´ì„œëŠ” êµì‚¬ ê°•ìš”(Teacher Forcing) ì´ë¼ëŠ” ì–¸ì–´ ëª¨ë¸ì˜ í›ˆë ¨ ê¸°ë²•ì„ ì´í•´í•´ì•¼ë§Œ í•©ë‹ˆë‹¤. ì•„ë˜ì˜ ê¸€ì„ í†µí•´ êµì‚¬ ê°•ìš”ì— ëŒ€í•´ ì•Œì•„ë´…ì‹œë‹¤. (ëª¨ë‘ ì½ì„ í•„ìš”ëŠ” ì—†ê³ , êµì‚¬ ê°•ìš” ë¶€ë¶„ê¹Œì§€ë§Œ ì½ì–´ë„ ë©ë‹ˆë‹¤.)\n",
    "\n",
    "- ìœ„í‚¤ë…ìŠ¤: RNN ì–¸ì–´ ëª¨ë¸(https://wikidocs.net/46496)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee333865",
   "metadata": {},
   "source": [
    "- êµì‚¬ ê°•ìš”(teacher forcing) : \n",
    "    - í…ŒìŠ¤íŠ¸ ê³¼ì •ì—ì„œ t ì‹œì ì˜ ì¶œë ¥ì´ t+1 ì‹œì ì˜ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©ë˜ëŠ” RNN ëª¨ë¸ì„ í›ˆë ¨ì‹œí‚¬ ë•Œ ì‚¬ìš©í•˜ëŠ” í›ˆë ¨ ê¸°ë²•\n",
    "    - í›ˆë ¨í•  ë•Œ êµì‚¬ ê°•ìš”ë¥¼ ì‚¬ìš©í•  ê²½ìš°, ëª¨ë¸ì´ t ì‹œì ì—ì„œ ì˜ˆì¸¡í•œ ê°’ì„ t+1 ì‹œì ì— ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©í•˜ì§€ ì•Šê³ , t ì‹œì ì˜ ë ˆì´ë¸”. ì¦‰, ì‹¤ì œ ì•Œê³ ìˆëŠ” ì •ë‹µì„ t+1 ì‹œì ì˜ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd61ab5",
   "metadata": {},
   "source": [
    "ì§ˆë¬¸ê³¼ ë‹µë³€ì˜ ìŒì„ tf.data.Dataset APIì˜ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©í•˜ì—¬ íŒŒì´í”„ë¼ì¸ì„ êµ¬ì„±í•©ë‹ˆë‹¤. ì´ë•Œ, êµì‚¬ ê°•ìš”ë¥¼ ìœ„í•´ì„œ answers[:, :-1]ë¥¼ ë””ì½”ë”ì˜ ì…ë ¥ê°’, answers[:, 1:]ë¥¼ ë””ì½”ë”ì˜ ë ˆì´ë¸”ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5160c0a8",
   "metadata": {},
   "source": [
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 20000\n",
    "\n",
    "# ë””ì½”ë”ëŠ” ì´ì „ì˜ targetì„ ë‹¤ìŒì˜ inputìœ¼ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "# ì´ì— ë”°ë¼ outputsì—ì„œëŠ” START_TOKENì„ ì œê±°í•˜ê² ìŠµë‹ˆë‹¤.\n",
    "dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    {\n",
    "        'inputs': questions,\n",
    "        'dec_inputs': answers[:, :-1]\n",
    "    },\n",
    "    {\n",
    "        'outputs': answers[:, 1:]\n",
    "    },\n",
    "))\n",
    "\n",
    "dataset = dataset.cache()\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE)\n",
    "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "print(\"ìŠ=3\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6d740c37",
   "metadata": {},
   "source": [
    "def transformer(vocab_size,\n",
    "                num_layers,\n",
    "                units,\n",
    "                d_model,\n",
    "                num_heads,\n",
    "                dropout,\n",
    "                name=\"transformer\"):\n",
    "    inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n",
    "    dec_inputs = tf.keras.Input(shape=(None,), name=\"dec_inputs\")\n",
    "\n",
    "    # ì¸ì½”ë”ì—ì„œ íŒ¨ë”©ì„ ìœ„í•œ ë§ˆìŠ¤í¬\n",
    "    enc_padding_mask = tf.keras.layers.Lambda(\n",
    "      create_padding_mask, output_shape=(1, 1, None),\n",
    "      name='enc_padding_mask')(inputs)\n",
    "\n",
    "    # ë””ì½”ë”ì—ì„œ ë¯¸ë˜ì˜ í† í°ì„ ë§ˆìŠ¤í¬ í•˜ê¸° ìœ„í•´ì„œ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "    # ë‚´ë¶€ì ìœ¼ë¡œ íŒ¨ë”© ë§ˆìŠ¤í¬ë„ í¬í•¨ë˜ì–´ì ¸ ìˆìŠµë‹ˆë‹¤.\n",
    "    look_ahead_mask = tf.keras.layers.Lambda(\n",
    "      create_look_ahead_mask,\n",
    "      output_shape=(1, None, None),\n",
    "      name='look_ahead_mask')(dec_inputs)\n",
    "\n",
    "    # ë‘ ë²ˆì§¸ ì–´í…ì…˜ ë¸”ë¡ì—ì„œ ì¸ì½”ë”ì˜ ë²¡í„°ë“¤ì„ ë§ˆìŠ¤í‚¹\n",
    "    # ë””ì½”ë”ì—ì„œ íŒ¨ë”©ì„ ìœ„í•œ ë§ˆìŠ¤í¬\n",
    "    dec_padding_mask = tf.keras.layers.Lambda(\n",
    "      create_padding_mask, output_shape=(1, 1, None),\n",
    "      name='dec_padding_mask')(inputs)\n",
    "\n",
    "    # ì¸ì½”ë”\n",
    "    enc_outputs = encoder(\n",
    "      vocab_size=vocab_size,\n",
    "      num_layers=num_layers,\n",
    "      units=units,\n",
    "      d_model=d_model,\n",
    "      num_heads=num_heads,\n",
    "      dropout=dropout,\n",
    "    )(inputs=[inputs, enc_padding_mask])\n",
    "\n",
    "    # ë””ì½”ë”\n",
    "    dec_outputs = decoder(\n",
    "      vocab_size=vocab_size,\n",
    "      num_layers=num_layers,\n",
    "      units=units,\n",
    "      d_model=d_model,\n",
    "      num_heads=num_heads,\n",
    "      dropout=dropout,\n",
    "    )(inputs=[dec_inputs, enc_outputs, look_ahead_mask, dec_padding_mask])\n",
    "\n",
    "    # ì™„ì „ì—°ê²°ì¸µ\n",
    "    outputs = tf.keras.layers.Dense(units=vocab_size, name=\"outputs\")(dec_outputs)\n",
    "\n",
    "    return tf.keras.Model(inputs=[inputs, dec_inputs], outputs=outputs, name=name)\n",
    "print(\"ìŠ=3\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "e73832af",
   "metadata": {},
   "source": [
    "### 1. ëª¨ë¸ ìƒì„±\n",
    "num_layers, d-Model, unitsëŠ” ì „ë¶€ ì‚¬ìš©ìê°€ ì •í•  ìˆ˜ ìˆëŠ” í•˜ì´í¼íŒŒë¼ë¯¸í„° ê°’ì…ë‹ˆë‹¤.\n",
    "\n",
    "ë…¼ë¬¸ì—ì„œ num_layersëŠ” 6, d-Modelì€ 512ì˜€ì§€ë§Œ, ë¹ ë¥´ê³  ì›í™œí•œ í›ˆë ¨ì„ ìœ„í•´ ì—¬ê¸°ì„œëŠ” ê° í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ë…¼ë¬¸ì—ì„œë³´ë‹¤ëŠ” ì‘ì€ ê°’ì„ ì‚¬ìš©í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "73fae78f",
   "metadata": {},
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# í•˜ì´í¼íŒŒë¼ë¯¸í„°\n",
    "NUM_LAYERS = 2 # ì¸ì½”ë”ì™€ ë””ì½”ë”ì˜ ì¸µì˜ ê°œìˆ˜\n",
    "D_MODEL = 256 # ì¸ì½”ë”ì™€ ë””ì½”ë” ë‚´ë¶€ì˜ ì…, ì¶œë ¥ì˜ ê³ ì • ì°¨ì›\n",
    "NUM_HEADS = 8 # ë©€í‹° í—¤ë“œ ì–´í…ì…˜ì—ì„œì˜ í—¤ë“œ ìˆ˜ \n",
    "UNITS = 512 # í”¼ë“œ í¬ì›Œë“œ ì‹ ê²½ë§ì˜ ì€ë‹‰ì¸µì˜ í¬ê¸°\n",
    "DROPOUT = 0.1 # ë“œë¡­ì•„ì›ƒì˜ ë¹„ìœ¨\n",
    "\n",
    "model = transformer(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    units=UNITS,\n",
    "    d_model=D_MODEL,\n",
    "    num_heads=NUM_HEADS,\n",
    "    dropout=DROPOUT)\n",
    "\n",
    "model.summary()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "e1740a23",
   "metadata": {},
   "source": [
    "### 2. ì†ì‹¤ í•¨ìˆ˜(Loss function)\n",
    "ë ˆì´ë¸”ì¸ ì‹œí€€ìŠ¤ì— íŒ¨ë”©ì´ ë˜ì–´ ìˆìœ¼ë¯€ë¡œ, lossë¥¼ ê³„ì‚°í•  ë•Œ íŒ¨ë”© ë§ˆìŠ¤í¬ë¥¼ ì ìš©í•´ì•¼ í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "081bef39",
   "metadata": {},
   "source": [
    "def loss_function(y_true, y_pred):\n",
    "    y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n",
    "\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "      from_logits=True, reduction='none')(y_true, y_pred)\n",
    "\n",
    "    mask = tf.cast(tf.not_equal(y_true, 0), tf.float32)\n",
    "    loss = tf.multiply(loss, mask)\n",
    "\n",
    "    return tf.reduce_mean(loss)\n",
    "print(\"ìŠ=3\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "fc48b7dd",
   "metadata": {},
   "source": [
    "### 3. ì»¤ìŠ¤í…€ ëœ í•™ìŠµë¥ (Learning rate)\n",
    "ë”¥ëŸ¬ë‹ ëª¨ë¸í•™ìŠµ ì‹œ learning rateëŠ” ë§¤ìš° ì¤‘ìš”í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„°ì…ë‹ˆë‹¤. ìµœê·¼ì—ëŠ” ëª¨ë¸í•™ìŠµ ì´ˆê¸°ì— learning rateë¥¼ ê¸‰ê²©íˆ ë†’ì˜€ë‹¤ê°€, ì´í›„ train stepì´ ì§„í–‰ë¨ì— ë”°ë¼ ì„œì„œíˆ ë‚®ì¶”ì–´ ê°€ë©´ì„œ ì•ˆì •ì ìœ¼ë¡œ ìˆ˜ë ´í•˜ê²Œ í•˜ëŠ” ê³ ê¸‰ ê¸°ë²•ì„ ë„ë¦¬ ì‚¬ìš©í•˜ê³  ìˆìŠµë‹ˆë‹¤. ì´ëŸ° ë°©ë²•ì„ ì»¤ìŠ¤í…€ í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ë§(Custom Learning rate Scheduling)ì´ë¼ê³  í•©ë‹ˆë‹¤.\n",
    "\n",
    "ë…¼ë¬¸ì— ë‚˜ì˜¨ ê³µì‹ì„ ì°¸ê³ í•˜ì—¬ ì»¤ìŠ¤í…€ í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬ë¥¼ í†µí•œ ì•„ë‹´ ì˜µí‹°ë§ˆì´ì €ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. ë…¼ë¬¸ì— ë‚˜ì˜¨ ê³µì‹ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\n",
    "\n",
    "$$ lrate = d^{-0.5}_{model} \\cdot min(step\\_num^{-0.5}, step\\_num \\cdot warmup\\_steps^{-1.5}) $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5bef6468",
   "metadata": {},
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(CustomSchedule, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps**-1.5)\n",
    "\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
    "    \n",
    "print(\"ìŠ=3\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c54e88d0",
   "metadata": {},
   "source": [
    "sample_learning_rate = CustomSchedule(d_model=128)\n",
    "\n",
    "plt.plot(sample_learning_rate(tf.range(200000, dtype=tf.float32)))\n",
    "plt.ylabel(\"Learning Rate\")\n",
    "plt.xlabel(\"Train Step\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "1924918c",
   "metadata": {},
   "source": [
    "### 4. ëª¨ë¸ ì»´íŒŒì¼\n",
    "ì†ì‹¤ í•¨ìˆ˜ì™€ ì»¤ìŠ¤í…€ ëœ í•™ìŠµë¥ (learning rate)ì„ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ ì»´íŒŒì¼í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d04b3df9",
   "metadata": {},
   "source": [
    "learning_rate = CustomSchedule(D_MODEL)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(\n",
    "    learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "    y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n",
    "    return tf.keras.metrics.sparse_categorical_accuracy(y_true, y_pred)\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss_function, metrics=[accuracy])\n",
    "print(\"ìŠ=3\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "d8a8a3e3",
   "metadata": {},
   "source": [
    "### 5. í›ˆë ¨í•˜ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ed78ec1d",
   "metadata": {},
   "source": [
    "EPOCHS = 10\n",
    "model.fit(dataset, epochs=EPOCHS, verbose=1)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "c89121fe",
   "metadata": {},
   "source": [
    "## **ì±—ë´‡ í…ŒìŠ¤íŠ¸í•˜ê¸°**\n",
    "\n",
    "ì˜ˆì¸¡(inference)Â ë‹¨ê³„ëŠ”Â ê¸°ë³¸ì ìœ¼ë¡œÂ ë‹¤ìŒê³¼Â ê°™ì€Â ê³¼ì •ì„Â ê±°ì¹©ë‹ˆë‹¤.\n",
    "1. ìƒˆë¡œìš´Â ì…ë ¥Â ë¬¸ì¥ì—Â ëŒ€í•´ì„œëŠ”Â í›ˆë ¨ ë•Œì™€Â ë™ì¼í•œÂ ì „ì²˜ë¦¬ë¥¼Â ê±°ì¹œë‹¤.\n",
    "2. ì…ë ¥Â ë¬¸ì¥ì„Â í† í¬ë‚˜ì´ì§•í•˜ê³ ,Â `START_TOKEN`ê³¼Â `END_TOKEN`ì„Â ì¶”ê°€í•œë‹¤.\n",
    "3. íŒ¨ë”©Â ë§ˆìŠ¤í‚¹ê³¼Â ë£©Â ì–´í—¤ë“œÂ ë§ˆìŠ¤í‚¹ì„Â ê³„ì‚°í•œë‹¤.\n",
    "4. ë””ì½”ë”ëŠ”Â ì…ë ¥Â ì‹œí€€ìŠ¤ë¡œë¶€í„°Â ë‹¤ìŒÂ ë‹¨ì–´ë¥¼Â ì˜ˆì¸¡í•œë‹¤.\n",
    "5. ë””ì½”ë”ëŠ”Â ì˜ˆì¸¡ëœÂ ë‹¤ìŒÂ ë‹¨ì–´ë¥¼Â ê¸°ì¡´ì˜Â ì…ë ¥Â ì‹œí€€ìŠ¤ì—Â ì¶”ê°€í•˜ì—¬Â ìƒˆë¡œìš´Â ì…ë ¥ìœ¼ë¡œÂ ì‚¬ìš©í•œë‹¤.\n",
    "6. `END_TOKEN`ì´Â ì˜ˆì¸¡ë˜ê±°ë‚˜Â ë¬¸ì¥ì˜Â ìµœëŒ€Â ê¸¸ì´ì—Â ë„ë‹¬í•˜ë©´Â ë””ì½”ë”ëŠ”Â ë™ì‘ì„Â ë©ˆì¶˜ë‹¤.\n",
    "ìœ„ì˜ ê³¼ì •ì„ ëª¨ë‘ ë‹´ì€Â `decoder_inference()`Â í•¨ìˆ˜ë¥¼ ë§Œë“­ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "9d019478",
   "metadata": {},
   "source": [
    "def decoder_inference(sentence):\n",
    "    sentence = preprocess_sentence(sentence)\n",
    "\n",
    "    # ì…ë ¥ëœ ë¬¸ì¥ì„ ì •ìˆ˜ ì¸ì½”ë”© í›„, ì‹œì‘ í† í°ê³¼ ì¢…ë£Œ í† í°ì„ ì•ë’¤ë¡œ ì¶”ê°€.\n",
    "    # ex) Where have you been? â†’ [[8331   86   30    5 1059    7 8332]]\n",
    "    sentence = tf.expand_dims(\n",
    "      START_TOKEN + tokenizer.encode(sentence) + END_TOKEN, axis=0)\n",
    "\n",
    "    # ë””ì½”ë”ì˜ í˜„ì¬ê¹Œì§€ì˜ ì˜ˆì¸¡í•œ ì¶œë ¥ ì‹œí€€ìŠ¤ê°€ ì§€ì†ì ìœ¼ë¡œ ì €ì¥ë˜ëŠ” ë³€ìˆ˜.\n",
    "    # ì²˜ìŒì—ëŠ” ì˜ˆì¸¡í•œ ë‚´ìš©ì´ ì—†ìŒìœ¼ë¡œ ì‹œì‘ í† í°ë§Œ ë³„ë„ ì €ì¥. ex) 8331\n",
    "    output_sequence = tf.expand_dims(START_TOKEN, 0)\n",
    "\n",
    "    # ë””ì½”ë”ì˜ ì¸í¼ëŸ°ìŠ¤ ë‹¨ê³„\n",
    "    for i in range(MAX_LENGTH):\n",
    "        # ë””ì½”ë”ëŠ” ìµœëŒ€ MAX_LENGTHì˜ ê¸¸ì´ë§Œí¼ ë‹¤ìŒ ë‹¨ì–´ ì˜ˆì¸¡ì„ ë°˜ë³µí•©ë‹ˆë‹¤.\n",
    "        predictions = model(inputs=[sentence, output_sequence], training=False)\n",
    "        predictions = predictions[:, -1:, :]\n",
    "\n",
    "        # í˜„ì¬ ì˜ˆì¸¡í•œ ë‹¨ì–´ì˜ ì •ìˆ˜\n",
    "        predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
    "\n",
    "        # ë§Œì•½ í˜„ì¬ ì˜ˆì¸¡í•œ ë‹¨ì–´ê°€ ì¢…ë£Œ í† í°ì´ë¼ë©´ forë¬¸ì„ ì¢…ë£Œ\n",
    "        if tf.equal(predicted_id, END_TOKEN[0]):\n",
    "            break\n",
    "\n",
    "        # ì˜ˆì¸¡í•œ ë‹¨ì–´ë“¤ì€ ì§€ì†ì ìœ¼ë¡œ output_sequenceì— ì¶”ê°€ë©ë‹ˆë‹¤.\n",
    "        # ì´ output_sequenceëŠ” ë‹¤ì‹œ ë””ì½”ë”ì˜ ì…ë ¥ì´ ë©ë‹ˆë‹¤.\n",
    "        output_sequence = tf.concat([output_sequence, predicted_id], axis=-1)\n",
    "\n",
    "    return tf.squeeze(output_sequence, axis=0)\n",
    "    print(\"ìŠ=3\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d57d4915",
   "metadata": {},
   "source": [
    "def sentence_generation(sentence):\n",
    "    # ì…ë ¥ ë¬¸ì¥ì— ëŒ€í•´ì„œ ë””ì½”ë”ë¥¼ ë™ì‘ ì‹œì¼œ ì˜ˆì¸¡ëœ ì •ìˆ˜ ì‹œí€€ìŠ¤ë¥¼ ë¦¬í„´ë°›ìŠµë‹ˆë‹¤.\n",
    "    prediction = decoder_inference(sentence)\n",
    "\n",
    "    # ì •ìˆ˜ ì‹œí€€ìŠ¤ë¥¼ ë‹¤ì‹œ í…ìŠ¤íŠ¸ ì‹œí€€ìŠ¤ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.\n",
    "    predicted_sentence = tokenizer.decode(\n",
    "      [i for i in prediction if i < tokenizer.vocab_size])\n",
    "\n",
    "    print('ì…ë ¥ : {}'.format(sentence))\n",
    "    print('ì¶œë ¥ : {}'.format(predicted_sentence))\n",
    "\n",
    "    return predicted_sentence\n",
    "print(\"ìŠ=3\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ceab162b",
   "metadata": {},
   "source": [
    "sentence_generation('Where have you been?')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "7bb5a3a1",
   "metadata": {},
   "source": [
    "sentence_generation(\"It's a trap\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f36c3d5",
   "metadata": {},
   "source": [],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
