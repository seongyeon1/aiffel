{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21e20650",
   "metadata": {},
   "source": [
    "# 프로젝트: GPT-1 논문 기반의 한국어 데이터 챗봇 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "825c6de5",
   "metadata": {},
   "source": [
    "## 변경사항\n",
    "\n",
    "### 전처리 단계\n",
    "  \n",
    "    - Unsupervised Pre-training\n",
    "        - 모델이 다음 토큰을 예측하도록 하나의 문장 데이터를 학습하는 구조\n",
    "        - 질문, 답변 문장을 연결하지 않고 학습을 위해 row 방향으로 붙여주었다\n",
    "        - 답변에서 중복 문장들이 많았기에 총 19436개의 데이터를 학습하였다\n",
    "        - train data의 토큰 길이를 바탕으로 Pre-training에서의 max length는 40으로 설정해줬다\n",
    "    - Supervised Fine-tuning\n",
    "        - 질문과 답변 데이터를 구분자 토큰과 함께 하나의 시퀀스로 결합\n",
    "        - delimiter token 을 추가\n",
    "        - Supervised Fine-tuning dataset은 기존 pre-training 방식과 데이터셋이 다르게 들어가야한다\n",
    "        - [[start] 질문 [delimiter] 답 [end]] 구조이므로 길이도 좀 더 길게 설정해줘야한다\n",
    "        - train data의 토큰 길이를 바탕으로 finetuning에서의 max length는 70으로 설정해줬다\n",
    "\n",
    "### 모델 구현 단계\n",
    "    - GPT는 디코더만으로 구성된 트랜스포머 모델이다\n",
    "    - Encoder-Decoder Attention 제거 :\n",
    "        - GPT는 디코더만으로 구성되기 때문에 encoder와 비교하는 encoder-decoder Attention Layer가 필요가 없다.\n",
    "        - 기존 코드에서 두 번째 서브 레이어를 제거한다\n",
    "        ---------------------------------------------------------------------------------------\n",
    "                attention2 = MultiHeadAttention(\n",
    "                    d_model, num_heads, name=\"attention_2\")(inputs={\n",
    "                          'query': attention1,\n",
    "                          'key': enc_outputs,\n",
    "                          'value': enc_outputs,\n",
    "                          'mask': padding_mask\n",
    "                      })\n",
    "\n",
    "                # 마스크드 멀티 헤드 어텐션의 결과는\n",
    "                # Dropout과 LayerNormalization이라는 훈련을 돕는 테크닉을 수행\n",
    "                attention2 = tf.keras.layers.Dropout(rate=dropout)(attention2)\n",
    "                attention2 = tf.keras.layers.LayerNormalization(\n",
    "                  epsilon=1e-6)(attention2 + attention1)\n",
    "       -----------------------------------------------------------------------------------------\n",
    "    - 인코더 출력 제거 : encoder_outputs 제거\n",
    "    - 마스킹 방식 : padding mask 제거, look_ahead_mask 만 사용하면됨\n",
    "    \n",
    "### 예측 단계\n",
    "    - input으로 look_ahead_mask 추가해주기\n",
    "    - 마지막에 end_token이 아니라 delimiter_token을 붙여준다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36423546",
   "metadata": {},
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03d182ad",
   "metadata": {},
   "source": [
    "chatbot = pd.read_csv('./data/ChatbotData .csv')\n",
    "chatbot"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "f880b36e",
   "metadata": {},
   "source": [
    "# 데이터 전처리하기\n",
    "- 공백, 특수문자 처리\n",
    "- 챗봇 특성상 숫자의 의미가 별로 필요없는 것으로 보이므로 숫자도 제거해준다\n",
    "    - 옵션을 통해 숫자가 필요하지 않으면 없애는 걸로 코드를 수정했다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41aa7b51",
   "metadata": {},
   "source": [
    "# 전처리 함수\n",
    "def preprocess_sentence(sentence, num=True):\n",
    "    # 입력받은 sentence를 소문자로 변경하고 양쪽 공백을 제거\n",
    "    sentence = sentence.strip()\n",
    "\n",
    "    # 단어와 구두점(punctuation) 사이의 거리를 만듭니다.\n",
    "    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "\n",
    "    # (가-힣, a-z, A-Z, \".\", \"?\", \"!\", \",\")를 제외한 모든 문자를 공백인 ' '로 대체합니다.\n",
    "    if num == False: # 숫자제거\n",
    "        sentence = re.sub(r'[^가-힣a-zA-Z.?!,]', ' ', sentence)\n",
    "    else:\n",
    "        sentence = re.sub(r'[^가-힣0-9a-zA-Z.?!,]', ' ', sentence)\n",
    "    sentence = sentence.strip()\n",
    "    return sentence"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12bde0f3",
   "metadata": {},
   "source": [
    "sentence = '3박4일 놀러가고 싶다'"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ccd21e3d",
   "metadata": {},
   "source": [
    "preprocess_sentence(sentence)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "b6c7ad5b",
   "metadata": {},
   "source": [
    "## SubwordTextEncoder 사용하기\n",
    "\n",
    "- start token, end token, delimiter 토큰을 추가합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a98cef4",
   "metadata": {},
   "source": [
    "questions = chatbot.Q.tolist()\n",
    "answers = chatbot.A.tolist()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01fab535",
   "metadata": {},
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "# 질문과 답변 데이터셋에 대해서 Vocabulary 생성\n",
    "tokenizer = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(questions + answers, target_vocab_size=2**13)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb6b2fb8",
   "metadata": {},
   "source": [
    "# 시작 토큰과 종료 토큰에 고유한 정수를 부여합니다.\n",
    "START_TOKEN, END_TOKEN, DELIMITER_TOKEN = [tokenizer.vocab_size], [tokenizer.vocab_size + 1], [tokenizer.vocab_size + 2]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "acfbd1ad",
   "metadata": {},
   "source": [
    "print('START_TOKEN의 번호 :' ,[tokenizer.vocab_size])\n",
    "print('END_TOKEN의 번호 :' ,[tokenizer.vocab_size + 1])\n",
    "print('DELIMITER_TOKEN의 번호 :' ,[tokenizer.vocab_size + 2])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6d52e59c",
   "metadata": {},
   "source": [
    "# 시작 토큰과 종료 토큰을 고려하여 +2를 하여 단어장의 크기를 산정합니다.\n",
    "VOCAB_SIZE = tokenizer.vocab_size + 3\n",
    "print(VOCAB_SIZE)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0f167b2d",
   "metadata": {},
   "source": [
    "# 임의의 22번째 샘플에 대해서 정수 인코딩 작업을 수행.\n",
    "# 각 토큰을 고유한 정수로 변환\n",
    "print('정수 인코딩 후의 21번째 질문 샘플: {}'.format(tokenizer.encode(questions[21])))\n",
    "print('정수 인코딩 후의 21번째 답변 샘플: {}'.format(tokenizer.encode(answers[21])))"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "2e2975e7",
   "metadata": {},
   "source": [
    "### 적절한 패딩 길이 구하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b32e4123",
   "metadata": {},
   "source": [
    "import matplotlib.pyplot as plt"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "21fbddcd",
   "metadata": {},
   "source": [
    "num_questions = [len(tokens) for tokens in questions]\n",
    "num_questions = np.array(num_questions)\n",
    "\n",
    "\n",
    "# 평균값, 최댓값, 표준편차\n",
    "print(f\"토큰 길이 평균: {np.mean(num_questions)}\")\n",
    "print(f\"토큰 길이 최대: {np.max(num_questions)}\")\n",
    "print(f\"토큰 길이 표준편차: {np.std(num_questions)}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "073e1faa",
   "metadata": {},
   "source": [
    "num_answers = [len(tokens) for tokens in answers]\n",
    "num_answers = np.array(num_answers)\n",
    "\n",
    "\n",
    "# 평균값, 최댓값, 표준편차\n",
    "print(f\"토큰 길이 평균: {np.mean(num_answers)}\")\n",
    "print(f\"토큰 길이 최대: {np.max(num_answers)}\")\n",
    "print(f\"토큰 길이 표준편차: {np.std(num_answers)}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "df3697d3",
   "metadata": {},
   "source": [
    "plt.title('all text length')\n",
    "plt.hist(num_answers, bins=100)\n",
    "plt.hist(num_questions, bins=100)\n",
    "plt.xlabel('length of samples')\n",
    "plt.ylabel('number of samples')\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "12fb898d",
   "metadata": {},
   "source": [
    "select_length = 40\n",
    "\n",
    "def below_threshold_len(max_len, nested_list):\n",
    "    cnt = 0\n",
    "    for s in nested_list:\n",
    "        if(len(s) <= max_len):\n",
    "            cnt = cnt + 1\n",
    "        \n",
    "    print('전체 샘플 중 길이가 %s 이하인 샘플의 비율: %s'%(max_len, (cnt / len(nested_list))))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8cacb0d5",
   "metadata": {},
   "source": [
    "below_threshold_len(select_length, answers)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eed0c149",
   "metadata": {},
   "source": [
    "below_threshold_len(select_length, questions)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cbc4961a",
   "metadata": {},
   "source": [
    "# 샘플의 최대 허용 길이 또는 패딩 후의 최종 길이\n",
    "MAX_LENGTH = 40\n",
    "print(MAX_LENGTH)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "0254bf2a",
   "metadata": {},
   "source": [
    "# Unsupervised Pre-training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830fe65b",
   "metadata": {},
   "source": [
    "## Unsupervised Pre-training 데이터셋 준비\n",
    "\n",
    "- train data와 validation data로 나누어서 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f069cc0e",
   "metadata": {},
   "source": [
    "sentences = pd.concat([chatbot.A, chatbot.Q]).drop_duplicates().tolist()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "c08f539f",
   "metadata": {},
   "source": [
    "### 정수 인코딩, 최대 길이를 초과하는 샘플 제거, 패딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1da1e419",
   "metadata": {},
   "source": [
    "def unsupervised_tokenize_and_filter(inputs):\n",
    "    tokenized_inputs, tokenized_outputs = [], []\n",
    "\n",
    "    for sentence in inputs:\n",
    "        # 정수 인코딩 과정에서 시작 토큰과 종료 토큰을 추가\n",
    "        sentence = START_TOKEN + tokenizer.encode(sentence) + END_TOKEN\n",
    "\n",
    "        # 최대 길이 40 이하인 경우에만 데이터셋으로 허용\n",
    "        if len(sentence) <= MAX_LENGTH:\n",
    "            tokenized_inputs.append(sentence)\n",
    "\n",
    "    # 최대 길이 40으로 모든 데이터셋을 패딩\n",
    "    tokenized_inputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "      tokenized_inputs, maxlen=MAX_LENGTH, padding='post')\n",
    "\n",
    "    return tokenized_inputs"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a298a25b",
   "metadata": {},
   "source": [
    "sentences = unsupervised_tokenize_and_filter(sentences)\n",
    "print('단어장의 크기 :',(VOCAB_SIZE))\n",
    "print('필터링 후의 문장 샘플 개수: {}'.format(len(sentences)))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1c248b93",
   "metadata": {},
   "source": [
    "def create_padding_mask(x):\n",
    "    mask = tf.cast(tf.math.equal(x, 0), tf.float32)\n",
    "    # (batch_size, 1, 1, sequence length)\n",
    "    return mask[:, tf.newaxis, tf.newaxis, :]\n",
    "\n",
    "def create_look_ahead_mask(x):\n",
    "    seq_len = tf.shape(x)[1]\n",
    "    look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
    "    padding_mask = create_padding_mask(x)\n",
    "    return tf.maximum(look_ahead_mask, padding_mask)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "69296616",
   "metadata": {},
   "source": [
    "# look-ahead 마스크 생성\n",
    "look_ahead_mask = create_look_ahead_mask(sentences[:, :-1])\n",
    "\n",
    "# BATCH_SIZE와 BUFFER_SIZE 정의\n",
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 20000\n",
    "\n",
    "# 질문과 답변 데이터를 tf.data.Dataset으로 만듭니다.\n",
    "dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    {\n",
    "        'inputs': sentences[:, :-1],\n",
    "        'look_ahead_mask': look_ahead_mask,\n",
    "    },\n",
    "    {\n",
    "        'outputs': sentences[:, 1:]\n",
    "    },\n",
    "))\n",
    "\n",
    "# 전체 데이터셋 크기 계산\n",
    "dataset_size = len(sentences)\n",
    "\n",
    "# 학습 데이터셋과 검증 데이터셋 크기 계산\n",
    "train_size = int(0.8 * dataset_size)\n",
    "val_size = dataset_size - train_size\n",
    "\n",
    "# 데이터셋 캐싱, 셔플링, 배칭 및 prefetch 적용\n",
    "dataset = dataset.cache()\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "\n",
    "# 학습 데이터셋과 검증 데이터셋으로 분할\n",
    "train_dataset = dataset.take(train_size).batch(BATCH_SIZE).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "val_dataset = dataset.skip(train_size).batch(BATCH_SIZE).prefetch(tf.data.experimental.AUTOTUNE)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dc3eac3a",
   "metadata": {},
   "source": [
    "sentences.shape"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "4ea6961d",
   "metadata": {},
   "source": [
    "## Unsupervised Pre-training 모델 구성하기\n",
    "\n",
    "추가적으로 모델 저장을 위해 `get_config()`와 `__init__`부분을 수정하였다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b04c8f86",
   "metadata": {},
   "source": [
    "class PositionalEncoding(tf.keras.layers.Layer):\n",
    "    def __init__(self, position, d_model, **kwargs):\n",
    "        super(PositionalEncoding, self).__init__(**kwargs)\n",
    "        self.position = position\n",
    "        self.d_model = d_model\n",
    "        self.pos_encoding = self.positional_encoding(position, d_model)\n",
    "    \n",
    "    def get_angles(self, position, i, d_model):\n",
    "        angles = 1 / tf.pow(10000, (2 * (i // 2)) / tf.cast(d_model, tf.float32))\n",
    "        return position * angles\n",
    "\n",
    "    def positional_encoding(self, position, d_model):\n",
    "        angle_rads = self.get_angles(\n",
    "            position=tf.range(position, dtype=tf.float32)[:, tf.newaxis],\n",
    "            i=tf.range(d_model, dtype=tf.float32)[tf.newaxis, :],\n",
    "            d_model=d_model)\n",
    "\n",
    "        sines = tf.math.sin(angle_rads[:, 0::2])\n",
    "        cosines = tf.math.cos(angle_rads[:, 1::2])\n",
    "\n",
    "        pos_encoding = tf.stack([sines, cosines], axis=0)\n",
    "        pos_encoding = tf.transpose(pos_encoding, [1, 2, 0])\n",
    "        pos_encoding = tf.reshape(pos_encoding, [position, d_model])\n",
    "\n",
    "        pos_encoding = pos_encoding[tf.newaxis, ...]\n",
    "        return tf.cast(pos_encoding, tf.float32)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return inputs + self.pos_encoding[:, :tf.shape(inputs)[1], :]\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(PositionalEncoding, self).get_config()\n",
    "        config.update({\n",
    "            'position': self.position,\n",
    "            'd_model': self.d_model\n",
    "        })\n",
    "        return config"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "82d4cc89",
   "metadata": {},
   "source": [
    "# 스케일드 닷 프로덕트 어텐션 함수\n",
    "def scaled_dot_product_attention(query, key, value, mask):\n",
    "    # 어텐션 가중치는 Q와 K의 닷 프로덕트\n",
    "    matmul_qk = tf.matmul(query, key, transpose_b=True)\n",
    "\n",
    "    # 가중치를 정규화\n",
    "    depth = tf.cast(tf.shape(key)[-1], tf.float32)\n",
    "    logits = matmul_qk / tf.math.sqrt(depth)\n",
    "\n",
    "    # 패딩에 마스크 추가\n",
    "    if mask is not None:\n",
    "        logits += (mask * -1e9)\n",
    "\n",
    "    # softmax적용\n",
    "    attention_weights = tf.nn.softmax(logits, axis=-1)\n",
    "\n",
    "    # 최종 어텐션은 가중치와 V의 닷 프로덕트\n",
    "    output = tf.matmul(attention_weights, value)\n",
    "    return output"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a029e53d",
   "metadata": {},
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, d_model, num_heads, **kwargs):\n",
    "        super(MultiHeadAttention, self).__init__(**kwargs)\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        assert d_model % self.num_heads == 0\n",
    "\n",
    "        self.depth = d_model // self.num_heads\n",
    "\n",
    "        self.query_dense = tf.keras.layers.Dense(units=d_model)\n",
    "        self.key_dense = tf.keras.layers.Dense(units=d_model)\n",
    "        self.value_dense = tf.keras.layers.Dense(units=d_model)\n",
    "\n",
    "        self.dense = tf.keras.layers.Dense(units=d_model)\n",
    "\n",
    "    def split_heads(self, inputs, batch_size):\n",
    "        inputs = tf.reshape(\n",
    "            inputs, shape=(batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(inputs, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, inputs):\n",
    "        query, key, value, mask = inputs['query'], inputs['key'], inputs['value'], inputs['mask']\n",
    "        batch_size = tf.shape(query)[0]\n",
    "\n",
    "        # Q, K, V에 각각 Dense를 적용합니다\n",
    "        query = self.query_dense(query)\n",
    "        key = self.key_dense(key)\n",
    "        value = self.value_dense(value)\n",
    "\n",
    "        # 병렬 연산을 위한 머리를 여러 개 만듭니다\n",
    "        query = self.split_heads(query, batch_size)\n",
    "        key = self.split_heads(key, batch_size)\n",
    "        value = self.split_heads(value, batch_size)\n",
    "\n",
    "        # 스케일드 닷 프로덕트 어텐션 함수\n",
    "        scaled_attention = scaled_dot_product_attention(query, key, value, mask)\n",
    "\n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
    "\n",
    "        # 어텐션 연산 후에 각 결과를 다시 연결(concatenate)합니다\n",
    "        concat_attention = tf.reshape(scaled_attention,\n",
    "                                      (batch_size, -1, self.d_model))\n",
    "\n",
    "        # 최종 결과에도 Dense를 한 번 더 적용합니다\n",
    "        outputs = self.dense(concat_attention)\n",
    "\n",
    "        return outputs\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super(MultiHeadAttention, self).get_config()\n",
    "        config.update({\n",
    "            'd_model': self.d_model,\n",
    "            'num_heads': self.num_heads,\n",
    "        })\n",
    "        return config"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "57cbc275",
   "metadata": {},
   "source": [
    "def gpt_decoder_layer(units, d_model, num_heads, dropout, name=\"gpt_decoder_layer\"):\n",
    "    \n",
    "    inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n",
    "    look_ahead_mask = tf.keras.Input(shape=(1, None, None), name=\"look_ahead_mask\")\n",
    "    \n",
    "    \n",
    "    # 첫 번째 서브 레이어 : 멀티 헤드 어텐션 수행 (셀프 어텐션)\n",
    "    attention = MultiHeadAttention(\n",
    "        d_model, num_heads, name=\"attention_1\")(inputs={\n",
    "          'query': inputs,\n",
    "          'key': inputs,\n",
    "          'value': inputs,\n",
    "          'mask': look_ahead_mask\n",
    "      })\n",
    "\n",
    "    # 멀티 헤드 어텐션의 결과는 LayerNormalization이라는 훈련을 돕는 테크닉을 수행\n",
    "    attention = tf.keras.layers.LayerNormalization(epsilon=1e-6)(attention + inputs)\n",
    "\n",
    "    # 두 번째 서브 레이어를 제거하고 바로 dense layer로 연결된다\n",
    "    # 두 번째 서브 레이어 : 2개의 완전연결층 \n",
    "    outputs = tf.keras.layers.Dense(units=units, activation='relu')(attention)\n",
    "    outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n",
    "\n",
    "    # 완전연결층의 결과는 Dropout과 LayerNormalization 수행\n",
    "    outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
    "    outputs = tf.keras.layers.LayerNormalization(epsilon=1e-6)(outputs + attention)\n",
    "\n",
    "    return tf.keras.Model(inputs=[inputs, look_ahead_mask], outputs=outputs, name=name)\n",
    "\n",
    "def gpt_decoder(vocab_size, num_layers, units, d_model, num_heads, dropout, name='gpt_decoder'):\n",
    "    \n",
    "    # 기존 decoder에 있던 padding mask 제거 (padding mask는 encoder에서 필요했던 것이기 때문)\n",
    "    inputs = tf.keras.Input(shape=(None,), name='inputs')\n",
    "    look_ahead_mask = tf.keras.Input(shape=(1, None, None), name='look_ahead_mask')\n",
    "\n",
    "    # 임베딩 레이어\n",
    "    embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n",
    "    embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
    "\n",
    "    # 포지셔널 인코딩\n",
    "    embeddings = PositionalEncoding(vocab_size, d_model)(embeddings)\n",
    "\n",
    "    # Dropout이라는 훈련을 돕는 테크닉을 수행\n",
    "    outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n",
    "\n",
    "    for i in range(num_layers):\n",
    "        outputs = gpt_decoder_layer(units=units, d_model=d_model, num_heads=num_heads, dropout=dropout, \n",
    "                                name='gpt_decoder_layer_{}'.format(i))(inputs=[outputs, look_ahead_mask])\n",
    "    \n",
    "    logits = tf.keras.layers.Dense(vocab_size, name='outputs')(outputs)\n",
    "\n",
    "    return tf.keras.Model(inputs=[inputs, look_ahead_mask], outputs=logits, name=name)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "03a7ffc9",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# 하이퍼파라미터\n",
    "NUM_LAYERS = 2 # 인코더와 디코더의 층의 개수\n",
    "D_MODEL = 256 # 인코더와 디코더 내부의 입, 출력의 고정 차원\n",
    "NUM_HEADS = 8 # 멀티 헤드 어텐션에서의 헤드 수 \n",
    "UNITS = 512 # 피드 포워드 신경망의 은닉층의 크기\n",
    "DROPOUT = 0.1 # 드롭아웃의 비율\n",
    "\n",
    "model = gpt_decoder(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    units=UNITS,\n",
    "    d_model=D_MODEL,\n",
    "    num_heads=NUM_HEADS,\n",
    "    dropout=DROPOUT)\n",
    "\n",
    "model.summary()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "14e3e43d",
   "metadata": {},
   "source": [
    "# 손실 함수\n",
    "def loss_function(y_true, y_pred):\n",
    "    y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n",
    "\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "      from_logits=True, reduction='none')(y_true, y_pred)\n",
    "\n",
    "    mask = tf.cast(tf.not_equal(y_true, 0), tf.float32)\n",
    "    loss = tf.multiply(loss, mask)\n",
    "\n",
    "    return tf.reduce_mean(loss)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "013c8819",
   "metadata": {},
   "source": [
    "### 커스텀 된 학습률(Learning rate)\n",
    "\n",
    "$$ lrate = d^{-0.5}_{model} \\cdot min(step\\_num^{-0.5}, step\\_num \\cdot warmup\\_steps^{-1.5}) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6afbb689",
   "metadata": {},
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(CustomSchedule, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps**-1.5)\n",
    "\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
    "    \n",
    "    def get_config(self):\n",
    "        return {\n",
    "            'd_model': int(self.d_model.numpy()),\n",
    "            'warmup_steps': self.warmup_steps,\n",
    "        }"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "173ad9dd",
   "metadata": {},
   "source": [
    "# 모델 컴파일\n",
    "learning_rate = CustomSchedule(D_MODEL)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(\n",
    "    learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "    y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n",
    "    return tf.keras.metrics.sparse_categorical_accuracy(y_true, y_pred)\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss_function, metrics=[accuracy])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b9ec7211",
   "metadata": {},
   "source": [
    "# 체크포인트 콜백 정의\n",
    "checkpoint_path = \"./checkpoints/cp-{epoch:04d}.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "# 모델 체크포인트 콜백 생성\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_path,\n",
    "    save_weights_only=True,\n",
    "    save_best_only=True,\n",
    "    monitor='val_loss',\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# 모델 학습\n",
    "EPOCHS = 10\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=val_dataset,\n",
    "    callbacks=[checkpoint_callback],\n",
    "    verbose=1\n",
    ")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "b492f5fa",
   "metadata": {},
   "source": [
    "### Loss와 Accuracy 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "551939f4",
   "metadata": {},
   "source": [
    "history_dict = history.history\n",
    "print(history_dict.keys())"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "79f4bd70",
   "metadata": {},
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# history에서 손실과 정확도, 검증 손실과 검증 정확도를 추출합니다.\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "fig = plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs, acc, 'ro', label='Training accuracy')\n",
    "plt.plot(epochs, val_acc, 'r', label='Validation accuracy')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "d7631ffc",
   "metadata": {},
   "source": [
    "### 체크포인트 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6b458669",
   "metadata": {},
   "source": [
    "# 모델 로드 (구조를 재정의하고 가중치를 로드)\n",
    "def create_gpt_model():\n",
    "    return gpt_decoder(\n",
    "        vocab_size=VOCAB_SIZE,\n",
    "        num_layers=NUM_LAYERS,\n",
    "        units=UNITS,\n",
    "        d_model=D_MODEL,\n",
    "        num_heads=NUM_HEADS,\n",
    "        dropout=DROPOUT)\n",
    "\n",
    "# 모델 인스턴스 생성 및 컴파일\n",
    "ck_model = create_gpt_model()\n",
    "ck_model.compile(optimizer=optimizer, loss=loss_function, metrics=['accuracy'])\n",
    "\n",
    "# 체크포인트 콜백 정의\n",
    "checkpoint_path = \"./checkpoints/cp-{epoch:04d}.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "# 최신 체크포인트 파일 경로 (예: 마지막 에포크의 체크포인트)\n",
    "latest_checkpoint = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "\n",
    "# 저장된 가중치를 로드\n",
    "if latest_checkpoint:\n",
    "    ck_model.load_weights(latest_checkpoint)\n",
    "    print(f\"Checkpoint loaded: {latest_checkpoint}\")\n",
    "else:\n",
    "    print(\"No checkpoint found.\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "48da7e80",
   "metadata": {},
   "source": [
    "# Supervised fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbcad53d",
   "metadata": {},
   "source": [
    "## Supervised Fine-tuning 데이터셋 준비\n",
    "\n",
    "- train data와 validation data로 나누어서 학습\n",
    "- Supervised Fine-tuning dataset은 기존 pre-training 방식과 데이터셋이 다르게 들어가야한다\n",
    "- [[start] 질문 [delimiter] 답 [end]] 구조이므로 길이도 좀 더 길게 설정해줘야한다\n",
    "- train data의 토큰 길이를 바탕으로 finetuning에서의 max length는 70으로 설정해줬다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "ceb33004",
   "metadata": {},
   "source": [
    "data = (chatbot.Q + ' & ' + chatbot.A ).tolist()\n",
    "data[:5]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "ee9029cd",
   "metadata": {},
   "source": [
    "num_data = [len(tokens) for tokens in data]\n",
    "num_data = np.array(num_data)\n",
    "\n",
    "\n",
    "# 평균값, 최댓값, 표준편차\n",
    "print(f\"토큰 길이 평균: {np.mean(num_answers)}\")\n",
    "print(f\"토큰 길이 최대: {np.max(num_answers)}\")\n",
    "print(f\"토큰 길이 표준편차: {np.std(num_answers)}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "006c1519",
   "metadata": {},
   "source": [
    "plt.title('all text length')\n",
    "plt.hist(num_data, bins=100)\n",
    "plt.xlabel('length of samples')\n",
    "plt.ylabel('number of samples')\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "a9f5216d",
   "metadata": {},
   "source": [
    "select_length = 70\n",
    "\n",
    "def below_threshold_len(max_len, nested_list):\n",
    "    cnt = 0\n",
    "    for s in nested_list:\n",
    "        if(len(s) <= max_len):\n",
    "            cnt = cnt + 1\n",
    "        \n",
    "    print('전체 샘플 중 길이가 %s 이하인 샘플의 비율: %s'%(max_len, (cnt / len(nested_list))))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "b8d051c8",
   "metadata": {},
   "source": [
    "below_threshold_len(select_length, data)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "ba47913e",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# 샘플의 최대 허용 길이 또는 패딩 후의 최종 길이\n",
    "MAX_LENGTH = 70\n",
    "print(MAX_LENGTH)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "7221b930",
   "metadata": {},
   "source": [
    "questions = chatbot.Q.tolist()\n",
    "answers = chatbot.A.tolist()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "9b5ba9f2",
   "metadata": {},
   "source": [
    "DELIMITER_TOKEN"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "0515cf4a",
   "metadata": {},
   "source": [
    "def tokenize_and_filter(questions, answers):\n",
    "    tokenized_inputs = []\n",
    "\n",
    "    for (sentence1, sentence2) in zip(questions, answers):\n",
    "        # 정수 인코딩 과정에서 시작 토큰과 종료 토큰을 추가\n",
    "        sentence = START_TOKEN + tokenizer.encode(sentence1) + DELIMITER_TOKEN + tokenizer.encode(sentence2) + END_TOKEN\n",
    "\n",
    "        # 최대 길이 70 이하인 경우에만 데이터셋으로 허용\n",
    "        if len(sentence) <= MAX_LENGTH :\n",
    "            tokenized_inputs.append(sentence)\n",
    "\n",
    "    # 최대 길이 80으로 모든 데이터셋을 패딩\n",
    "    tokenized_inputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "      tokenized_inputs, maxlen=MAX_LENGTH, padding='post')\n",
    "    \n",
    "    return tokenized_inputs"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "fe18267d",
   "metadata": {},
   "source": [
    "concate_sentences = tokenize_and_filter(questions, answers)\n",
    "print('단어장의 크기 :',(VOCAB_SIZE))\n",
    "print('필터링 후의 문장 샘플 개수: {}'.format(len(concate_sentences)))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "194ce085",
   "metadata": {},
   "source": [
    "concate_sentences.shape"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "3bee0375",
   "metadata": {},
   "source": [
    "concate_sentences"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "06162e82",
   "metadata": {},
   "source": [
    "# look-ahead 마스크 생성\n",
    "look_ahead_mask = create_look_ahead_mask(concate_sentences[:, :-1])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "882b553f",
   "metadata": {},
   "source": [
    "concate_sentences.shape"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "332c72d1",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "look_ahead_mask.shape"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "06942052",
   "metadata": {},
   "source": [
    "# BATCH_SIZE와 BUFFER_SIZE 정의\n",
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 20000\n",
    "\n",
    "# 질문과 답변 데이터를 tf.data.Dataset으로 만듭니다.\n",
    "dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    {\n",
    "        'inputs': concate_sentences[:, :-1],\n",
    "        'look_ahead_mask': look_ahead_mask,\n",
    "    },\n",
    "    {\n",
    "        'outputs': concate_sentences[:, 1:]\n",
    "    },\n",
    "))\n",
    "\n",
    "# 전체 데이터셋 크기 계산\n",
    "dataset_size = len(concate_sentences)\n",
    "\n",
    "# 학습 데이터셋과 검증 데이터셋 크기 계산\n",
    "train_size = int(0.8 * dataset_size)\n",
    "val_size = dataset_size - train_size\n",
    "\n",
    "# 데이터셋 캐싱, 셔플링, 배칭 및 prefetch 적용\n",
    "dataset = dataset.cache()\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "\n",
    "# 학습 데이터셋과 검증 데이터셋으로 분할\n",
    "train_dataset = dataset.take(train_size).batch(BATCH_SIZE).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "val_dataset = dataset.skip(train_size).batch(BATCH_SIZE).prefetch(tf.data.experimental.AUTOTUNE)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "923d3894",
   "metadata": {},
   "source": [
    "## Supervised fine-tuning 모델 정의\n",
    "- 문장 단위로 학습했던 모델 기반으로 finetuning을 진행한다\n",
    "- pretrain 모델의 weight를 fine-tuning model로 불러왔다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "f76185fd",
   "metadata": {},
   "source": [
    "def gpt_supervised_finetuning(vocab_size, num_layers, units, d_model, num_heads, dropout):\n",
    "    inputs = tf.keras.Input(shape=(None,), name='inputs')\n",
    "    look_ahead_mask = tf.keras.Input(shape=(1, None, None), name='look_ahead_mask')\n",
    "\n",
    "    # 임베딩 레이어\n",
    "    embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n",
    "    embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
    "\n",
    "    # 포지셔널 인코딩\n",
    "    embeddings = PositionalEncoding(vocab_size, d_model)(embeddings)\n",
    "\n",
    "    # Dropout 수행\n",
    "    outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n",
    "\n",
    "    for i in range(num_layers):\n",
    "        outputs = gpt_decoder_layer(units=units, d_model=d_model, num_heads=num_heads, dropout=dropout, \n",
    "                                    name='gpt_decoder_layer_{}'.format(i))(inputs=[outputs, look_ahead_mask])\n",
    "    \n",
    "    outputs = tf.keras.layers.Dense(vocab_size, name='outputs')(outputs)\n",
    "    \n",
    "    return tf.keras.Model(inputs=[inputs, look_ahead_mask], outputs=outputs, name='gpt_supervised_finetuning')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "416ba478",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "max_length_finetune = 70\n",
    "\n",
    "# Supervised Fine-tuning 모델 생성\n",
    "finetune_model = gpt_supervised_finetuning(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    units=UNITS,\n",
    "    d_model=D_MODEL,\n",
    "    num_heads=NUM_HEADS,\n",
    "    dropout=DROPOUT)\n",
    "\n",
    "finetune_model.summary()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "29a2a3a8",
   "metadata": {},
   "source": [
    "# 모델 컴파일\n",
    "finetune_model.compile(optimizer=optimizer, loss=loss_function, metrics=[accuracy])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014b87b9",
   "metadata": {},
   "source": [
    "# fine-tuning model로 weight을 불러옴\n",
    "## 이부분은 생각보다 성능이 좋지 않아서 아래 코드는 빼고 학습을 진행했다\n",
    "for i in range(NUM_LAYERS):\n",
    "    finetune_model.get_layer(f'gpt_decoder_layer_{i}').set_weights(\n",
    "        model.get_layer(f'gpt_decoder_layer_{i}').get_weights()\n",
    "    )\n",
    "\n",
    "finetune_model.get_layer('embedding_2').set_weights(model.get_layer('embedding').get_weights())\n",
    "finetune_model.get_layer('positional_encoding_2').set_weights(model.get_layer('positional_encoding').get_weights())"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "86065c25",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# 체크포인트 콜백 정의\n",
    "checkpoint_path = \"./finetune_checkpoints/cp-{epoch:04d}.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "# 모델 체크포인트 콜백 생성\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_path,\n",
    "    save_weights_only=True,\n",
    "    save_best_only=True,\n",
    "    monitor='val_loss',\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# 모델 학습\n",
    "EPOCHS = 10\n",
    "history = finetune_model.fit(\n",
    "    train_dataset,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=val_dataset,\n",
    "    callbacks=[checkpoint_callback],\n",
    "    verbose=1\n",
    ")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "5a4ab405",
   "metadata": {},
   "source": [
    "### Loss와 Accuracy 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "24b71817",
   "metadata": {},
   "source": [
    "history_dict = history.history\n",
    "print(history_dict.keys())"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "b7be7952",
   "metadata": {},
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# history에서 손실과 정확도, 검증 손실과 검증 정확도를 추출합니다.\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "fig = plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs, acc, 'ro', label='Training accuracy')\n",
    "plt.plot(epochs, val_acc, 'r', label='Validation accuracy')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "7befb1d8",
   "metadata": {},
   "source": [
    "### 모델 평가하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "544cc1f2",
   "metadata": {},
   "source": [
    "def decoder_inference(sentence, model, tokenizer, max_length):\n",
    "    # 문장 전처리\n",
    "    sentence = preprocess_sentence(sentence)\n",
    "\n",
    "    # 토큰화한 후에 뒤에 DELIMITER_TOKEN을 붙여줌\n",
    "    input_sequence = tf.constant(START_TOKEN + tokenizer.encode(sentence) + DELIMITER_TOKEN)\n",
    "\n",
    "    input_sequence = tf.expand_dims(input_sequence, axis=0)  # Shape: (1, seq_length)\n",
    "\n",
    "    # look-ahead mask를 input으로 넣어줘야 하기에 이를 추가해준다\n",
    "    look_ahead_mask = create_look_ahead_mask(input_sequence)\n",
    "\n",
    "    # DELIMITER_TOKEN으로 output을 초기화\n",
    "    output_sequence = tf.constant([DELIMITER_TOKEN])\n",
    "\n",
    "    for _ in range(max_length):\n",
    "        # 다음 토큰을 예측\n",
    "        predictions = model(inputs=[input_sequence, look_ahead_mask], training=False)\n",
    "        predictions = predictions[:, -1:, :]\n",
    "\n",
    "        predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
    "\n",
    "        if tf.equal(predicted_id, END_TOKEN):\n",
    "            break\n",
    "\n",
    "        output_sequence = tf.concat([output_sequence, predicted_id], axis=-1)\n",
    "\n",
    "        input_sequence = tf.concat([input_sequence, predicted_id], axis=-1)\n",
    "        look_ahead_mask = create_look_ahead_mask(input_sequence)\n",
    "\n",
    "    output_sequence = tf.squeeze(output_sequence, axis=0)\n",
    "    predicted_sentence = tokenizer.decode([i for i in output_sequence.numpy() if i < tokenizer.vocab_size])\n",
    "\n",
    "    return predicted_sentence\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "c435c4f9",
   "metadata": {},
   "source": [
    "def sentence_generation(sentence, model, tokenizer, max_length):\n",
    "    predicted_sentence = decoder_inference(sentence, model, tokenizer, max_length)\n",
    "\n",
    "    print('입력 : {}'.format(sentence))\n",
    "    print('출력 : {}'.format(predicted_sentence))\n",
    "\n",
    "    return predicted_sentence\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "2afd0f17",
   "metadata": {},
   "source": [
    "## 테스트 및 평가"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ecab5d",
   "metadata": {},
   "source": [
    "- 기존에 transformer 모델보다 일반화 성능이 더 좋은 것으로 보인다\n",
    "- 시간상의 이유로 10 epoch을 하였음에도 기존에 transformer는 10 epoch 수행시 엉뚱한 말만 했는데 어울리는 말을 잘 하는 것을 볼 수 있다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "10ed8f5f",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "sentence_generation('나 오늘 헤어졌어', finetune_model, tokenizer, max_length=50)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "c75bfd67",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "sentence_generation('좋아하는 사람 생겼어', finetune_model, tokenizer, max_length=50)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "9d918f2a",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "sentence_generation(\"오늘 날씨 좋다\", finetune_model, tokenizer, max_length=50)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "96c1380c",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "sentence_generation(\"여행 가고 싶어\", finetune_model, tokenizer, max_length=50)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "0e46e6a7",
   "metadata": {},
   "source": [
    "# 회고\n",
    "\n",
    "- 배운 점 \n",
    "    - 논문을 읽고 이를 바탕으로 GPT 모델을 직접 구현해볼 수 있었다\n",
    "- 아쉬운 점 \n",
    "    - Unsupervised pre-training 단계에서 좀 더 다양한 문장들을 실험해보면 좋을 것 같았다\n",
    "    - 데이터셋이 적어서 pretrain weight를 불러오는게 오히려 성능이 좋지 않았다\n",
    "- 느낀 점\n",
    "    - 어려운 개념이라도 노력하면 이해할 수 있다는 것을 느꼈다\n",
    "    - transformer보다도 일반화 성능이 잘 되는 것 같았다\n",
    "- 어려웠던 점 \n",
    "    - 직접 구현하는 과정에서 모델 shape를 맞추는 것이 생각보다 까다로웠다\n",
    "    - 오히려 기존 코드를 수정하는 과정에서 오류가 많이 났다"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
