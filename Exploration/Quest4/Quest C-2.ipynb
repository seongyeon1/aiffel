{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Sequential model",
   "id": "2096e3da1e618ce"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-28T05:04:44.312015Z",
     "start_time": "2024-05-28T05:04:42.461504Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(64, activation=\"relu\"),\n",
    "    layers.Dense(10, activation=\"softmax\")\n",
    "])"
   ],
   "id": "715c86d550cb799",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-28 14:04:44.306171: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-05-28 14:04:44.306261: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "점진적으로 Sequential Model 만들기",
   "id": "e9ad57e5c7e97e16"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-28T05:04:46.217921Z",
     "start_time": "2024-05-28T05:04:46.212393Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = keras.Sequential()\n",
    "model.add(layers.Dense(64, activation=\"relu\"))\n",
    "model.add(layers.Dense(10, activation=\"softmax\"))"
   ],
   "id": "f3d8b4571deb38ba",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Functional API model",
   "id": "7863c5fe881bcb44"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-28T05:04:47.898008Z",
     "start_time": "2024-05-28T05:04:47.674955Z"
    }
   },
   "cell_type": "code",
   "source": [
    "inputs = keras.Input(shape=(3,), name=\"my_input\")\n",
    "features = layers.Dense(64, activation=\"relu\")(inputs)\n",
    "outputs = layers.Dense(10, activation=\"softmax\")(features)\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)"
   ],
   "id": "d3f8dff9f34d6523",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Subclassing model ",
   "id": "f21e7d5c4cf97ccf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-28T05:10:29.654792Z",
     "start_time": "2024-05-28T05:10:29.566098Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "num_samples = 1280\n",
    "vocabulary_size = 10000\n",
    "num_tags = 100\n",
    "num_departments = 4\n",
    "\n",
    "title_data = np.random.randint(0, 2, size=(num_samples, vocabulary_size)) # dummy 입력 데이터\n",
    "text_body_data = np.random.randint(0, 2, size=(num_samples, vocabulary_size))\n",
    "tags_data = np.random.randint(0, 2, size=(num_samples, num_tags)) # dummy 입력 데이터\n",
    "\n",
    "priority_data = np.random.random(size=(num_samples, 1)) # dummy 타겟 데이터\n",
    "department_data = np.random.randint(0, 2, size=(num_samples, num_departments)) # dummy 타겟 데이터"
   ],
   "id": "537a12937da23be3",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-28T05:10:30.476724Z",
     "start_time": "2024-05-28T05:10:30.472082Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class CustomerTicketModel(keras.Model):\n",
    "\n",
    "    def __init__(self, num_departments):\n",
    "        super().__init__()  # 부모 클래스의 생성자를 호출\n",
    "        # 생성자에서 층을 정의\n",
    "        self.concat_layer = layers.Concatenate() \n",
    "        self.mixing_layer = layers.Dense(64, activation=\"relu\")\n",
    "        self.priority_scorer = layers.Dense(1, activation=\"sigmoid\")\n",
    "        self.department_classifier = layers.Dense(\n",
    "            num_departments, activation=\"softmax\")\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        '''\n",
    "        call() 메서드에서 정방향 패스를 정의\n",
    "        '''\n",
    "        title = inputs[\"title\"]\n",
    "        text_body = inputs[\"text_body\"]\n",
    "        tags = inputs[\"tags\"]\n",
    "\n",
    "        features = self.concat_layer([title, text_body, tags])\n",
    "        features = self.mixing_layer(features)\n",
    "        priority = self.priority_scorer(features)\n",
    "        department = self.department_classifier(features)\n",
    "        return priority, department"
   ],
   "id": "57d8ca08d36e09bd",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-28T05:10:31.850611Z",
     "start_time": "2024-05-28T05:10:31.314727Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = CustomerTicketModel(num_departments=4)\n",
    "\n",
    "priority, department = model(\n",
    "    {\"title\": title_data, \"text_body\": text_body_data, \"tags\": tags_data})"
   ],
   "id": "7127dd0f6c908835",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 혼합 모델",
   "id": "32d0b6ccfe6413db"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-28T05:10:41.123132Z",
     "start_time": "2024-05-28T05:10:41.095790Z"
    }
   },
   "cell_type": "code",
   "source": [
    "vocabulary_size = 10000\n",
    "num_tags = 100\n",
    "num_departments = 4\n",
    "\n",
    "# 모델의 입력을 정의\n",
    "title = keras.Input(shape=(vocabulary_size,), name=\"title\")          \n",
    "text_body = keras.Input(shape=(vocabulary_size,), name=\"text_body\")\n",
    "tags = keras.Input(shape=(num_tags,), name=\"tags\")\n",
    "\n",
    "features = layers.Concatenate()([title, text_body, tags]) # 입력 특성을 하나의 텐서 features로 연결\n",
    "features = layers.Dense(64, activation=\"relu\")(features)  # 중간층을 적용하여 입력 특성을 더 풍부한 표현으로 재결합\n",
    "\n",
    "# 모델의 출력 정의\n",
    "priority = layers.Dense(1, activation=\"sigmoid\", name=\"priority\")(features) \n",
    "department = layers.Dense(\n",
    "    num_departments, activation=\"softmax\", name=\"department\")(features)\n",
    "\n",
    "# 입력과 출력을 지정하여 모델을 만듦\n",
    "model = keras.Model(inputs=[title, text_body, tags], outputs=[priority, department])"
   ],
   "id": "5b9c980cb6dc1626",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 사용자 정의 훈련 스탭을 사용하는 모델",
   "id": "53317efa91d252d8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-28T05:32:39.795079Z",
     "start_time": "2024-05-28T05:32:39.527060Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "# 1) 모델 생성 (나중에 재사용하기 용이하기 위해 별도의 함수로 만듦)\n",
    "def get_mnist_model():\n",
    "    inputs = keras.Input(shape=(28 * 28,))\n",
    "    features = layers.Dense(512, activation=\"relu\")(inputs)\n",
    "    features = layers.Dropout(0.5)(features)\n",
    "    outputs = layers.Dense(10, activation=\"softmax\")(features)\n",
    "    model = keras.Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "# 2) load and split data\n",
    "(images, labels), (test_images, test_labels) = mnist.load_data()\n",
    "images = images.reshape((60000, 28 * 28)).astype(\"float32\") / 255\n",
    "test_images = test_images.reshape((10000, 28 * 28)).astype(\"float32\") / 255\n",
    "train_images, val_images = images[10000:], images[:10000]\n",
    "train_labels, val_labels = labels[10000:], labels[:10000]\n"
   ],
   "id": "8a6394c9067b76a3",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-28T05:32:41.129064Z",
     "start_time": "2024-05-28T05:32:41.053361Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = get_mnist_model()\n",
    "\n",
    "loss_fn = keras.losses.SparseCategoricalCrossentropy()  # 손실함수 정의\n",
    "optimizer = keras.optimizers.RMSprop()                  # 옵티마이저 준비\n",
    "metrics = [keras.metrics.SparseCategoricalAccuracy()]   # 모니터링할 지표 리스트 준비\n",
    "loss_tracking_metric = keras.metrics.Mean()             # 손실 평균을 추적할 평균 지표 준비\n",
    "\n",
    "def train_step(inputs, targets):\n",
    "    # 정방향 패스를 실행. training=True 전달\n",
    "    with tf.GradientTape() as tape:                     \n",
    "        predictions = model(inputs, training=True)\n",
    "        loss = loss_fn(targets, predictions)\n",
    "    # 역방향 패스를 실행. model.trainable_weights 사용\n",
    "    gradients = tape.gradient(loss, model.trainable_weights)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_weights))\n",
    "    logs = {}\n",
    "    \n",
    "    # 측정 지표를 계산\n",
    "    for metric in metrics:\n",
    "        metric.update_state(targets, predictions)\n",
    "        logs[metric.name] = metric.result()\n",
    "    # 손실 평균을 계산\n",
    "    loss_tracking_metric.update_state(loss)\n",
    "    logs[\"loss\"] = loss_tracking_metric.result()\n",
    "    return logs # 지표와 손실의 현재 값을 반환"
   ],
   "id": "106df8623aadc7f9",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-28T05:32:42.760480Z",
     "start_time": "2024-05-28T05:32:42.756159Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def reset_metrics():\n",
    "    for metric in metrics:\n",
    "        metric.reset_state()\n",
    "    loss_tracking_metric.reset_state()"
   ],
   "id": "63641b9a699b029b",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-28T05:33:32.461192Z",
     "start_time": "2024-05-28T05:32:43.960015Z"
    }
   },
   "cell_type": "code",
   "source": [
    "training_dataset = tf.data.Dataset.from_tensor_slices((train_images, train_labels))\n",
    "training_dataset = training_dataset.batch(32)\n",
    "epochs = 3\n",
    "for epoch in range(epochs):\n",
    "    reset_metrics()\n",
    "    for inputs_batch, targets_batch in training_dataset:\n",
    "        logs = train_step(inputs_batch, targets_batch)\n",
    "    print(f\"Results at the end of epoch {epoch}\")\n",
    "    for key, value in logs.items():\n",
    "        print(f\"...{key}: {value:.4f}\")"
   ],
   "id": "64f24e67a2bb7054",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results at the end of epoch 0\n",
      "...sparse_categorical_accuracy: 0.9189\n",
      "...loss: 0.2748\n",
      "Results at the end of epoch 1\n",
      "...sparse_categorical_accuracy: 0.9659\n",
      "...loss: 0.1232\n",
      "Results at the end of epoch 2\n",
      "...sparse_categorical_accuracy: 0.9783\n",
      "...loss: 0.0797\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-28T05:33:34.287670Z",
     "start_time": "2024-05-28T05:33:32.462684Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def test_step(inputs, targets):\n",
    "    predictions = model(inputs, training=False)\n",
    "    loss = loss_fn(targets, predictions)\n",
    "\n",
    "    logs = {}\n",
    "    for metric in metrics:\n",
    "        metric.update_state(targets, predictions)\n",
    "        logs[\"val_\" + metric.name] = metric.result()\n",
    "\n",
    "    loss_tracking_metric.update_state(loss)\n",
    "    logs[\"val_loss\"] = loss_tracking_metric.result()\n",
    "    return logs\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((val_images, val_labels))\n",
    "val_dataset = val_dataset.batch(32)\n",
    "reset_metrics()\n",
    "for inputs_batch, targets_batch in val_dataset:\n",
    "    logs = test_step(inputs_batch, targets_batch)\n",
    "print(\"Evaluation results:\")\n",
    "for key, value in logs.items():\n",
    "    print(f\"...{key}: {value:.4f}\")"
   ],
   "id": "920b3f1751d18c31",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results:\n",
      "...val_sparse_categorical_accuracy: 0.9599\n",
      "...val_loss: 0.1692\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-28T05:11:00.473564Z",
     "start_time": "2024-05-28T05:11:00.471161Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ],
   "id": "fb2eb130b5a99822",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-28T05:13:09.681198Z",
     "start_time": "2024-05-28T05:13:09.402010Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# # MNIST 데이터셋 로드\n",
    "# (train_images, train_labels), (test_images, test_labels) = keras.datasets.mnist.load_data()\n",
    "# \n",
    "# # 전처리 (정규화 및 차원 확장)\n",
    "# train_images = train_images.astype(\"float32\") / 255.0\n",
    "# test_images = test_images.astype(\"float32\") / 255.0\n",
    "# train_images = tf.expand_dims(train_images, -1)\n",
    "# test_images = tf.expand_dims(test_images, -1)"
   ],
   "id": "5d481421269d02fc",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-28T05:13:10.524565Z",
     "start_time": "2024-05-28T05:13:10.520214Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# def create_model():\n",
    "#     inputs = keras.Input(shape=(28, 28, 1))\n",
    "#     x = layers.Conv2D(32, 3, activation='relu')(inputs)\n",
    "#     x = layers.MaxPooling2D()(x)\n",
    "#     x = layers.Conv2D(64, 3, activation='relu')(x)\n",
    "#     x = layers.MaxPooling2D()(x)\n",
    "#     x = layers.Flatten()(x)\n",
    "#     x = layers.Dense(100, activation='relu')(x)\n",
    "#     outputs = layers.Dense(10)(x)\n",
    "#     model = keras.Model(inputs, outputs)\n",
    "#     return model"
   ],
   "id": "b90076ffc5a08727",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-28T05:38:11.860886Z",
     "start_time": "2024-05-28T05:38:11.838548Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class CustomModel(keras.Model):\n",
    "    def train_step(self, data):\n",
    "        images, labels = data\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = self(images, training=True)\n",
    "            loss = self.compiled_loss(labels, predictions, regularization_losses=self.losses)\n",
    "\n",
    "        gradients = tape.gradient(loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "        self.compiled_metrics.update_state(labels, predictions)\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "# 모델 인스턴스화\n",
    "model = get_mnist_model()\n",
    "custom_model = CustomModel(inputs=model.input, outputs=model.output)"
   ],
   "id": "be7bbeb756e72c48",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-28T05:39:01.797570Z",
     "start_time": "2024-05-28T05:38:13.929121Z"
    }
   },
   "cell_type": "code",
   "source": [
    "custom_model.compile(optimizer=keras.optimizers.Adam(),\n",
    "                     loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                     metrics=[keras.metrics.SparseCategoricalAccuracy()])\n",
    "\n",
    "\n",
    "\n",
    "# 모델 훈련\n",
    "custom_model.fit(train_images, train_labels, epochs=5, batch_size=32)"
   ],
   "id": "7d494a45cdd85f4e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/DL-tf/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:1082: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
      "  return dispatch_target(*args, **kwargs)\n",
      "2024-05-28 14:38:14.228752: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1563/1563 [==============================] - 9s 6ms/step - loss: 0.2775 - sparse_categorical_accuracy: 0.9163\n",
      "Epoch 2/5\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 0.1371 - sparse_categorical_accuracy: 0.9581\n",
      "Epoch 3/5\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 0.1044 - sparse_categorical_accuracy: 0.9674\n",
      "Epoch 4/5\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 0.0819 - sparse_categorical_accuracy: 0.9737\n",
      "Epoch 5/5\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 0.0712 - sparse_categorical_accuracy: 0.9781\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1773d1e20>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 학습률이 변화하는 모델",
   "id": "81bdab7e177f078b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-28T05:39:01.823829Z",
     "start_time": "2024-05-28T05:39:01.799260Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class CustomModel(keras.Model):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(CustomModel, self).__init__(*args, **kwargs)\n",
    "        self.initial_lr = 0.001\n",
    "\n",
    "    def compile(self, optimizer, loss, metrics, schedule_lr=None):\n",
    "        super(CustomModel, self).compile(optimizer, loss, metrics)\n",
    "        self.schedule_lr = schedule_lr\n",
    "\n",
    "    def train_step(self, data):\n",
    "        images, labels = data\n",
    "        if self.schedule_lr:\n",
    "            self.optimizer.learning_rate = self.schedule_lr(self.optimizer.iterations)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = self(images, training=True)\n",
    "            loss = self.compiled_loss(labels, predictions, regularization_losses=self.losses)\n",
    "\n",
    "        gradients = tape.gradient(loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "\n",
    "        self.compiled_metrics.update_state(labels, predictions)\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "# 학습률 스케줄링 함수 정의\n",
    "def schedule_lr(step):\n",
    "    initial_lr = 0.001\n",
    "    decay_steps = 1000\n",
    "    decay_rate = 0.1\n",
    "    lr = initial_lr * (decay_rate ** (step // decay_steps))\n",
    "    return lr\n",
    "\n",
    "# 모델 인스턴스화\n",
    "model = get_mnist_model()\n",
    "custom_model = CustomModel(inputs=model.input, outputs=model.output)"
   ],
   "id": "cdadb2edf4004a28",
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Image Data : Layer 의 너비가 달라지는 경우",
   "id": "a8f6401329cc2f84"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# MNIST 데이터셋 로드\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train = x_train.reshape(-1, 28 * 28) / 255.0\n",
    "x_test = x_test.reshape(-1, 28 * 28) / 255.0\n",
    "y_train = to_categorical(y_train, 10)\n",
    "y_test = to_categorical(y_test, 10)\n",
    "\n",
    "# 모델 1: 작은 너비의 레이어\n",
    "model_small = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(784,)),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model_small.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# 모델 2: 큰 너비의 레이어\n",
    "model_large = Sequential([\n",
    "    Dense(256, activation='relu', input_shape=(784,)),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model_large.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# 모델 학습\n",
    "print(\"Training small model...\")\n",
    "history_small = model_small.fit(x_train, y_train, epochs=10, validation_data=(x_test, y_test), verbose=2)\n",
    "\n",
    "print(\"Training large model...\")\n",
    "history_large = model_large.fit(x_train, y_train, epochs=10, validation_data=(x_test, y_test), verbose=2)\n",
    "\n",
    "# 모델 성능 비교\n",
    "print(\"\\nSmall Model Performance:\")\n",
    "loss_small, accuracy_small = model_small.evaluate(x_test, y_test, verbose=2)\n",
    "\n",
    "print(f\"Test loss: {loss_small}, Test accuracy: {accuracy_small}\")\n",
    "print(\"\\nLarge Model Performance:\")\n",
    "loss_large, accuracy_large = model_large.evaluate(x_test, y_test, verbose=2)\n",
    "print(f\"Test loss: {loss_large}, Test accuracy: {accuracy_large}\")"
   ],
   "id": "62879b6fe3dd0199"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-28T06:21:39.776605Z",
     "start_time": "2024-05-28T06:11:20.273472Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "\n",
    "\n",
    "# MNIST 데이터셋 로드 및 전처리\n",
    "def load_mnist_data():\n",
    "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "    x_train = x_train.reshape(-1, 28 * 28) / 255.0\n",
    "    x_test = x_test.reshape(-1, 28 * 28) / 255.0\n",
    "    y_train = to_categorical(y_train, 10)\n",
    "    y_test = to_categorical(y_test, 10)\n",
    "    return x_train, y_train, x_test, y_test\n",
    "\n",
    "# 모델 생성 함수\n",
    "def create_model(layer_widths):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(layer_widths[0], activation='relu', input_shape=(784,)))\n",
    "    for width in layer_widths[1:]:\n",
    "        model.add(Dense(width, activation='relu'))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# 학습 및 평가 함수\n",
    "def train_and_evaluate(model, x_train, y_train, x_test, y_test, epochs=10):\n",
    "    model.fit(x_train, y_train, epochs=epochs, validation_data=(x_test, y_test), verbose=2)\n",
    "    loss, accuracy = model.evaluate(x_test, y_test, verbose=2)\n",
    "    return loss, accuracy\n",
    "\n",
    "# 주 실행부\n",
    "def main():\n",
    "    x_train, y_train, x_test, y_test = load_mnist_data()\n",
    "    # 모델 정의\n",
    "    basic_model = create_model([128, 64])\n",
    "    wide_model = create_model([512, 256])\n",
    "    narrow_model = create_model([32, 16])\n",
    "    \n",
    "    # 학습 및 성능 비교\n",
    "    print(\"Training basic model...\")\n",
    "    basic_loss, basic_accuracy = train_and_evaluate(basic_model, x_train, y_train, x_test, y_test)\n",
    "    \n",
    "    print(\"Training wide model...\")\n",
    "    wide_loss, wide_accuracy = train_and_evaluate(wide_model, x_train, y_train, x_test, y_test)\n",
    "\n",
    "    print(\"Training narrow model...\")\n",
    "    narrow_loss, narrow_accuracy = train_and_evaluate(narrow_model, x_train, y_train, x_test, y_test)\n",
    "\n",
    "    print(\"\\nPerformance Comparison:\")\n",
    "    print(f\"Basic Model - Test Loss: {basic_loss}, Test Accuracy: {basic_accuracy}\")\n",
    "    print(f\"Wide Model - Test Loss: {wide_loss}, Test Accuracy: {wide_accuracy}\")\n",
    "    print(f\"Narrow Model - Test Loss: {narrow_loss}, Test Accuracy: {narrow_accuracy}\")\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "id": "8808966cb0e247a4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training basic model...\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-28 15:11:21.679839: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-05-28 15:11:37.792124: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875/1875 - 19s - loss: 0.2425 - accuracy: 0.9279 - val_loss: 0.1294 - val_accuracy: 0.9594 - 19s/epoch - 10ms/step\n",
      "Epoch 2/10\n",
      "1875/1875 - 18s - loss: 0.1015 - accuracy: 0.9691 - val_loss: 0.0972 - val_accuracy: 0.9697 - 18s/epoch - 9ms/step\n",
      "Epoch 3/10\n",
      "1875/1875 - 18s - loss: 0.0712 - accuracy: 0.9783 - val_loss: 0.0849 - val_accuracy: 0.9735 - 18s/epoch - 10ms/step\n",
      "Epoch 4/10\n",
      "1875/1875 - 18s - loss: 0.0544 - accuracy: 0.9828 - val_loss: 0.0823 - val_accuracy: 0.9775 - 18s/epoch - 10ms/step\n",
      "Epoch 5/10\n",
      "1875/1875 - 19s - loss: 0.0421 - accuracy: 0.9858 - val_loss: 0.0893 - val_accuracy: 0.9738 - 19s/epoch - 10ms/step\n",
      "Epoch 6/10\n",
      "1875/1875 - 24s - loss: 0.0336 - accuracy: 0.9888 - val_loss: 0.0785 - val_accuracy: 0.9777 - 24s/epoch - 13ms/step\n",
      "Epoch 7/10\n",
      "1875/1875 - 20s - loss: 0.0277 - accuracy: 0.9909 - val_loss: 0.0919 - val_accuracy: 0.9761 - 20s/epoch - 11ms/step\n",
      "Epoch 8/10\n",
      "1875/1875 - 22s - loss: 0.0233 - accuracy: 0.9921 - val_loss: 0.1022 - val_accuracy: 0.9732 - 22s/epoch - 12ms/step\n",
      "Epoch 9/10\n",
      "1875/1875 - 19s - loss: 0.0214 - accuracy: 0.9926 - val_loss: 0.0811 - val_accuracy: 0.9787 - 19s/epoch - 10ms/step\n",
      "Epoch 10/10\n",
      "1875/1875 - 18s - loss: 0.0191 - accuracy: 0.9938 - val_loss: 0.0929 - val_accuracy: 0.9769 - 18s/epoch - 10ms/step\n",
      "313/313 - 2s - loss: 0.0929 - accuracy: 0.9769 - 2s/epoch - 8ms/step\n",
      "Training wide model...\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-28 15:14:39.305530: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-05-28 15:14:58.156684: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875/1875 - 24s - loss: 0.1858 - accuracy: 0.9431 - val_loss: 0.0970 - val_accuracy: 0.9696 - 24s/epoch - 13ms/step\n",
      "Epoch 2/10\n",
      "1875/1875 - 22s - loss: 0.0785 - accuracy: 0.9755 - val_loss: 0.0723 - val_accuracy: 0.9772 - 22s/epoch - 12ms/step\n",
      "Epoch 3/10\n",
      "1875/1875 - 21s - loss: 0.0529 - accuracy: 0.9831 - val_loss: 0.0692 - val_accuracy: 0.9805 - 21s/epoch - 11ms/step\n",
      "Epoch 4/10\n",
      "1875/1875 - 22s - loss: 0.0408 - accuracy: 0.9870 - val_loss: 0.0704 - val_accuracy: 0.9786 - 22s/epoch - 12ms/step\n",
      "Epoch 5/10\n",
      "1875/1875 - 20s - loss: 0.0328 - accuracy: 0.9897 - val_loss: 0.0662 - val_accuracy: 0.9821 - 20s/epoch - 11ms/step\n",
      "Epoch 6/10\n",
      "1875/1875 - 22s - loss: 0.0281 - accuracy: 0.9906 - val_loss: 0.0937 - val_accuracy: 0.9776 - 22s/epoch - 12ms/step\n",
      "Epoch 7/10\n",
      "1875/1875 - 20s - loss: 0.0234 - accuracy: 0.9923 - val_loss: 0.0927 - val_accuracy: 0.9799 - 20s/epoch - 11ms/step\n",
      "Epoch 8/10\n",
      "1875/1875 - 21s - loss: 0.0192 - accuracy: 0.9937 - val_loss: 0.0857 - val_accuracy: 0.9799 - 21s/epoch - 11ms/step\n",
      "Epoch 9/10\n",
      "1875/1875 - 20s - loss: 0.0188 - accuracy: 0.9942 - val_loss: 0.1343 - val_accuracy: 0.9757 - 20s/epoch - 11ms/step\n",
      "Epoch 10/10\n",
      "1875/1875 - 20s - loss: 0.0164 - accuracy: 0.9949 - val_loss: 0.0979 - val_accuracy: 0.9828 - 20s/epoch - 10ms/step\n",
      "313/313 - 3s - loss: 0.0979 - accuracy: 0.9828 - 3s/epoch - 8ms/step\n",
      "Training narrow model...\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-28 15:18:14.131724: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-05-28 15:18:31.312628: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875/1875 - 20s - loss: 0.3714 - accuracy: 0.8954 - val_loss: 0.1959 - val_accuracy: 0.9428 - 20s/epoch - 11ms/step\n",
      "Epoch 2/10\n",
      "1875/1875 - 19s - loss: 0.1829 - accuracy: 0.9458 - val_loss: 0.1577 - val_accuracy: 0.9548 - 19s/epoch - 10ms/step\n",
      "Epoch 3/10\n",
      "1875/1875 - 20s - loss: 0.1458 - accuracy: 0.9571 - val_loss: 0.1393 - val_accuracy: 0.9582 - 20s/epoch - 11ms/step\n",
      "Epoch 4/10\n",
      "1875/1875 - 21s - loss: 0.1232 - accuracy: 0.9631 - val_loss: 0.1253 - val_accuracy: 0.9631 - 21s/epoch - 11ms/step\n",
      "Epoch 5/10\n",
      "1875/1875 - 21s - loss: 0.1086 - accuracy: 0.9674 - val_loss: 0.1200 - val_accuracy: 0.9640 - 21s/epoch - 11ms/step\n",
      "Epoch 6/10\n",
      "1875/1875 - 20s - loss: 0.0976 - accuracy: 0.9713 - val_loss: 0.1270 - val_accuracy: 0.9634 - 20s/epoch - 11ms/step\n",
      "Epoch 7/10\n",
      "1875/1875 - 20s - loss: 0.0885 - accuracy: 0.9728 - val_loss: 0.1106 - val_accuracy: 0.9659 - 20s/epoch - 11ms/step\n",
      "Epoch 8/10\n",
      "1875/1875 - 20s - loss: 0.0823 - accuracy: 0.9750 - val_loss: 0.1189 - val_accuracy: 0.9659 - 20s/epoch - 11ms/step\n",
      "Epoch 9/10\n",
      "1875/1875 - 19s - loss: 0.0768 - accuracy: 0.9762 - val_loss: 0.1181 - val_accuracy: 0.9679 - 19s/epoch - 10ms/step\n",
      "Epoch 10/10\n",
      "1875/1875 - 22s - loss: 0.0703 - accuracy: 0.9784 - val_loss: 0.1162 - val_accuracy: 0.9686 - 22s/epoch - 12ms/step\n",
      "313/313 - 3s - loss: 0.1162 - accuracy: 0.9686 - 3s/epoch - 9ms/step\n",
      "\n",
      "Performance Comparison:\n",
      "Basic Model - Test Loss: 0.09291435778141022, Test Accuracy: 0.9768999814987183\n",
      "Wide Model - Test Loss: 0.09785723686218262, Test Accuracy: 0.9828000068664551\n",
      "Narrow Model - Test Loss: 0.1162160187959671, Test Accuracy: 0.9685999751091003\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "c366e08f2c7f91ae"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
