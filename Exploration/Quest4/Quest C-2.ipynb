{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Sequential model",
   "id": "2096e3da1e618ce"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-28T05:04:44.312015Z",
     "start_time": "2024-05-28T05:04:42.461504Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(64, activation=\"relu\"),\n",
    "    layers.Dense(10, activation=\"softmax\")\n",
    "])"
   ],
   "id": "715c86d550cb799",
   "execution_count": 3,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "점진적으로 Sequential Model 만들기",
   "id": "e9ad57e5c7e97e16"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-28T05:04:46.217921Z",
     "start_time": "2024-05-28T05:04:46.212393Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = keras.Sequential()\n",
    "model.add(layers.Dense(64, activation=\"relu\"))\n",
    "model.add(layers.Dense(10, activation=\"softmax\"))"
   ],
   "id": "f3d8b4571deb38ba",
   "execution_count": 4,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Functional API model",
   "id": "7863c5fe881bcb44"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-28T05:04:47.898008Z",
     "start_time": "2024-05-28T05:04:47.674955Z"
    }
   },
   "cell_type": "code",
   "source": [
    "inputs = keras.Input(shape=(3,), name=\"my_input\")\n",
    "features = layers.Dense(64, activation=\"relu\")(inputs)\n",
    "outputs = layers.Dense(10, activation=\"softmax\")(features)\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)"
   ],
   "id": "d3f8dff9f34d6523",
   "execution_count": 5,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Subclassing model ",
   "id": "f21e7d5c4cf97ccf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-28T05:10:29.654792Z",
     "start_time": "2024-05-28T05:10:29.566098Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "num_samples = 1280\n",
    "vocabulary_size = 10000\n",
    "num_tags = 100\n",
    "num_departments = 4\n",
    "\n",
    "title_data = np.random.randint(0, 2, size=(num_samples, vocabulary_size)) # dummy 입력 데이터\n",
    "text_body_data = np.random.randint(0, 2, size=(num_samples, vocabulary_size))\n",
    "tags_data = np.random.randint(0, 2, size=(num_samples, num_tags)) # dummy 입력 데이터\n",
    "\n",
    "priority_data = np.random.random(size=(num_samples, 1)) # dummy 타겟 데이터\n",
    "department_data = np.random.randint(0, 2, size=(num_samples, num_departments)) # dummy 타겟 데이터"
   ],
   "id": "537a12937da23be3",
   "execution_count": 11,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-28T05:10:30.476724Z",
     "start_time": "2024-05-28T05:10:30.472082Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class CustomerTicketModel(keras.Model):\n",
    "\n",
    "    def __init__(self, num_departments):\n",
    "        super().__init__()  # 부모 클래스의 생성자를 호출\n",
    "        # 생성자에서 층을 정의\n",
    "        self.concat_layer = layers.Concatenate() \n",
    "        self.mixing_layer = layers.Dense(64, activation=\"relu\")\n",
    "        self.priority_scorer = layers.Dense(1, activation=\"sigmoid\")\n",
    "        self.department_classifier = layers.Dense(\n",
    "            num_departments, activation=\"softmax\")\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        '''\n",
    "        call() 메서드에서 정방향 패스를 정의\n",
    "        '''\n",
    "        title = inputs[\"title\"]\n",
    "        text_body = inputs[\"text_body\"]\n",
    "        tags = inputs[\"tags\"]\n",
    "\n",
    "        features = self.concat_layer([title, text_body, tags])\n",
    "        features = self.mixing_layer(features)\n",
    "        priority = self.priority_scorer(features)\n",
    "        department = self.department_classifier(features)\n",
    "        return priority, department"
   ],
   "id": "57d8ca08d36e09bd",
   "execution_count": 12,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-28T05:10:31.850611Z",
     "start_time": "2024-05-28T05:10:31.314727Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = CustomerTicketModel(num_departments=4)\n",
    "\n",
    "priority, department = model(\n",
    "    {\"title\": title_data, \"text_body\": text_body_data, \"tags\": tags_data})"
   ],
   "id": "7127dd0f6c908835",
   "execution_count": 13,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 혼합 모델",
   "id": "32d0b6ccfe6413db"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-28T05:10:41.123132Z",
     "start_time": "2024-05-28T05:10:41.095790Z"
    }
   },
   "cell_type": "code",
   "source": [
    "vocabulary_size = 10000\n",
    "num_tags = 100\n",
    "num_departments = 4\n",
    "\n",
    "# 모델의 입력을 정의\n",
    "title = keras.Input(shape=(vocabulary_size,), name=\"title\")          \n",
    "text_body = keras.Input(shape=(vocabulary_size,), name=\"text_body\")\n",
    "tags = keras.Input(shape=(num_tags,), name=\"tags\")\n",
    "\n",
    "features = layers.Concatenate()([title, text_body, tags]) # 입력 특성을 하나의 텐서 features로 연결\n",
    "features = layers.Dense(64, activation=\"relu\")(features)  # 중간층을 적용하여 입력 특성을 더 풍부한 표현으로 재결합\n",
    "\n",
    "# 모델의 출력 정의\n",
    "priority = layers.Dense(1, activation=\"sigmoid\", name=\"priority\")(features) \n",
    "department = layers.Dense(\n",
    "    num_departments, activation=\"softmax\", name=\"department\")(features)\n",
    "\n",
    "# 입력과 출력을 지정하여 모델을 만듦\n",
    "model = keras.Model(inputs=[title, text_body, tags], outputs=[priority, department])"
   ],
   "id": "5b9c980cb6dc1626",
   "execution_count": 14,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 사용자 정의 훈련 스탭을 사용하는 모델\n",
    "\n",
    "사용자 정의 훈련 스텝을 사용하는 이유\n",
    "1. **복잡한 모델 트레이닝 로직:**\n",
    "    - 기본 `fit` 메서드는 일반적인 훈련 루프를 제공하지만, 특별한 요구사항이 있는 경우 이를 커스터마이즈해야 합니다.\n",
    "        - 예를 들어, GAN(Generative Adversarial Networks)처럼 두 개 이상의 모델을 동시에 훈련시켜야 하는 경우나, 추가적인 손실 함수를 사용하는 경우입니다.\n",
    "2. **보다 세밀한 제어:**\n",
    "    - 훈련 프로세스를 더 세밀하게 제어하고 각 단계에서 무슨 일이 일어나는지 명확히 이해하고자 하는 경우에 유용합니다. 이를 통해 문제를 디버깅하거나 최적화할 수 있습니다.\n",
    "3. **동적 학습률 변경:**\n",
    "    - 학습 중 특정 조건에 따라 학습률을 동적으로 변경하거나 맞춤형 학습률 스케줄링을 적용할 수 있습니다.\n",
    "4. **맞춤형 손실 함수 및 메트릭:**\n",
    "    - 기본 컴파일 옵션에서 제공되지 않는 맞춤형 손실 함수나 메트릭을 사용해야 하는 경우입니다.\n",
    "5. **특별한 데이터 전처리 또는 후처리:**\n",
    "    - 입력 데이터를 특정 방식으로 전처리하거나, 예측 값을 특별한 방식으로 후처리해야 하는 경우입니다.\n",
    "6. **복잡한 그래디언트 계산 및 업데이트:**\n",
    "    - 표준 옵티마이저 업데이트 방식 대신, 맞춤형 그래디언트 계산 및 변수 업데이트 로직을 적용하고자 할 때 사용될 수 있습니다.\n",
    "7. **학습 중 특정 로직 삽입:**\n",
    "    - 학습 중에 특정 조건을 만족할 때마다 특정 작업을 수행하려는 경우, 예를 들어 모델의 일부 가중치를 고정하거나, 조건부 상태 관리를 구현할 때 유용합니다."
   ],
   "id": "53317efa91d252d8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 원래 train_step 코드 (기본 fit 메서드를 실행하면 이 코드가 실행된다)",
   "id": "512510ddba0a885a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "  def train_step(self, data):\n",
    "    \"\"\"The logic for one training step.\n",
    "\n",
    "    This method can be overridden to support custom training logic.\n",
    "    For concrete examples of how to override this method see\n",
    "    [Customizing what happends in fit](https://www.tensorflow.org/guide/keras/customizing_what_happens_in_fit).\n",
    "    This method is called by `Model.make_train_function`.\n",
    "\n",
    "    This method should contain the mathematical logic for one step of training.\n",
    "    This typically includes the forward pass, loss calculation, backpropagation,\n",
    "    and metric updates.\n",
    "\n",
    "    Configuration details for *how* this logic is run (e.g. `tf.function` and\n",
    "    `tf.distribute.Strategy` settings), should be left to\n",
    "    `Model.make_train_function`, which can also be overridden.\n",
    "\n",
    "    Args:\n",
    "      data: A nested structure of `Tensor`s.\n",
    "\n",
    "    Returns:\n",
    "      A `dict` containing values that will be passed to\n",
    "      `tf.keras.callbacks.CallbackList.on_train_batch_end`. Typically, the\n",
    "      values of the `Model`'s metrics are returned. Example:\n",
    "      `{'loss': 0.2, 'accuracy': 0.7}`.\n",
    "    \"\"\"\n",
    "    x, y, sample_weight = data_adapter.unpack_x_y_sample_weight(data)\n",
    "    \n",
    "    # Run forward pass.\n",
    "    with tf.GradientTape() as tape:\n",
    "      y_pred = self(x, training=True)\n",
    "      loss = self.compute_loss(x, y, y_pred, sample_weight)\n",
    "    self._validate_target_and_loss(y, loss)\n",
    "    \n",
    "    # Run backwards pass.\n",
    "    self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\n",
    "    return self.compute_metrics(x, y, y_pred, sample_weight)"
   ],
   "id": "c208362cb0a35590",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 교재 실습",
   "id": "e3f8132869ace29a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-28T05:32:39.795079Z",
     "start_time": "2024-05-28T05:32:39.527060Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "# 1) 모델 생성 (나중에 재사용하기 용이하기 위해 별도의 함수로 만듦)\n",
    "def get_mnist_model():\n",
    "    inputs = keras.Input(shape=(28 * 28,))\n",
    "    features = layers.Dense(512, activation=\"relu\")(inputs)\n",
    "    features = layers.Dropout(0.5)(features)\n",
    "    outputs = layers.Dense(10, activation=\"softmax\")(features)\n",
    "    model = keras.Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "# 2) load and split data\n",
    "(images, labels), (test_images, test_labels) = mnist.load_data()\n",
    "images = images.reshape((60000, 28 * 28)).astype(\"float32\") / 255\n",
    "test_images = test_images.reshape((10000, 28 * 28)).astype(\"float32\") / 255\n",
    "train_images, val_images = images[10000:], images[:10000]\n",
    "train_labels, val_labels = labels[10000:], labels[:10000]\n"
   ],
   "id": "8a6394c9067b76a3",
   "execution_count": 21,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-28T05:32:41.129064Z",
     "start_time": "2024-05-28T05:32:41.053361Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = get_mnist_model()\n",
    "\n",
    "loss_fn = keras.losses.SparseCategoricalCrossentropy()  # 손실함수 정의\n",
    "optimizer = keras.optimizers.RMSprop()                  # 옵티마이저 준비\n",
    "metrics = [keras.metrics.SparseCategoricalAccuracy()]   # 모니터링할 지표 리스트 준비\n",
    "loss_tracking_metric = keras.metrics.Mean()             # 손실 평균을 추적할 평균 지표 준비\n",
    "\n",
    "def train_step(inputs, targets):\n",
    "    # 정방향 패스를 실행. training=True 전달\n",
    "    with tf.GradientTape() as tape:                     \n",
    "        predictions = model(inputs, training=True)\n",
    "        loss = loss_fn(targets, predictions)\n",
    "    # 역방향 패스를 실행. model.trainable_weights 사용\n",
    "    gradients = tape.gradient(loss, model.trainable_weights)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_weights))\n",
    "    logs = {}\n",
    "    \n",
    "    # 측정 지표를 계산\n",
    "    for metric in metrics:\n",
    "        metric.update_state(targets, predictions)\n",
    "        logs[metric.name] = metric.result()\n",
    "    # 손실 평균을 계산\n",
    "    loss_tracking_metric.update_state(loss)\n",
    "    logs[\"loss\"] = loss_tracking_metric.result()\n",
    "    return logs # 지표와 손실의 현재 값을 반환"
   ],
   "id": "106df8623aadc7f9",
   "execution_count": 22,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-28T05:32:42.760480Z",
     "start_time": "2024-05-28T05:32:42.756159Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def reset_metrics():\n",
    "    for metric in metrics:\n",
    "        metric.reset_state()\n",
    "    loss_tracking_metric.reset_state()"
   ],
   "id": "63641b9a699b029b",
   "execution_count": 23,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-28T05:33:32.461192Z",
     "start_time": "2024-05-28T05:32:43.960015Z"
    }
   },
   "cell_type": "code",
   "source": [
    "training_dataset = tf.data.Dataset.from_tensor_slices((train_images, train_labels))\n",
    "training_dataset = training_dataset.batch(32)\n",
    "epochs = 3\n",
    "for epoch in range(epochs):\n",
    "    reset_metrics()\n",
    "    for inputs_batch, targets_batch in training_dataset:\n",
    "        logs = train_step(inputs_batch, targets_batch)\n",
    "    print(f\"Results at the end of epoch {epoch}\")\n",
    "    for key, value in logs.items():\n",
    "        print(f\"...{key}: {value:.4f}\")"
   ],
   "id": "64f24e67a2bb7054",
   "execution_count": 24,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-28T05:33:34.287670Z",
     "start_time": "2024-05-28T05:33:32.462684Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def test_step(inputs, targets):\n",
    "    predictions = model(inputs, training=False)\n",
    "    loss = loss_fn(targets, predictions)\n",
    "\n",
    "    logs = {}\n",
    "    for metric in metrics:\n",
    "        metric.update_state(targets, predictions)\n",
    "        logs[\"val_\" + metric.name] = metric.result()\n",
    "\n",
    "    loss_tracking_metric.update_state(loss)\n",
    "    logs[\"val_loss\"] = loss_tracking_metric.result()\n",
    "    return logs\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((val_images, val_labels))\n",
    "val_dataset = val_dataset.batch(32)\n",
    "reset_metrics()\n",
    "for inputs_batch, targets_batch in val_dataset:\n",
    "    logs = test_step(inputs_batch, targets_batch)\n",
    "print(\"Evaluation results:\")\n",
    "for key, value in logs.items():\n",
    "    print(f\"...{key}: {value:.4f}\")"
   ],
   "id": "920b3f1751d18c31",
   "execution_count": 25,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-28T05:11:00.473564Z",
     "start_time": "2024-05-28T05:11:00.471161Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ],
   "id": "fb2eb130b5a99822",
   "execution_count": 15,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-28T05:38:11.860886Z",
     "start_time": "2024-05-28T05:38:11.838548Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class CustomModel(keras.Model):\n",
    "    def train_step(self, data):\n",
    "        images, labels = data\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = self(images, training=True)\n",
    "            loss = self.compiled_loss(labels, predictions, regularization_losses=self.losses)\n",
    "\n",
    "        gradients = tape.gradient(loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "        self.compiled_metrics.update_state(labels, predictions)\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "# 모델 인스턴스화\n",
    "model = get_mnist_model()\n",
    "custom_model = CustomModel(inputs=model.input, outputs=model.output)"
   ],
   "id": "be7bbeb756e72c48",
   "execution_count": 27,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-28T05:39:01.797570Z",
     "start_time": "2024-05-28T05:38:13.929121Z"
    }
   },
   "cell_type": "code",
   "source": [
    "custom_model.compile(optimizer=keras.optimizers.Adam(),\n",
    "                     loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                     metrics=[keras.metrics.SparseCategoricalAccuracy()])\n",
    "\n",
    "\n",
    "\n",
    "# 모델 훈련\n",
    "custom_model.fit(train_images, train_labels, epochs=5, batch_size=32)"
   ],
   "id": "7d494a45cdd85f4e",
   "execution_count": 28,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 추가 실습 ) 학습률이 변화하는 모델\n",
    "- 스텝(1000)마다 학습률은 decay_rate 비율(0.1)만큼 줄어들게 됨\n",
    "$$ \\text{lr} = \\text{initial\\_lr} \\times (\\text{decay\\_rate})^{\\left(\\frac{\\text{step}}{\\text{decay\\_steps}}\\right)} $$"
   ],
   "id": "81bdab7e177f078b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-28T05:39:01.823829Z",
     "start_time": "2024-05-28T05:39:01.799260Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class CustomModel(keras.Model):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(CustomModel, self).__init__(*args, **kwargs)\n",
    "        self.initial_lr = 0.001\n",
    "\n",
    "    def compile(self, optimizer, loss, metrics, schedule_lr=None):\n",
    "        super(CustomModel, self).compile(optimizer, loss, metrics)\n",
    "        self.schedule_lr = schedule_lr\n",
    "\n",
    "    def train_step(self, data):\n",
    "        images, labels = data\n",
    "        if self.schedule_lr:\n",
    "            self.optimizer.learning_rate = self.schedule_lr(self.optimizer.iterations)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = self(images, training=True)\n",
    "            loss = self.compiled_loss(labels, predictions, regularization_losses=self.losses)\n",
    "\n",
    "        gradients = tape.gradient(loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "\n",
    "        self.compiled_metrics.update_state(labels, predictions)\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "# 학습률 스케줄링 함수 정의\n",
    "def schedule_lr(step):\n",
    "    initial_lr = 0.001\n",
    "    decay_steps = 1000\n",
    "    decay_rate = 0.1\n",
    "    lr = initial_lr * (decay_rate ** (step // decay_steps))\n",
    "    return lr\n",
    "\n",
    "# 모델 인스턴스화\n",
    "model = get_mnist_model()\n",
    "custom_model = CustomModel(inputs=model.input, outputs=model.output)"
   ],
   "id": "cdadb2edf4004a28",
   "execution_count": 29,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### RNN 모델",
   "id": "6d6f238876ea49c0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Layer, LSTMCell, RNN, Dense\n",
    "\n",
    "\n",
    "class CustomRNNCell(LSTMCell):\n",
    "    def call(self, inputs, states, training=None):\n",
    "        # Custom behavior for each time step can be added here\n",
    "        return super().call(inputs, states, training=training)\n",
    "\n",
    "\n",
    "class MyCustomModel(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(MyCustomModel, self).__init__()\n",
    "        self.rnn_cell = CustomRNNCell(128)\n",
    "        self.rnn_layer = RNN(self.rnn_cell)\n",
    "        self.dense = Dense(10)\n",
    "\n",
    "    def call(self, inputs, training=None, mask=None):\n",
    "        x = self.rnn_layer(inputs)\n",
    "        return self.dense(x)\n",
    "\n",
    "# Dummy data\n",
    "dummy_data = tf.random.normal((64, 10, 20))  # (batch_size, time_steps, features)\n",
    "dummy_labels = tf.random.uniform((64,), maxval=10, dtype=tf.int32)\n",
    "\n",
    "\n",
    "# Model instantiation and compilation\n",
    "model = MyCustomModel()\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Model training\n",
    "model.fit(dummy_data, dummy_labels, epochs=5)"
   ],
   "id": "e8aad51589863f59",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### GAN 모델",
   "id": "fb9f0148a5fb7c7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "class GAN(keras.Model):\n",
    "\n",
    "    def __init__(self, generator, discriminator):\n",
    "        super(GAN, self).__init__()\n",
    "        self.generator = generator\n",
    "        self.discriminator = discriminator\n",
    "\n",
    "    def compile(self, generator_optimizer, discriminator_optimizer, loss_fn):\n",
    "        super(GAN, self).compile()\n",
    "        self.generator_optimizer = generator_optimizer\n",
    "        self.discriminator_optimizer = discriminator_optimizer\n",
    "        self.loss_fn = loss_fn\n",
    "\n",
    "    def train_step(self, data):\n",
    "        real_images, _ = data\n",
    "\n",
    "        # 생성기 그라디언트 계산\n",
    "        with tf.GradientTape() as gen_tape:\n",
    "            generated_images = self.generator(tf.random.normal(shape=(batch_size, noise_dim)), training=True)\n",
    "            fake_predictions = self.discriminator(generated_images, training=True)\n",
    "            gen_loss = self.loss_fn(tf.ones_like(fake_predictions), fake_predictions)\n",
    "            \n",
    "        gradients_of_generator = gen_tape.gradient(gen_loss, self.generator.trainable_variables)\n",
    "        self.generator_optimizer.apply_gradients(zip(gradients_of_generator, self.generator.trainable_variables))\n",
    "\n",
    "        # 판별기 그라디언트 계산\n",
    "        with tf.GradientTape() as disc_tape:\n",
    "            real_predictions = self.discriminator(real_images, training=True)\n",
    "            fake_predictions = self.discriminator(generated_images, training=True)\n",
    "            disc_loss = (self.loss_fn(tf.ones_like(real_predictions), real_predictions) + self.loss_fn(tf.zeros_like(fake_predictions), fake_predictions)) / 2\n",
    "\n",
    "\n",
    "\n",
    "        gradients_of_discriminator = disc_tape.gradient(disc_loss, self.discriminator.trainable_variables)\n",
    "        self.discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, self.discriminator.trainable_variables))\n",
    "\n",
    "        return {\"gen_loss\": gen_loss, \"disc_loss\": disc_loss}\n",
    "\n",
    "# 모델, 옵티마이저, 손실 함수 정의\n",
    "generator = create_generator_model()\n",
    "discriminator = create_discriminator_model()\n",
    "gan = GAN(generator, discriminator)\n",
    "\n",
    "gan.compile(generator_optimizer=keras.optimizers.Adam(),\n",
    "            discriminator_optimizer=keras.optimizers.Adam(),\n",
    "            loss_fn=keras.losses.BinaryCrossentropy(from_logits=True))"
   ],
   "id": "9bef7f6dc29b390c",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Image Data : Layer 의 너비가 달라지는 경우",
   "id": "a8f6401329cc2f84"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### DNN 모델",
   "id": "a5399edd546fcc0b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# MNIST 데이터셋 로드 및 전처리\n",
    "def load_mnist_data():\n",
    "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "    x_train = x_train.reshape(-1, 28 * 28) / 255.0\n",
    "    x_test = x_test.reshape(-1, 28 * 28) / 255.\n",
    "    y_train = to_categorical(y_train, 10)\n",
    "    y_test = to_categorical(y_test, 10)\n",
    "    return x_train, y_train, x_test, y_test\n",
    "\n",
    "\n",
    "# 모델 생성 함수\n",
    "\n",
    "def create_model(layer_widths):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(layer_widths[0], activation='relu', input_shape=(784,)))\n",
    "    for width in layer_widths[1:]:\n",
    "        model.add(Dense(width, activation='relu'))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "# 학습 및 평가 함수\n",
    "def train_model(model, x_train, y_train, x_test, y_test, epochs=10):\n",
    "    history = model.fit(x_train, y_train, epochs=epochs, validation_data=(x_test, y_test), verbose=2)\n",
    "    return history\n",
    "\n",
    "\n",
    "# 학습 및 검증 정확도를 그래프로 시각화\n",
    "def plot_and_save_history(histories, titles, filename='model_comparison.png'):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    for history, title in zip(histories, titles):\n",
    "        plt.plot(history.history['val_accuracy'], label=f'{title} val_accuracy')\n",
    "        plt.plot(history.history['accuracy'], label=f'{title} accuracy')\n",
    "    plt.title('Model accuracy comparison')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.savefig(filename)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def main():\n",
    "    x_train, y_train, x_test, y_test = load_mnist_data()\n",
    "    # 모델 정의\n",
    "    basic_model = create_model([128, 64])\n",
    "    wide_model = create_model([512, 256])\n",
    "    narrow_model = create_model([32, 16])\n",
    "    # 학습 및 성능 비교\n",
    "    print(\"Training basic model...\")\n",
    "    basic_history = train_model(basic_model, x_train, y_train, x_test, y_test)\n",
    "    print(\"Training wide model...\")\n",
    "    wide_history = train_model(wide_model, x_train, y_train, x_test, y_test)\n",
    "    print(\"Training narrow model...\")\n",
    "    narrow_history = train_model(narrow_model, x_train, y_train, x_test, y_test)\n",
    "\n",
    "    # 성능 비교 그래프 저장\n",
    "    plot_and_save_history([basic_history, wide_history, narrow_history],\n",
    "                          ['Basic Model', 'Wide Model', 'Narrow Model'])\n",
    "\n",
    "    print(\"\\nPerformance Comparison:\")\n",
    "    basic_loss, basic_accuracy = basic_model.evaluate(x_test, y_test, verbose=2)\n",
    "    wide_loss, wide_accuracy = wide_model.evaluate(x_test, y_test, verbose=2)\n",
    "    narrow_loss, narrow_accuracy = narrow_model.evaluate(x_test, y_test, verbose=2)\n",
    "    print(f\"Basic Model - Test Loss: {basic_loss}, Test Accuracy: {basic_accuracy}\")\n",
    "    print(f\"Wide Model - Test Loss: {wide_loss}, Test Accuracy: {wide_accuracy}\")\n",
    "    print(f\"Narrow Model - Test Loss: {narrow_loss}, Test Accuracy: {narrow_accuracy}\")"
   ],
   "id": "62879b6fe3dd0199",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 실험 2\n",
    "- playground 의 기존 코드를 활용해서 실험을 진행\n",
    "- **conv filter**를 조절하는 실험을 진행"
   ],
   "id": "975c869ffbb83ef1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# MNIST 데이터셋 로드\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "X_train = X_train.reshape(X_train.shape[0], 28, 28, 1).astype('float32') / 255\n",
    "X_test = X_test.reshape(X_test.shape[0], 28, 28, 1).astype('float32') / 255\n",
    "y_train = to_categorical(y_train, 10)\n",
    "y_test = to_categorical(y_test, 10)\n",
    "\n",
    "# 모델 생성 함수\n",
    "def create_model(conv_out_channels=5, fc_output_size=10):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(filters=conv_out_channels, kernel_size=(3, 3), strides=(1, 1), padding='same', activation='relu', input_shape=(28, 28, 1)))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='same'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(fc_output_size, activation='relu'))\n",
    "    model.add(Dense(10, activation='softmax'))  # MNIST는 10개의 클래스가 있음\n",
    "    model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# 하이퍼파라미터 설정\n",
    "conv_out_channels_list = [5, 10, 20]\n",
    "fc_output_sizes = [50, 100, 200]\n",
    "\n",
    "# 실험 결과 저장\n",
    "results = []\n",
    "\n",
    "# 각 설정에 대해 모델 학습 및 평가\n",
    "for conv_out_channels in conv_out_channels_list:\n",
    "    for fc_output_size in fc_output_sizes:\n",
    "        model = create_model(conv_out_channels=conv_out_channels, fc_output_size=fc_output_size)\n",
    "        history = model.fit(X_train, y_train, epochs=5, validation_split=0.2, verbose=1)\n",
    "        loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "        results.append({\n",
    "            'conv_out_channels': conv_out_channels,\n",
    "            'fc_output_size': fc_output_size,\n",
    "            'loss': loss,\n",
    "            'accuracy': accuracy,\n",
    "            'history': history.history\n",
    "        })\n",
    "        print(f\"Conv Out Channels: {conv_out_channels}, FC Output Size: {fc_output_size}, Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# 결과 시각화 (예시: 첫 번째 결과만)\n",
    "plt.plot(results[0]['history']['accuracy'], label='Training accuracy')\n",
    "plt.plot(results[0]['history']['val_accuracy'], label='Validation accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ],
   "id": "262459a055bf32f1",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### 결과\n",
    "<img alt=\"img\" height=\"500\" src=\"result.png\" width=\"800\"/>    \n",
    "\n",
    "\n",
    "----\n",
    "layer의 너비가 넓을 수록 빠른 수렴이 된다"
   ],
   "id": "89d6277433bab51f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### **추가 실험**\n",
    "- conv filter를 위에서 구성한 DNN 코드와 같은 파라미터로 실험을 진행하였다\n",
    "```\n",
    "    basic_model = create_cnn_model([32, 64], 128)\n",
    "    wide_model = create_cnn_model([128, 256], 512)\n",
    "    narrow_model = create_cnn_model([16, 32], 64)\n",
    "```"
   ],
   "id": "44d643669b352fc5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten, MaxPooling2D\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 모델 생성 함수\n",
    "def create_cnn_model(conv_filters, dense_units):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(conv_filters[0], kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Conv2D(conv_filters[1], kernel_size=(3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(dense_units, activation='relu'))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "# 학습 및 평가 함수\n",
    "def train_model(model, x_train, y_train, x_test, y_test, epochs=10):\n",
    "    history = model.fit(x_train, y_train, epochs=epochs, validation_data=(x_test, y_test), verbose=2)\n",
    "    return history\n",
    "\n",
    "\n",
    "# 학습 및 검증 정확도와 손실 값을 그래프로 시각화 및 저장\n",
    "def plot_and_save_history(histories, titles, filename_prefix='cnn_model'):\n",
    "    fig, axs = plt.subplots(2, 1, figsize=(12, 12))\n",
    "\n",
    "    for history, title in zip(histories, titles):\n",
    "        axs[0].plot(history.history['val_accuracy'], label=f'{title} val_accuracy')\n",
    "        axs[0].plot(history.history['accuracy'], label=f'{title} accuracy')\n",
    "        axs[1].plot(history.history['val_loss'], label=f'{title} val_loss')\n",
    "        axs[1].plot(history.history['loss'], label=f'{title} loss')\n",
    "    axs[0].set_title('Model Accuracy Comparison')\n",
    "    axs[0].set_xlabel('Epoch')\n",
    "    axs[0].set_ylabel('Accuracy')\n",
    "    axs[0].legend(loc='lower right')\n",
    "    axs[1].set_title('Model Loss Comparison')\n",
    "    axs[1].set_xlabel('Epoch')\n",
    "    axs[1].set_ylabel('Loss')\n",
    "    axs[1].legend(loc='upper right')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{filename_prefix}_comparison.png')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def main():\n",
    "    x_train, y_train, x_test, y_test = load_mnist_data()\n",
    "    # 모델 정의\n",
    "    basic_model = create_cnn_model([32, 64], 128)\n",
    "    wide_model = create_cnn_model([128, 256], 512)\n",
    "    narrow_model = create_cnn_model([16, 32], 64)\n",
    "\n",
    "    # 학습 및 성능 비교\n",
    "    print(\"Training basic model...\")\n",
    "    basic_history = train_model(basic_model, x_train, y_train, x_test, y_test)\n",
    "    print(\"Training wide model...\")\n",
    "    wide_history = train_model(wide_model, x_train, y_train, x_test, y_test)\n",
    "    print(\"Training narrow model...\")\n",
    "    narrow_history = train_model(narrow_model, x_train, y_train, x_test, y_test)\n",
    "\n",
    "    # 성능 비교 그래프 저장\n",
    "    plot_and_save_history([basic_history, wide_history, narrow_history],\n",
    "                          ['Basic Model', 'Wide Model', 'Narrow Model'])\n",
    "\n",
    "    print(\"\\nPerformance Comparison:\")\n",
    "    basic_loss, basic_accuracy = basic_model.evaluate(x_test, y_test, verbose=2)\n",
    "    wide_loss, wide_accuracy = wide_model.evaluate(x_test, y_test, verbose=2)\n",
    "    narrow_loss, narrow_accuracy = narrow_model.evaluate(x_test, y_test, verbose=2)\n",
    "\n",
    "    print(f\"Basic Model - Test Loss: {basic_loss}, Test Accuracy: {basic_accuracy}\")\n",
    "    print(f\"Wide Model - Test Loss: {wide_loss}, Test Accuracy: {wide_accuracy}\")\n",
    "    print(f\"Narrow Model - Test Loss: {narrow_loss}, Test Accuracy: {narrow_accuracy}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "id": "734f430f9633a1a8",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### 실험 결과\n",
    "<img alt=\"img\" height=\"600\" src=\"cnn_model_comparison.png\" width=\"600\"/>\n",
    "\n",
    "| 모델 유형   | Test Loss | Test Acc | 해석  |\n",
    "|---|---|----|-----|\n",
    "| 기본 모델   | 0.0326    | 0.9920   | - 가장 낮은 손실 값으로 최적의 성능<br>- 매우 높은 정확도<br>- 적절한 컴퓨팅 자원을 사용하여 균형 잡힌 성능  |\n",
    "| 넓은 모델   | 0.0426    | 0.9919   | - 손실 값이 조금 높지만 매우 높은 정확도<br>- 기본 모델과 거의 동일한 성능<br>- 과적합 가능성이 있으므로 추가 검토 필요<br>- 복잡한 패턴을 잘 학습 |\n",
    "| 좁은 모델   | 0.0421    | 0.9883   | - 상대적으로 높은 손실 값<br>- 여전히 우수한 성능을 보이지만 다른 모델보다 약간 낮은 정확도<br>- 표현력의 한계로 인해 일부 패턴 학습에 어려움<br>- 계산 비용이 적어 빠른 학습 가능 |"
   ],
   "id": "95224c2d44c8d85f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 회고\n",
    "- keras에 있는 기본 모델들을 사용해보기만 했지 직접 밑바닥부터 구현해보는 경험을 통해 좀 더 학습 과정을 명확하게 볼 수 있어서 좋았다.\n",
    "- playground 사이트가 자꾸 안돼서 실험을 진행할 수는 없었지만 오히려 직접 모델링을 진행해보는 과정에서 좀 더 효율적인 실험을 위해 모델을 만드는 방법을 알게 되었다(함수로 구현)"
   ],
   "id": "b3b03f715752243c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "4ed9720be82a7387"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
